---
title: 'Spark之MLlib'
date: 2019-04-25 23:10:40
tags: [Spark,MLlib]
categories: Spark
---
# Spark 之 MLlib

## 1. 机器学习

### 1.1 什么是机器学习

机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及**概率论**、**统计学**、**逼近论**、**凸分析**、**算法复杂度理论**等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。简单来说就是通过算法使计算机能够模拟人类的判别能力。

它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。

### 1.2 机器学习的应用

![image_1cn1s012qel8e8i1i2cbm9hrp9.png-176.1kB][1]

### 1.3 机器学习中的几个分类

监督学习(supervised learning)
: 监督学习，即在机械学习过程中提供对错指示。一般实在是数据组中包含最终结果（0，1）。通过算法让机器自我减少误差。这一类学习主要应用于分类和预测 (regression & classify)。监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。

非监督学习(unsupervised learning)
: 非监督学习又称归纳性学习（clustering）利用K方式(Kmeans)，建立中心（centriole），通过循环和递减运算(iteration&descent)来减小误差，达到分类的目的。

强化学习
: Alphago用的就是强化学习，强化学习是一种学习模型，它并不会直接给你解决方案——你要通过试错去找到解决方案。
强化学习不需要标签，你选择的行动（move）越好，得到的反馈越多，所以你能通过执行这些行动看是输是赢来学习下围棋，不需要有人告诉你什么是好的行动什么是坏的行动。

深度学习
: 深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。



### 1.4 机器学习怎么用

![image_1cn1t695fk4715t1jat1stgl4d16.png-109.3kB][2]


### 1.5 常见的几个概念

#### 1.5.1 数据集与测试集
一组数据的集合被称作数据集，用于模型训练的数据集叫训练集，用于测试的数据集叫测试集。一个数据集包含多条数据，一条数据包含多个属性。
<center>![image_1cn1tej521deo18qrh901pou1n2l1j.png-42.9kB][3]</center>

#### 1.5.2 维度、特征
对于西瓜数据集，色泽、根蒂、敲声就是维度，也叫特征值。
<center>![image_1cn1tej521deo18qrh901pou1n2l1j.png-42.9kB][4]</center>

#### 1.5.3 泛化能力
是指机器学习通过训练集进行模型的训练之后对未知的输入的准确判断能力

#### 1.5.4 过拟合和欠拟合
过拟合是指在利用训练数据进行模型训练的时候，模型过多的依赖训练数据中过多的特征属性。欠拟合是指没有通过训练集达到识别的能力。
![image_1cn1u7lun153163k10cb70p33220.png-120.6kB][5]


#### 1.5.5 模型

模型就是复杂的数学相关函数，只是该函数具有很多的未知的参数，通过训练集训练来确定模型中的参数，生成的已知参数的函数就是模型。学习就是根据业务构建模型的过程。

机器学习分为有监督学习和无监督学习，有监督学习是指训练集中有明确的标记，如下数据集：各种特征的西瓜是不是好瓜，有明确的标记。分类就是典型的有监督学习。无监督学习是指训练集中没有明确的标记，聚类就是典型的无监督学习。


#### 1.5.6 损失函数
在统计学，统计决策理论和经济学中，损失函数是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数。更通俗地说，在统计学中损失函数是一种衡量损失和错误（这种损失与“错误地”估计有关，如费用或者设备的损失）程度的函数。简单来说，通过模型预测的预测值与真实值的差产生的函数就是损失函数。
常见的损失函数如下：
$$ J(\theta)=\frac{1}{2}\sum_{i-1}^m{h_\theta(x^i-y^i)^2}$$


常用损失函数
: 

 - 分类算法，其损失函数一般有和两种:
     - 对数损失函数，与Adaboost类似
     - 指数损失函数，分为二元分类和多元分类两种

 - 回归算法，常用损失函数有如下4种:
     - 均方差
     - 绝对损失
     - Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。


#### 1.5.7 查全率与查准率

查准率（正确率）和查全率（召回率）是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；查全率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。
举个例子：

某池塘有1400条鲤鱼，300只虾，300只鳖。现在以捕鲤鱼为目的。撒一大网，逮着了700条鲤鱼，200只虾，100只鳖。那么，这些指标分别如下：
**查准率** = 700 / (700 + 200 + 100) = 70%
**查全率** = 700 / 1400 = 50%

#### 1.5.8 评估参数

均方误差（MSE）
: 均方误差（mean-square error, MSE）是反映估计量与被估计量之间差异程度的一种度量。是指参数估计值与参数真实值之差平方的期望。MSE 可以评价数据的变化程度，MSE 的值越小，说明预测模型描述实验数据具有更好的精确度。
$$ MSE=\frac{1}{N}\sum_{t=1}^N{(observed_t-predicted_t)^2}$$


均方根误差（RMSE）
: 均方根误差（Root Mean Square Error）亦称标准误差,是均方误差的算术平方根。
$$ RMSE=\sqrt{\frac{1}{N}\sum_{t=1}^N{(observed_t-predicted_t)^2}}$$

平均绝对误差（MAE）
: 平均绝对误差（Mean Absolute Deviation），又叫平均绝对离差，是所有单个观测值与算术平均值的偏差的绝对值的平均。平均绝对误差可以避免误差相互抵消的问题，因而可以准确反映实际预测误差的大小。
$$ MAE=\frac{1}{N}\sum_{i=1}^N{\left|(f_i-y_i)\right|}$$


标准差（SD）
: 标准差（Standard Deviation） ，中文环境中又常称均方差，是离均差平方的算术平均数的平方根，用$\sigma$表示。反应的是一个数据集的离散程度，平均数相容的两组数据，标准差未必相同。
$$ RMSE=\sqrt{\frac{1}{N}\sum_{i=1}^N{(x_i-u)^2}}$$
其中$u$表示平均值（$u=\frac{1}{N}(x_1+...+x_N)$）


### 1.7 机器学习的常见算法

逻辑回归
: 

 - 优点：计算代价不高，易于理解和实现。 
 - 缺点：容易欠拟合，分类精度可能不高。
 - 关键词：Sigmoid函数、Softmax解决多分类
 - 适用数据类型：数值型和标称型数据。
 - 其它：逻辑回归函数虽然是一个非线性的函数，但其实其去除Sigmoid映射函数之后，其他步骤都和线性回归一致。


支持向量机
: 

 - 优点：适合小数量样本数据，可以解决高维问题，理论基础比较完善，对于学数学的来说它的理论很美;可以提高泛化能力；
 - 缺点：数据量大时，内存资源消耗大（存储训练样本和核矩阵），时间复杂度高，这时候LR等算法就比SVM要好；对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数,对于核函数的运用对SVM来说确实是一个亮点，但是核函数不是SVM专属的，其他算法一旦涉及到内积运算，就可以使用核函数。它的优化方向的话就是各种不同的场景了，比如扩展到多分类，类别标签不平衡等都可以对SVM做些改变来适应场景
 - 关键词：最优超平面 最大间隔 拉格朗日乘子法 对偶问题 SMO求解 核函数 hinge损失 松弛变量 惩罚因子 多分类
 - 适用数据类型：数值型和标称型数据。
 - 参数：选择核函数，如径向基函数（低维到高维）、线性核函数，以及核函数的参数; 惩罚因子
 - 其它：SVM也并不是在任何场景都比其他算法好，SVM在邮件分类上不如逻辑回归、KNN、bayes的效果好，是基于距离的模型，需要归一化


决策树
: 

 - 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
 - 缺点：过拟合，可限制树深度及叶子节点个数
 - 关键词：ID3(信息增益) C4.5(信息增益比) CART(基尼系数)
 - 数据要求：标称型数据，因此数值型数据必须离散化



随机森林
: 

 - 优点：
     - 对很多数据集表现很好，精确度比较高
     - 不容易过拟合
     - 可以得到变量的重要性排序
     - 既能处理离散型数据，也能处理连续型数据，且不需要进行归一化处理
     - 能够很好的处理缺失数据
     - 容易并行化



Adaboost
: Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。是一种典型的boosting算法。下面是总结下它的优缺点。

 - 优点
     - adaboost是一种有很高精度的分类器。
     - 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。
     - 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。
     - 简单，不用做特征筛选。
     - 不容易发生overfitting。
 - 缺点：对outlier比较敏感



GBDT
: 

 - 优点：
     - 精度高
     - 适用于线性和非线性数据
     - 能处理多特征类型，共线性特征
     - 可以灵活处理各种类型的数据，包括连续值和离散值。
     - 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。
     - 能处理缺失值。使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。
- 缺点：
     - 难并行
     - 多类别分类，时空复杂度高
     - 正则化防止过拟合：
         - 第一种是和Adaboost类似的正则化项，即步长(learning rate)
         - 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。
         - 第三种是对于弱学习器即CART回归树进行正则化剪枝。






xgboost
: 这是一个近年来出现在各大比赛的大杀器，夺冠选手很大部分都使用了它。
高准确率高效率高并发，支持自定义损失函数，既可以用来分类又可以用来回归
可以像随机森林一样输出特征重要性，因为速度快，适合作为高维特征选择的一大利器
在目标函数中加入正则项，控制了模型的复杂程度，可以避免过拟合
支持列抽样，也就是随机选择特征，增强了模型的稳定性
对缺失值不敏感，可以学习到包含缺失值的特征的分裂方向
另外一个广受欢迎的原因是支持并行，速度杠杠的
用的好，你会发现他的全部都是优点


朴素贝叶斯
: 优点：对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练; 对缺失数据不太敏感，算法也比较简单，常用于文本分类。
缺点：朴素贝叶斯模型假设属性之间相互独立，实际应用中属性之间相关性较大时，分类效果不好;需要知道先验概率，且先验概率很多时候取决于假设
数据类型：标称型数据。
朴素：特征之间相互独立；每个特征同等重要。高偏差低方差模型

注意事项：Laplace校准



K-近邻算法(KNN)
: 优点：精度高、对异常值不敏感、无数据输入假定。
缺点：计算复杂度高，空间复杂度，数据不平衡问题。KD-Tree
数据类型：数值型和标称型。
其它：K值如何选择；数据不平衡时分类倾向更多样本的类，解决方法是距离加权重

k值的选择：当k值较小时，预测结果对近邻的实例点非常敏感，容易发生过拟合；如果k值过大模型会倾向大类，容易欠拟合；通常k是不大于20的整数（参考《机器学习实战》）



K-Means(K 均值算法)
: 优点：容易实现。
缺点：K值不容易确定，对初始值敏感，可能收敛到局部最小值。KD-Tree
数据类型：数值型数据。
K值的确定：簇类指标（半径、直径）出现拐点
克服K-均值算法收敛于局部最小值，需确定初始聚类中心：
K-Means++算法：初始的聚类中心之间的相互距离要尽可能的远。
二分K-均值算法：首先将所有点作为一个簇，然后将簇一分为二。之后选择其中一个簇继续划分，选择哪个一簇进行划分取决于对其划分是否可以最大程度降低SSE(Sum of Squared Error，两个簇的总误差平方和)的值。





Apriori算法
: 优点：容易实现。
缺点：在大型数据集上速度较慢。空间复杂度高，主要是C2候选项;时间复杂度高，需多次扫描数据库
数据类型：数值型或标称型数据
原理：如果某个项集时频繁的，那么他的所有子集也是频繁的。
简述：Apriori算法是发现频繁项集的一种方法。Apriori算法的两个输入参数分别是最小支持度和数据集。该算法首先会生成所有单个item的项集列表。然后扫描列表计算每个item的项集支持度，将低于最小支持度的item排除掉，然后将每个item两两组合，然后重新计算整合后的item列表的支持度并且和最小支持度比较。重复这一过程，直至所有项集都被去掉。


FPGrowth算法
: 优点：时间复杂度和空间复杂度都要优于Apriori。
缺点：实现比较困难，在某些数据集上性能会下降
原理：标称型数据
关键词：过滤不频繁集合，项头表支持度排序 FP树 条件模式基 条件树
简述：FP-growth也是用于发现频繁项集的算法，基本数据结构包含一个一棵FP树和一个项头表。构建FP树时只对数据集扫描两次，第二次从FP树中挖掘频繁项集。
继续改进方法：包括数据库划分，数据采样



人工神经网络
: 优点：
分类的准确度高；
并行分布处理能力强,分布存储及学习能力强，
对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系；
具备联想记忆的功能。
缺点：
神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值；
不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度；
学习时间过长,甚至可能达不到学习的目的。


### 1.8 什么是Spark MLlib
![image_1cn219m8o16aau1q1tc5h57rad47.png-70.8kB][6]

MLlib是Spark的机器学习（Machine Learning）库，旨在简化机器学习的工程实践工作，并方便扩展到更大规模。MLlib由一些通用的学习算法和工具组成，包括分类、回归、聚类、协同过滤、降维等，同时还包括底层的优化原语和高层的管道API。

### 

  ![1]( http://static.zybuluo.com/zhangwen100/3bg3kt2pcdtrdrqshuy0dqe2/image_1cn1s012qel8e8i1i2cbm9hrp9.png)
  ![2]( http://static.zybuluo.com/zhangwen100/bdauvrxoz00yii9p07gmexqy/image_1cn1t695fk4715t1jat1stgl4d16.png)
  ![3]( http://static.zybuluo.com/zhangwen100/sp0vj1f5klxadfbgjinszptt/image_1cn1tej521deo18qrh901pou1n2l1j.png)
  ![4]( http://static.zybuluo.com/zhangwen100/sp0vj1f5klxadfbgjinszptt/image_1cn1tej521deo18qrh901pou1n2l1j.png)
  ![5]( http://static.zybuluo.com/zhangwen100/tmh623e0zr5qy4xe0mnabago/image_1cn1u7lun153163k10cb70p33220.png)
  ![6]( http://static.zybuluo.com/zhangwen100/yb7ityrtl393d6qun2wbcs94/image_1cn219m8o16aau1q1tc5h57rad47.png)