<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<script>
    (function(){
        if(''){
            if (prompt('请输入查看该篇文章的密码') !== ''){
                alert('密码错误，请咨询博主');
                history.back();
            }
        }
    })();
</script>


  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "39f09b08"
    });
  daovoice('update');
  </script>















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Spark,SparkSQL,">










<meta name="description" content="Spark 第三天之Spark SQL1. Spark SQL概述1.1    什么是Spark SQL![image_1cmio2ig3j22l44vdi31a1m509.png-58.8kB][1]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布">
<meta name="keywords" content="Spark,SparkSQL">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark第三天之SparkSQL">
<meta property="og:url" content="http://yoursite.com/2019/04/25/1_Spark 第三天之Spark SQL/index.html">
<meta property="og:site_name" content="To树-HOME">
<meta property="og:description" content="Spark 第三天之Spark SQL1. Spark SQL概述1.1    什么是Spark SQL![image_1cmio2ig3j22l44vdi31a1m509.png-58.8kB][1]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/wz16d2sf49lv214honl95n1m/image_1cmio2ig3j22l44vdi31a1m509.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/f0ob8qrn64bk0eh6b69o40do/image_1cmioarhitcofuckg21mj819dc1m.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/s6w2ht1grmme7fxz0twt2k9m/image_1cmiobr9f16pf1s3s1bj6n3b9vl23.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/j6m690urvdi9ixikhet13v7p/image_1cmiocej31n931sm4qrsmgi1fad2g.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/7lkpy3h7gs64s1ahuy262rx1/image_1cmioct9e1c7519la14141tah1v722t.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/ugyb2nwgb5s13wnqt89z4n68/image_1cmiokv241engdid15k31kt72so9.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/6ppsc1z0yet35uma6hlwuz89/image_1cmioruf962jkms3t7uke10ahm.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/nkux489j4jvaj5fkgyf8vj6x/image_1cmip0l5k1ph1d5de5u11qo13eq13.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/uoiul4v1iubq2ab6lutths66/image_1cmipmbtrurtraot1m5tnmkr1g.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/gos4pjreefaejnehj7ylvoql/image_1cmmpgfn07vudg5177i1sa51uq09.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/euemtz5n6ouss6pxhz9en9w6/image_1cml86ltg16l2eio14m963b1q2rm.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/qo976u7wbxk8xgvmd8fcmgmo/image_1cml8bq7t3go1moefiirs01uh41g.png">
<meta property="og:image" content="http://static.zybuluo.com/zhangwen100/sc0hnsjqtpbs940fnx9t7b2k/image_1cml6v59u8p31c501bj97h31d859.png">
<meta property="og:updated_time" content="2019-05-26T08:58:42.552Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark第三天之SparkSQL">
<meta name="twitter:description" content="Spark 第三天之Spark SQL1. Spark SQL概述1.1    什么是Spark SQL![image_1cmio2ig3j22l44vdi31a1m509.png-58.8kB][1]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布">
<meta name="twitter:image" content="http://static.zybuluo.com/zhangwen100/wz16d2sf49lv214honl95n1m/image_1cmio2ig3j22l44vdi31a1m509.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/04/25/1_Spark 第三天之Spark SQL/">





  <title>Spark第三天之SparkSQL | To树-HOME</title>
  









<!-- katex样式 -->
  <link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
  <a href="https://github.com/zzckm" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">To树-HOME</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">千客云起时</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/25/1_Spark 第三天之Spark SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ckm_zz">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/ggg.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="To树-HOME">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark第三天之SparkSQL</h1>
        

        <div class="post-meta">
        <!-- TOP -->
        

          <span class="post-time">
            

            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-05-26T16:58:42+08:00">
                2019-05-26
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv">
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  5.3k 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  23 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Spark-第三天之Spark-SQL"><a href="#Spark-第三天之Spark-SQL" class="headerlink" title="Spark 第三天之Spark SQL"></a>Spark 第三天之Spark SQL</h1><h2 id="1-Spark-SQL概述"><a href="#1-Spark-SQL概述" class="headerlink" title="1. Spark SQL概述"></a>1. Spark SQL概述</h2><h3 id="1-1-什么是Spark-SQL"><a href="#1-1-什么是Spark-SQL" class="headerlink" title="1.1    什么是Spark SQL"></a><strong>1.1    什么是Spark SQL</strong></h3><p>![image_1cmio2ig3j22l44vdi31a1m509.png-58.8kB][1]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们已经学习了Hive，它是将Hive SQL 转换成 MapReduce 然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果说Hive是为了简化MapReduce操作，那么Spark SQL可以说是为了简化RDD的操作。让一个不懂RDD操作的人也可以轻松使用Spark。</p>
<ul>
<li>易整合<br>![image_1cmioarhitcofuckg21mj819dc1m.png-50.6kB][2]</li>
<li>统一的数据访问方式<br>![image_1cmiobr9f16pf1s3s1bj6n3b9vl23.png-62.6kB][3]</li>
<li>兼容Hive<br>![image_1cmiocej31n931sm4qrsmgi1fad2g.png-74.4kB][4]</li>
<li>标准的数据连接<br>![image_1cmioct9e1c7519la14141tah1v722t.png-54.9kB][5]<br>![image_1cmiokv241engdid15k31kt72so9.png-30.2kB][6]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。</li>
</ul>
<h3 id="1-2-Spark-SQl的数据抽象"><a href="#1-2-Spark-SQl的数据抽象" class="headerlink" title="1.2 Spark SQl的数据抽象"></a><strong>1.2 Spark SQl的数据抽象</strong></h3><p>![image_1cmioruf962jkms3t7uke10ahm.png-99.4kB][7]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：<br>RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。</p>
<dl><dt>Dataframe</dt><dd>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。<br>![image_1cmip0l5k1ph1d5de5u11qo13eq13.png-67.3kB][8]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待。简单来说DataFrame=RDD+Schema</dd></dl><blockquote>
<p>DataFrame也是懒执行的。<br>性能上比RDD要高，主要有两方面原因：<br><strong>定制化内存管理:</strong>数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制。<br><strong>优化的执行计划:</strong>查询计划通过Spark catalyst optimiser进行优化.<br><strong>Dataframe的劣势</strong>在于在编译期缺少类型安全检查，导致运行时出错.</p>
</blockquote>
<p>Dataset<br>: </p>
<ul>
<li>是Dataframe API的一个扩展，是Spark最新的数据抽象</li>
<li>用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。</li>
<li>Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</li>
<li>样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。</li>
<li>Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息都用Row来表示。</li>
<li>DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么，控制的粒度不一样。<br>     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;![image_1cmipmbtrurtraot1m5tnmkr1g.png-150.3kB][9]</p>
<p>三者的共性<br>: </p>
<ul>
<li>RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利</li>
<li>三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过.</li>
<li>三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li>
<li>三者都有partition的概念</li>
<li>三者有许多共同的函数，如filter，排序等</li>
<li>在对DataFrame和Dataset进行操作许多操作都需要spark.implicits._进行支持</li>
<li>DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型</li>
</ul>
<p>三者的区别<br>: </p>
<ul>
<li>RDD<ul>
<li>RDD一般和spark mllib同时使用</li>
<li>RDD不支持sparksql操作</li>
</ul>
</li>
<li>DataFrame<ul>
<li>与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值</li>
<li>DataFrame与Dataset一般不与spark mllib同时使用</li>
<li>DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作</li>
<li>DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然</li>
</ul>
</li>
<li>Dataset<ul>
<li>Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同</li>
<li>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息</li>
</ul>
</li>
</ul>
<h2 id="2-执行SparkSQL"><a href="#2-执行SparkSQL" class="headerlink" title="2. 执行SparkSQL"></a><strong>2. 执行SparkSQL</strong></h2><h3 id="2-1-命令式操作"><a href="#2-1-命令式操作" class="headerlink" title="2.1 命令式操作"></a><strong>2.1 命令式操作</strong></h3><h3 id="2-2-代码操作"><a href="#2-2-代码操作" class="headerlink" title="2.2 代码操作"></a><strong>2.2 代码操作</strong></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhiyou100.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestSparkSql</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"testSparkSql"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line">  <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> employee = spark.read.json(<span class="string">"C:\\Users\\zhang\\Desktop\\employees.json"</span>)</span><br><span class="line">  employee.show()</span><br><span class="line">  employee.createOrReplaceTempView(<span class="string">"employee"</span>)</span><br><span class="line">  spark.sql(<span class="string">"select * from employee"</span>).show()</span><br><span class="line">  spark.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-3-数据类型转换"><a href="#2-3-数据类型转换" class="headerlink" title="2.3 数据类型转换"></a><strong>2.3 数据类型转换</strong></h3><p>将RDD，DataFrame，DataSet之间进行互相转换</p>
<p>RDD  -》  DataFrame<br>: </p>
<ul>
<li><p>直接手动转换</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"> scala&gt;</span><span class="bash"> val people = spark.read.json(<span class="string">"/opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.json"</span>)</span></span><br><span class="line">people: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val people1 = sc.textFile(<span class="string">"/opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.txt"</span>)</span></span><br><span class="line">people1: org.apache.spark.rdd.RDD[String] = /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.txt MapPartitionsRDD[18] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val peopleSplit = people1.map&#123;x =&gt; val strs = x.split(<span class="string">","</span>);(strs(0),strs(1).trim.toInt)&#125;</span></span><br><span class="line">peopleSplit: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[19] at map at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> peopleSplit.collect</span></span><br><span class="line">res6: Array[(String, Int)] = Array((Michael,29), (Andy,30), (Justin,19))        </span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> peopleSplit.to</span></span><br><span class="line">toDF   toDS   toDebugString   toJavaRDD   toLocalIterator   toString   top</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> peopleSplit.toDF</span></span><br><span class="line">res7: org.apache.spark.sql.DataFrame = [_1: string, _2: int]</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> peopleSplit.toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)</span></span><br><span class="line">res8: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> res8.show</span></span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过Scala编程实现</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 创建 schema </span></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val schema = StructType(StructField(<span class="string">"name"</span>,StringType)::StructField(<span class="string">"age"</span>,IntegerType)::Nil)</span></span><br><span class="line">schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true))</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 加载RDD数据</span></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val rdd = sc.textFile(<span class="string">"/opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.txt"</span>)</span></span><br><span class="line">rdd: org.apache.spark.rdd.RDD[String] = /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:30</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 创建Row对象</span></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val data = rdd.map&#123;x =&gt; val strs = x.split(<span class="string">","</span>);Row(strs(0),strs(1).trim.toInt)&#125;</span></span><br><span class="line">data: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[2] at map at &lt;console&gt;:32</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 生成DF</span></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.createDataFrame(data,schema)</span></span><br><span class="line">18/09/06 09:45:00 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0</span><br><span class="line">18/09/06 09:45:00 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException</span><br><span class="line">18/09/06 09:45:02 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</span><br><span class="line">res0: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br></pre></td></tr></table></figure>
</li>
<li><p>反射</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> <span class="keyword">case</span> class People(name:String,age:Int)</span></span><br><span class="line">defined class People</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> rdd.map&#123;x =&gt; val strs=x.split(<span class="string">","</span>);People(strs(0),strs(1).trim.toInt)&#125;.toDF</span></span><br><span class="line">res2: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<dl><dt>DataFrame -》 RDD</dt><dd><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> res8.rdd</span></span><br><span class="line">res10: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[26] at rdd at &lt;console&gt;:31</span><br></pre></td></tr></table></figure></dd></dl><dl><dt>RDD -》  DataSet</dt><dd><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    scala&gt; peopleSplit.toDS</span><br><span class="line">    res11: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]</span><br><span class="line">    </span><br><span class="line">    scala&gt; case class People(name:String,age:Int)</span><br><span class="line">    defined class People</span><br><span class="line">    </span><br><span class="line">    scala&gt; val peopleDSSplit = people1.map&#123;x =&gt; val strs = x.split(","); People(strs(0),strs(1).trim.toInt)&#125;</span><br><span class="line">    peopleDSSplit: org.apache.spark.rdd.RDD[People] = MapPartitionsRDD[27] at map at &lt;console&gt;:28</span><br><span class="line">    </span><br><span class="line">    scala&gt; peopleDSSplit.toDS</span><br><span class="line">    res12: org.apache.spark.sql.Dataset[People] = [name: string, age: int]</span><br><span class="line">    </span><br><span class="line">    scala&gt; res12.show</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure></dd></dl><p>DataSet -》 RDD<br>: </p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> res12.rdd</span></span><br><span class="line">res14: org.apache.spark.rdd.RDD[People] = MapPartitionsRDD[32] at rdd at &lt;console&gt;:33</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> res14.map(_.name).collect</span></span><br><span class="line">res15: Array[String] = Array(Michael, Andy, Justin)</span><br></pre></td></tr></table></figure></code></pre><dl><dt>DataSet -》  DataFrame</dt><dd><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> res12.toDF</span></span><br><span class="line">res16: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br></pre></td></tr></table></figure></dd></dl><dl><dt>DataFrame -》 Datset</dt><dd><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> res16.as[People]</span></span><br><span class="line">res17: org.apache.spark.sql.Dataset[People] = [name: string, age: int]</span><br></pre></td></tr></table></figure></dd></dl><h3 id="2-4-SQL的执行模式"><a href="#2-4-SQL的执行模式" class="headerlink" title="2.4 SQL的执行模式"></a><strong>2.4 SQL的执行模式</strong></h3><dl><dt>DSL风格语法</dt><dd><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val peopleDF = rdd.map&#123;x =&gt; val strs=x.split(",");People(strs(0),strs(1).trim.toInt)&#125;.toDF</span><br><span class="line">peopleDF: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.select("name").show</span><br><span class="line">+<span class="comment">-------+</span></span><br><span class="line">|   name|</span><br><span class="line">+<span class="comment">-------+</span></span><br><span class="line">|Michael|</span><br><span class="line">|   Andy|</span><br><span class="line">| Justin|</span><br><span class="line">+<span class="comment">-------+</span></span><br><span class="line">scala&gt; peopleDF.filter($"age"&gt;20).show</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">|   name|age|</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">scala&gt; peopleDF.groupBy("age").count.show</span><br><span class="line">+<span class="comment">---+-----+                                                                     </span></span><br><span class="line">|age|count|</span><br><span class="line">+<span class="comment">---+-----+</span></span><br><span class="line">| 19|    1|</span><br><span class="line">| 29|    1|</span><br><span class="line">| 30|    1|</span><br><span class="line">+<span class="comment">---+-----+</span></span><br></pre></td></tr></table></figure></dd></dl><dl><dt>SQL风格语法</dt><dd><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 借助于SparkSession</span></span><br><span class="line"><span class="comment">## createOrReplaceTempView：Session内可以访问，一旦session 停止，表自动删除。不需要前缀，如果表存在，会自动替换</span></span><br><span class="line"><span class="comment">## createGlobalOrReplaceTempView：一个应用级别的访问，多个session之间可以访问，但是一旦SparkContext关闭，也会删除。需要前缀</span></span><br><span class="line"><span class="comment">## createTempView：Session内可以访问，一旦session 停止，表自动删除。不需要前缀，如果表存在则报错</span></span><br><span class="line"><span class="comment">## createGlobalTempView：一个应用级别的访问，多个session之间可以访问，但是一旦SparkContext关闭，也会删除</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql("select * from people").show</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">|   name|age|</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">scala&gt; spark.newSession.sql("select * from people").show</span><br><span class="line"><span class="comment">## 出错</span></span><br><span class="line"></span><br><span class="line">scala&gt; spark.newSession.sql("select * from global_temp.people").show</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">|   name|age|</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">scala&gt; spark.sql("select * from global_temp.people").show</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">|   name|age|</span><br><span class="line">+<span class="comment">-------+---+</span></span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+<span class="comment">-------+---+</span></span><br></pre></td></tr></table></figure></dd></dl><h2 id="3-自定义函数"><a href="#3-自定义函数" class="headerlink" title="3. 自定义函数"></a>3. 自定义函数</h2><h3 id="3-1-UDF函数"><a href="#3-1-UDF函数" class="headerlink" title="3.1 UDF函数"></a>3.1 UDF函数</h3><dl><dt>需求： 在每一行数据的name列值的前面haha</dt><dd><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"select * from people"</span>).show</span></span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+-------+---+</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.udf.register(<span class="string">"add"</span>,(x:String)=&gt;<span class="string">"hehe"</span>+x)</span></span><br><span class="line">res22: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"select add(name),age from people"</span>).show</span></span><br><span class="line">+-------------+---+</span><br><span class="line">|UDF:add(name)|age|</span><br><span class="line">+-------------+---+</span><br><span class="line">|  heheMichael| 29|</span><br><span class="line">|     heheAndy| 30|</span><br><span class="line">|   heheJustin| 19|</span><br><span class="line">+-------------+---+</span><br></pre></td></tr></table></figure></dd></dl><h3 id="3-2-UDAF函数"><a href="#3-2-UDAF函数" class="headerlink" title="3.2 UDAF函数"></a>3.2 UDAF函数</h3><p>需要通过继承 UserDefinedAggregateFunction 来实现自定义聚合函数。案例：计算一下员工的平均工资</p>
<p>弱类型聚合函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhiyou100.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 弱类型的</span></span><br><span class="line"><span class="comment">  * 计算员工的平均薪资</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AverageSalaryRuo</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line">  <span class="comment">//输入的数据的格式</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"salary"</span>,<span class="type">IntegerType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  <span class="comment">//每个分区中共享的数据变量结构</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"sum"</span>,<span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">"count"</span>,<span class="type">IntegerType</span>):: <span class="type">Nil</span>)</span><br><span class="line">  <span class="comment">//输出的数据的类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line">  <span class="comment">//表示如果有相同的输入是否会存在相同的输出，是：true</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">  <span class="comment">//初始化的每个分区共享变量</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//每一个分区的每一条数据聚合的时候进行buffer的更新</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//将buffer中的薪资总和的数据进行更新，原数据加上新输入的数据，buffer就类似于resultSet</span></span><br><span class="line">    buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getInt(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">//每添加一个薪资，就将员工的个数加1</span></span><br><span class="line">    buffer(<span class="number">1</span>) = buffer.getInt(<span class="number">1</span>)+<span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//将每个分区的输出合并</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getInt(<span class="number">1</span>)+buffer2.getInt(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//获取最终的结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    <span class="comment">//计算平均薪资并返回</span></span><br><span class="line">    buffer.getLong(<span class="number">0</span>).toDouble/buffer.getInt(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AverageSalaryRuo</span> <span class="keyword">extends</span> <span class="title">App</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"udaf"</span>).setMaster(<span class="string">"local[3]"</span>)</span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line">  <span class="keyword">val</span> data = spark.read.json(<span class="string">"C:\\Users\\zhang\\Desktop\\employees.json"</span>)</span><br><span class="line">  data.createOrReplaceTempView(<span class="string">"employee"</span>)</span><br><span class="line">  <span class="comment">//注册自定义聚合函数</span></span><br><span class="line">  spark.udf.register(<span class="string">"avgSalary"</span>,<span class="keyword">new</span> <span class="type">AverageSalaryRuo</span>)</span><br><span class="line">  spark.sql(<span class="string">"select avgSalary(salary) from employee"</span>).show()</span><br><span class="line">  spark.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<dl><dt>强类型聚合函数</dt><dd><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhiyou100.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 弱类型的</span></span><br><span class="line"><span class="comment">  * 计算员工的平均薪资</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//对于强类型来说，无非就是借助于样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span>(<span class="params">name:<span class="type">String</span>,salary:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Average</span>(<span class="params">var sum:<span class="type">Long</span>,var count:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">AverageSalaryQiang</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Employee</span>,<span class="type">Average</span>,<span class="type">Double</span>]</span>&#123;</span><br><span class="line">  <span class="comment">//初始化方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Average</span> = <span class="type">Average</span>(<span class="number">0</span>L,<span class="number">0</span>)</span><br><span class="line">  <span class="comment">//一个分区内的聚合调用，类似于update方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Average</span>, a: <span class="type">Employee</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b.sum = b.sum + a.salary</span><br><span class="line">    b.count = b.count + <span class="number">1</span></span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Average</span>, b2: <span class="type">Average</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b1.sum = b1.sum + b2.sum</span><br><span class="line">    b1.count = b1.count + b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//最终的计算结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Average</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    reduction.sum.toDouble /reduction.count</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//对buffer编码</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Average</span>] = <span class="type">Encoders</span>.product</span><br><span class="line">  <span class="comment">//对out编码</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AverageSalaryQiang</span> <span class="keyword">extends</span> <span class="title">App</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"udaf"</span>).setMaster(<span class="string">"local[3]"</span>)</span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line">  <span class="keyword">import</span>  spark.implicits._</span><br><span class="line">  <span class="keyword">val</span> employee = spark.read.json(<span class="string">"C:\\Users\\zhang\\Desktop\\employees.json"</span>).as[<span class="type">Employee</span>]</span><br><span class="line">  employee.show()</span><br><span class="line">  employee.createOrReplaceTempView(<span class="string">"employee"</span>)</span><br><span class="line">  <span class="comment">//注册自定义函数</span></span><br><span class="line">  <span class="keyword">val</span> aaa = <span class="keyword">new</span> <span class="type">AverageSalaryQiang</span>().toColumn.name(<span class="string">"aaaa"</span>)</span><br><span class="line">  spark.sql(<span class="string">"select * from employee"</span>).show()</span><br><span class="line">  <span class="comment">//spark.sql("select aaaa(salary) from employee").show()</span></span><br><span class="line">  employee.select(aaa).show()</span><br><span class="line">  spark.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></dd></dl><h3 id="3-3-开窗函数"><a href="#3-3-开窗函数" class="headerlink" title="3.3 开窗函数"></a>3.3 开窗函数</h3><p>over（）开窗函数<br>: </p>
<ul>
<li>在使用聚合函数后，会将多行变成一行，而开窗函数是将一行变成多行；</li>
<li>并且在使用聚合函数后，如果要显示其他的列必须将列加入到group by中，而使用开窗函数后，可以不使用group by，直接将所有信息显示出来。</li>
<li>开窗函数适用于在每一行的最后一列添加聚合函数的结果。</li>
</ul>
<p>开窗函数作用<br>: </p>
<ul>
<li>为每条数据显示聚合信息.(聚合函数() over())</li>
<li>为每条数据提供分组的聚合函数结果(聚合函数() over(partition by 字段) as 别名) <pre><code>--按照字段分组，分组后进行计算</code></pre></li>
<li>与排名函数一起使用(row number() over(order by 字段) as 别名)</li>
</ul>
<p>常用分析函数：（最常用的应该是1.2.3 的排序）<br>: </p>
<ul>
<li>row_number() over(partition by … order by …)</li>
<li>rank() over(partition by … order by …)</li>
<li>dense_rank() over(partition by … order by …)</li>
<li>count() over(partition by … order by …)</li>
<li>max() over(partition by … order by …)</li>
<li>min() over(partition by … order by …)</li>
<li>sum() over(partition by … order by …)</li>
<li>avg() over(partition by … order by …)</li>
<li>first_value() over(partition by … order by …)</li>
<li>last_value() over(partition by … order by …)</li>
<li>lag() over(partition by … order by …)</li>
<li>lead() over(partition by … order by …)<br>lag 和lead 可以获取结果集中，按一定排序所排列的当前行的上下相邻若干offset 的某个行的某个列(不用结果集的自关联）；<br>lag ，lead 分别是向前，向后；<br>lag 和lead 有三个参数，第一个参数是列名，第二个参数是偏移的offset，第三个参数是超出记录窗口时的默认值</li>
</ul>
<dl><dt>案例实战</dt><dd>![image_1cmmpgfn07vudg5177i1sa51uq09.png-60.9kB][10]</dd></dl><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据格式</span><br><span class="line">姓名    班级    分数</span><br></pre></td></tr></table></figure>

<h2 id="4-Spark-SQL的数据源"><a href="#4-Spark-SQL的数据源" class="headerlink" title="4. Spark SQL的数据源"></a>4. Spark SQL的数据源</h2><h3 id="4-1-与Hive集成"><a href="#4-1-与Hive集成" class="headerlink" title="4.1 与Hive集成"></a>4.1 与Hive集成</h3><p>4.1.1 使用内置的Hive<br>: </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master spark-2.2.2-bin-hadoop2.7]# /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/bin/spark-shell --master spark://master:7077 --total-executor-cores 3 --executor-memory 512m --conf spark.sql.warehouse.dir=hdfs://master:9000/spark/warehouse</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"create table student(name String,age Int)"</span>)</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"load data local inpath '/opt/test/spark/student' into table student"</span>)</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"select * from student"</span>).show</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 只有在第一次操作的时候需要指定数据文件存放的目录，第二次不需要指定。</span></span></span><br></pre></td></tr></table></figure>

<p>4.1.2 使用外置的Hive<br>:  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 将Hive的配置文件放到spark 的conf目录</span></span></span><br><span class="line">[root@master conf]# ln -s /opt/apps/Hive/hive-2.3.3/conf/hive-site.xml ./hive-site.xml</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 将Hive中的mysql的驱动jar包上传到spark中</span></span></span><br><span class="line">[root@master lib]# cp mysql-connector-java-5.1.46-bin.jar /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/jars</span><br></pre></td></tr></table></figure>

<ul>
<li>启动spark-shell可以直接通过Hive的元数据信息进行Hive的管理</li>
<li>Spark SQL其实就是整合了很多数据库的操作，在于Hive整合的时候，只要有了Hive的元数据信息，以及元数据存储的数据的驱动就可以直接进行管理</li>
</ul>
<h3 id="4-2-输入输出格式"><a href="#4-2-输入输出格式" class="headerlink" title="4.2 输入输出格式"></a><strong>4.2 输入输出格式</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称定json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。<br>[数据源][11]</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> employee.write.format(<span class="string">"jdbc"</span>).option(<span class="string">"url"</span>,<span class="string">"jdbc:mysql://master:3306/mysql_bigdata"</span>).option(<span class="string">"user"</span>,<span class="string">"root"</span>).option(<span class="string">"password"</span>,<span class="string">"123456"</span>).option(<span class="string">"dbtable"</span>,<span class="string">"employee"</span>).mode(<span class="string">"overwrite"</span>).save()</span></span><br></pre></td></tr></table></figure>

<p>[保存数据的方式][12]</p>
<h3 id="4-3-与数据库交互"><a href="#4-3-与数据库交互" class="headerlink" title="4.3 与数据库交互"></a><strong>4.3 与数据库交互</strong></h3><dl><dt>将数据导入MySQL</dt><dd><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val employee = spark.read.json(<span class="string">"/opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/employees.json"</span>)</span></span><br><span class="line">employee: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> employee.show</span></span><br><span class="line">+-------+------+</span><br><span class="line">|   name|salary|</span><br><span class="line">+-------+------+</span><br><span class="line">|Michael|  3000|</span><br><span class="line">|   Andy|  4500|</span><br><span class="line">| Justin|  3500|</span><br><span class="line">|  Berta|  4000|</span><br><span class="line">+-------+------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> employee.write.format(<span class="string">"jdbc"</span>).option(<span class="string">"url"</span>,<span class="string">"jdbc:mysql://master:3306/mysql_bigdata"</span>).option(<span class="string">"user"</span>,<span class="string">"root"</span>).option(<span class="string">"password"</span>,<span class="string">"123456"</span>).option(<span class="string">"dbtable"</span>,<span class="string">"employee"</span>).save()</span></span><br></pre></td></tr></table></figure></dd></dl><dl><dt>读取MySQL中的数据</dt><dd><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.read.format(<span class="string">"jdbc"</span>).option(<span class="string">"url"</span>,<span class="string">"jdbc:mysql://master:3306/mysql_bigdata"</span>).option(<span class="string">"user"</span>,<span class="string">"root"</span>).option(<span class="string">"password"</span>,<span class="string">"123456"</span>).option(<span class="string">"dbtable"</span>,<span class="string">"student"</span>).load().show</span></span><br></pre></td></tr></table></figure></dd></dl><h2 id="5-Spark-SQL架构原理"><a href="#5-Spark-SQL架构原理" class="headerlink" title="5. Spark SQL架构原理"></a>5. Spark SQL架构原理</h2><h3 id="5-1-Spark-SQL-的模块"><a href="#5-1-Spark-SQL-的模块" class="headerlink" title="5.1 Spark SQL 的模块"></a>5.1 Spark SQL 的模块</h3><p>Spark SQL模块划分为Core、caralyst、hive和hive- ThriftServer四大模块。<br>![image_1cml86ltg16l2eio14m963b1q2rm.png-14.6kB][13]</p>
<h3 id="5-2-Spark-SQL的运行"><a href="#5-2-Spark-SQL的运行" class="headerlink" title="5.2 Spark SQL的运行"></a>5.2 Spark SQL的运行</h3><p>![image_1cml8bq7t3go1moefiirs01uh41g.png-93.6kB][14]</p>
<ul>
<li>使用SessionCatalog保存元数据<br>在解析SQL语句之前，会创建SparkSession，或者如果是2.0之前的版本初始化SQLContext，SparkSession只是封装了SparkContext和SQLContext的创建而已。会把元数据保存在SessionCatalog中，涉及到表名，字段名称和字段类型。创建临时表或者视图，其实就会往SessionCatalog注册</li>
<li>解析SQL,使用ANTLR生成未绑定的逻辑计划<br>当调用SparkSession的sql或者SQLContext的sql方法，我们以2.0为准，就会使用SparkSqlParser进行解析SQL. 使用的ANTLR进行词法解析和语法解析。它分为2个步骤来生成Unresolved LogicalPlan：词法分析：Lexical Analysis，负责将token分组成符号类，构建一个分析树或者语法树AST</li>
<li>使用分析器Analyzer绑定逻辑计划<br>在该阶段，Analyzer会使用Analyzer Rules，并结合SessionCatalog，对未绑定的逻辑计划进行解析，生成已绑定的逻辑计划。</li>
<li>使用优化器Optimizer优化逻辑计划<br>优化器也是会定义一套Rules，利用这些Rule对逻辑计划和Exepression进行迭代处理，从而使得树的节点进行和并和优化</li>
<li>使用SparkPlanner生成物理计划<br>SparkSpanner使用Planning Strategies，对优化后的逻辑计划进行转换，生成可以执行的物理计划SparkPlan.</li>
<li>使用QueryExecution执行物理计划<br>此时调用SparkPlan的execute方法，底层其实已经再触发JOB了，然后返回DataFrame</li>
</ul>
<h2 id="6-案例实战"><a href="#6-案例实战" class="headerlink" title="6. 案例实战"></a><strong>6. 案例实战</strong></h2><h3 id="6-1-数据说明"><a href="#6-1-数据说明" class="headerlink" title="6.1 数据说明"></a><strong>6.1 数据说明</strong></h3><p>数据集是货品交易数据集。<br>![image_1cml6v59u8p31c501bj97h31d859.png-64.1kB][15]<br>每个订单可能包含多个货品，每个订单可以产生多次交易，不同的货品有不同的单价。</p>
<h3 id="6-2-需求"><a href="#6-2-需求" class="headerlink" title="6.2 需求"></a><strong>6.2 需求</strong></h3><ul>
<li>统计所有订单中每年的销售单数、销售总额</li>
<li>统计每年最大金额订单的销售额</li>
<li>统计每年最畅销货品（哪个货品销售额amount在当年最高，哪个就是最畅销货品）</li>
</ul>
<p>  <img src="http://static.zybuluo.com/zhangwen100/wz16d2sf49lv214honl95n1m/image_1cmio2ig3j22l44vdi31a1m509.png" alt="1"><br>  <img src="http://static.zybuluo.com/zhangwen100/f0ob8qrn64bk0eh6b69o40do/image_1cmioarhitcofuckg21mj819dc1m.png" alt="2"><br>  <img src="http://static.zybuluo.com/zhangwen100/s6w2ht1grmme7fxz0twt2k9m/image_1cmiobr9f16pf1s3s1bj6n3b9vl23.png" alt="3"><br>  <img src="http://static.zybuluo.com/zhangwen100/j6m690urvdi9ixikhet13v7p/image_1cmiocej31n931sm4qrsmgi1fad2g.png" alt="4"><br>  <img src="http://static.zybuluo.com/zhangwen100/7lkpy3h7gs64s1ahuy262rx1/image_1cmioct9e1c7519la14141tah1v722t.png" alt="5"><br>  <img src="http://static.zybuluo.com/zhangwen100/ugyb2nwgb5s13wnqt89z4n68/image_1cmiokv241engdid15k31kt72so9.png" alt="6"><br>  <img src="http://static.zybuluo.com/zhangwen100/6ppsc1z0yet35uma6hlwuz89/image_1cmioruf962jkms3t7uke10ahm.png" alt="7"><br>  <img src="http://static.zybuluo.com/zhangwen100/nkux489j4jvaj5fkgyf8vj6x/image_1cmip0l5k1ph1d5de5u11qo13eq13.png" alt="8"><br>  <img src="http://static.zybuluo.com/zhangwen100/uoiul4v1iubq2ab6lutths66/image_1cmipmbtrurtraot1m5tnmkr1g.png" alt="9"><br>  <img src="http://static.zybuluo.com/zhangwen100/gos4pjreefaejnehj7ylvoql/image_1cmmpgfn07vudg5177i1sa51uq09.png" alt="10"><br>  ![11]( <a href="http://spark.apache.org/docs/2.2.2/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">http://spark.apache.org/docs/2.2.2/sql-programming-guide.html#data-sources</a><br>  ![12]( <a href="http://spark.apache.org/docs/2.2.2/sql-programming-guide.html#save-modes" target="_blank" rel="noopener">http://spark.apache.org/docs/2.2.2/sql-programming-guide.html#save-modes</a><br>  <img src="http://static.zybuluo.com/zhangwen100/euemtz5n6ouss6pxhz9en9w6/image_1cml86ltg16l2eio14m963b1q2rm.png" alt="13"><br>  <img src="http://static.zybuluo.com/zhangwen100/qo976u7wbxk8xgvmd8fcmgmo/image_1cml8bq7t3go1moefiirs01uh41g.png" alt="14"><br>  <img src="http://static.zybuluo.com/zhangwen100/sc0hnsjqtpbs940fnx9t7b2k/image_1cml6v59u8p31c501bj97h31d859.png" alt="15"></p>

      
    </div>
    
    
    

    

    

    


<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>




    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"><i class="fa fa-tag"></i> Spark</a>
          
            <a href="/tags/SparkSQL/" rel="tag"><i class="fa fa-tag"></i> SparkSQL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/25/1_Spark 第一天/" rel="next" title="Spark第一天">
                <i class="fa fa-chevron-left"></i> Spark第一天
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/25/1_Spark 第二天之RDD/" rel="prev" title="Spark第二天之RDD">
                Spark第二天之RDD <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80MzQ4MC8yMDAyMA"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/ggg.jpg" alt="Ckm_zz">
            
              <p class="site-author-name" itemprop="name">Ckm_zz</p>
              <p class="site-description motion-element" itemprop="description">—— not too bad luck</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">69</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">31</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">60</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zzckm" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i></a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://www.totree.cn" target="_blank" title="To树商城">
                      
                        <i class="fa fa-fw fa-skype"></i></a>
                  </span>
                
            </div>
          
<!-- 音乐播放器 -->
 
           <div>
              <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="210" height="110" src="//music.163.com/outchain/player?type=2&id=3932159&auto=1&height=66"></iframe>
           </div>
 

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-第三天之Spark-SQL"><span class="nav-number">1.</span> <span class="nav-text">Spark 第三天之Spark SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Spark-SQL概述"><span class="nav-number">1.1.</span> <span class="nav-text">1. Spark SQL概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-什么是Spark-SQL"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1    什么是Spark SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Spark-SQl的数据抽象"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 Spark SQl的数据抽象</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-执行SparkSQL"><span class="nav-number">1.2.</span> <span class="nav-text">2. 执行SparkSQL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-命令式操作"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 命令式操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-代码操作"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 代码操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-数据类型转换"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 数据类型转换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-SQL的执行模式"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 SQL的执行模式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-自定义函数"><span class="nav-number">1.3.</span> <span class="nav-text">3. 自定义函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-UDF函数"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 UDF函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-UDAF函数"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 UDAF函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-开窗函数"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3 开窗函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Spark-SQL的数据源"><span class="nav-number">1.4.</span> <span class="nav-text">4. Spark SQL的数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-与Hive集成"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1 与Hive集成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-输入输出格式"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2 输入输出格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-与数据库交互"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.3 与数据库交互</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Spark-SQL架构原理"><span class="nav-number">1.5.</span> <span class="nav-text">5. Spark SQL架构原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Spark-SQL-的模块"><span class="nav-number">1.5.1.</span> <span class="nav-text">5.1 Spark SQL 的模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Spark-SQL的运行"><span class="nav-number">1.5.2.</span> <span class="nav-text">5.2 Spark SQL的运行</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-案例实战"><span class="nav-number">1.6.</span> <span class="nav-text">6. 案例实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-数据说明"><span class="nav-number">1.6.1.</span> <span class="nav-text">6.1 数据说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-需求"><span class="nav-number">1.6.2.</span> <span class="nav-text">6.2 需求</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">©2019 by ZZckm</span>

  
</div>








  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>






        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      访客量
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>

<!-- 鼠标点击爆炸烟花效果 -->

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":180,"height":330},"mobile":{"show":true},"log":false});</script></body>
</html>
