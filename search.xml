<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F28%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[深入浅出学习]]></title>
    <url>%2F2019%2F07%2F18%2F%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[深入浅出—大数据第1章 小结1、大数据产业生态划分为三个层次：a. 大数据应用：企业开发出来的一些通用应用，例如：大数据可视化和分析数据、大数据商业智能工具或数据服务等 b. 大数据基础设施:指的是PaaS层的基础设施，如数据采集、存储、数据集成、数据并行处理和数据分析等基础的 平台层能力 c. 大数据技术:包括了数据采集、数据存取，数据处理，统计分析，数据挖掘，模型预测，结果呈现等技术。目前，Hadoop已经确立了其作为大数据生态系统基石的地位。且Hadoop是由Java实现的，它可以对分布式环境下的大数据以一种可靠、高效、可伸缩的方式处理 大数据的关键特征关键特征分别是: 数据量大(全球信息量几十万亿GB)、 数据类型多(其中80%是非数据结构化占比)、 处理速度快（达到秒级甚至毫秒级处理速度） 数据价值(数据价值高，价值密度低) 小知识点： 结构化数据：关系型数据库数据以及面向对象数据库中的数据。 半结构化数据：HTML,XML，各类报表等 非结构化数据：音频、视频等 数据思维 传统数据思维，是在在定义问题的时候有一定的假设，然后通过各种方式来证明这一假设是否正确。 大数据思维：是没有预知的假设，而是使用归纳推理的方法，从部分到整体的进行观察描述，通过问题存在的环境观察和解释现象，从而起到预测的效果。 数据处理 传统的数据处理：处理结构化或者关系型数据。且使用昂贵的硬件设备（小型计算机+磁盘列阵）或者一体机来完成数据分析处理挖掘的。 大数据处理是具备结构化、半结构化、非结构化混合处理的能力。数据的采集一般是存储在关系型数据库Oracle、非关系型数据库NoSQL、分布式文件系统HDFS，将这些多种类型的数据可通过并行计算（MapReduce、Spark）来提高数据处理的速度。小结：且大数据的硬件要求不高，使用x86作为分布式平台搭建的基础服务器，搭建高可用后，平台兼容性好，可扩展性高，无需高性能高可靠性的硬件。注：mapReduce是通过大量廉价服务器实现大数据的并行处理，且对数据的一致性不高，扩展性和可用性高，适用于海量的结构化、半结构化和分结构化的混合处理。数据分析. | 传统数据分析 | 大数据分析 |-|-分析对象 | 部分数据的采样 | 全部数据 |分析类型 | 结构化数据 | 结构化、半/非结构化数据 |精确性 | 精准、规范化的数据 | 无特别要求 |分析算法|对算法要求比较高|算法简单有效分析结果|注重因果关系|更注重相关性 数据治理是贯穿整个大数据的生命周期的系统全面的方法。大数据的数据治理包括： 隐私性 安全性 合规性 数据质量 元数据管理 主数据管理 集合业务特点的延伸（具体问题具体分析） 数据维度化：加强大数据应用的安全起源，明确数据认责 强制访问控制的保密要求，适当地屏蔽个人或组织隐私信息等措施。 治理目标：是否明确定义，明确责任方，数据内容是否符合标准要求，数据的存储与管理，数据分析，数据访问安全控制等方面进行制定。 被动安全机制大数据安全不仅限于大数据基础设施的方案，还要考虑到数据采集、大数据分析、大数据基础架构本身的安全。 大数据关键技术数据采集工具介绍FlumeFlume是一个高可用的，高可靠的，分布式的海量日志采集聚合和传输的系统。支持对数据进行简单的处理，并写到各种数据接受方的能力。 source源主要可以从console(控制台)、RPC(Thrift_RPC)、text(文件)、tail、syslog（日志系统支持TCP和UDP）、exec（命令执行）等数据源上收集数据的能力。 ScribeFacebook开源的日志收集系统，主要特点是容错性好，从各类数据源收集的数据，放到一个共享队列里，然后Push到后端的中央存储系统上，当存储系统出现故障后，Scribe可以暂时将数据（日志）存放到本地文件中，待中央存储系统恢复性能后，Scribe把本地日志续传到中央存储系统上。注：Buffer：是最常用的一种存储方式。包含两个子存储，其中一个是主存储（在此指中央存储系统），另一个是辅存储（在此指本地文件），日志数据优先存放在主存储中，如果主存储出现故障，则Scribe会将数据暂存放在辅存储中，待主存储恢复性能后，再将辅存储中的数据复制到主存储中。其中辅存储仅支持两种存储方式，一个是File（本地文件）一个是HDFS。 KafkaKafka是用Scala开发的分布式消息订阅发布系统，主要有三种角色，分别为生产者（producer）,代理（Broker）和数据消费者（Consumer）。生产者向指定Topic发布（Push-推）消息，而Consumer订阅（pull-拉）指定Topic的消息，进而一但这个指定的Topic里面有数据，Broker(代理)就会将数据信息传递给订阅该Topic的所有Consumer。注： Topic有多个partition便于管理数据进行负载均衡。 Time tunnel是阿里巴巴的一个高效的、可靠的、可扩展的的实时数据传输平台，支持消息多用户订阅。主要功能就是实时完成海量数据的交换，因此它的业务逻辑主要有两个，一个是将发布数据至Time Tunnel,一个是从Time Tunnel订阅数据。组件介绍：Time Tunnel大概由TT Manager、Client、Router、Zookeeper和Broker几个部分组成。 TT Manager:对外提供队列申请、删除、查询和集群的管理接口：对内 故障发现，发起队列迁移。 Client:是一组访问Time Tunnel的API，主要由安全认证API、发布API和订阅API(目前api支持JAva、Python、PHP) Router是访问Time Tunnel的门户，负责路由、安全认证和负载均衡，为客户端提供路由信息，找到为消息队列提供服务的Broker. Zookeeper是Hadoop的开源项目，主要功能是状态同步，Borker和Clienter的状态都存储在这里。 Broker是Time Tunnel的核心，负责消息的存储转发，承担实际流量，进行消息队列的读写。 操作流程： Client访问Time Tunnel的第一步是向Router进行安全认证，如果认证通过，Router会根据Client要发布或者要订阅的topic对Client进行路由（负载均衡保证让所有的broker平均地接收Client访问），使的Client和Broker进行连接。 Chukwa是一个开源的用于监控大型分布式系统的数据收集系统，是构建在Hadoop的HDFS和Map/Reduce框架之上的。包含一个灵活的工具集，用于展示、监控和分析以收集的数据。 大数据存储与管理技术 . 结构化数据 半结构化数据 非结构化数据 定义 有数据结构描述信息的数据 介于完全结构化数据和完全非结构化数据之间的数据 无固定结构来表现的数据 特点 先有结构，再有数据 先有数据再有结构 只有数据，没有结构 存储 分布式关系型数据库 分布式文件系统 分布式非关系型数据库 #### 分布式文件系统 1. Consistency(一致性)：文件系统中的所有数据备份在同一时间是否是同样的值。 2. Availability(可用性)：部分出错，是否会影响其他大部分的运作。 3. Partition Tolerance(分区容错性)：约束了一个分布式系统需要具有如下特性，分布式在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。 未完待续。。。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python学习第一轮]]></title>
    <url>%2F2019%2F07%2F09%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%BD%AE%2F</url>
    <content type="text"></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论坛项目总结]]></title>
    <url>%2F2019%2F05%2F30%2F%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[电商项目总结]]></title>
    <url>%2F2019%2F05%2F30%2F%E7%94%B5%E5%95%86%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Springboot-论坛项目知识点小结]]></title>
    <url>%2F2019%2F05%2F28%2FSpringboot-%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E7%9F%A5%E8%AF%86%E7%82%B9%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-论坛项目知识点小结]]></title>
    <url>%2F2019%2F05%2F28%2FSpringCloud-%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E7%9F%A5%E8%AF%86%E7%82%B9%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[基于SPringCloud论坛项目的知识点总结]]></content>
      <categories>
        <category>SpringCloud框架</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>Springboot</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot入门学习-sgg]]></title>
    <url>%2F2019%2F04%2F26%2FSpringBootsgg%2F</url>
    <content type="text"><![CDATA[一、Spring Boot 入门1、Spring Boot 简介 简化Spring应用开发的一个框架； 整个Spring技术栈的一个大整合； J2EE开发的一站式解决方案； 2、微服务2014，martin fowler 微服务：架构风格（服务微化） 一个应用应该是一组小型服务；可以通过HTTP的方式进行互通； 单体应用：ALL IN ONE 微服务：每一个功能元素最终都是一个可独立替换和独立升级的软件单元； 详细参照微服务文档 3、环境准备http://www.gulixueyuan.com/ 谷粒学院 环境约束 –jdk1.8：Spring Boot 推荐jdk1.7及以上；java version “1.8.0_112” –maven3.x：maven 3.3以上版本；Apache Maven 3.3.9 –IntelliJIDEA2017：IntelliJ IDEA 2017.2.2 x64、STS –SpringBoot 1.5.9.RELEASE：1.5.9； 统一环境； 1、MAVEN设置；给maven 的settings.xml配置文件的profiles标签添加 123456789101112&lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt;&lt;/profile&gt; 2、IDEA设置整合maven进来； 4、Spring Boot HelloWorld一个功能： 浏览器发送hello请求，服务器接受请求并处理，响应Hello World字符串； 1、创建一个maven工程；（jar）2、导入spring boot相关的依赖1234567891011&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3、编写一个主程序；启动Spring Boot应用12345678910111213/** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */@SpringBootApplicationpublic class HelloWorldMainApplication &#123; public static void main(String[] args) &#123; // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); &#125;&#125; 4、编写相关的Controller、Service123456789@Controllerpublic class HelloController &#123; @ResponseBody @RequestMapping("/hello") public String hello()&#123; return "Hello World!"; &#125;&#125; 5、运行主程序测试6、简化部署123456789&lt;!-- 这个插件，可以将应用打包成一个可执行的jar包；--&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 将这个应用打成jar包，直接使用java -jar的命令进行执行； 5、Hello World探究1、POM文件1、父项目1234567891011121314&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt;他的父项目是&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt;&lt;/parent&gt;他来真正管理Spring Boot应用里面的所有依赖版本； Spring Boot的版本仲裁中心； 以后我们导入依赖默认是不需要写版本；（没有在dependencies里面管理的依赖自然需要声明版本号） 2、启动器1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; spring-boot-starter-==web==： ​ spring-boot-starter：spring-boot场景启动器；帮我们导入了web模块正常运行所依赖的组件； Spring Boot将所有的功能场景都抽取出来，做成一个个的starters（启动器），只需要在项目里面引入这些starter相关场景的所有依赖都会导入进来。要用什么功能就导入什么场景的启动器 2、主程序类，主入口类123456789101112/** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */@SpringBootApplicationpublic class HelloWorldMainApplication &#123; public static void main(String[] args) &#123; // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); &#125;&#125; @SpringBootApplication: Spring Boot应用标注在某个类上说明这个类是SpringBoot的主配置类，SpringBoot就应该运行这个类的main方法来启动SpringBoot应用； 12345678910@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123; @SpringBootConfiguration:Spring Boot的配置类； ​ 标注在某个类上，表示这是一个Spring Boot的配置类； ​ @Configuration:配置类上来标注这个注解； ​ 配置类 —– 配置文件；配置类也是容器中的一个组件；@Component @EnableAutoConfiguration：开启自动配置功能； ​ 以前我们需要配置的东西，Spring Boot帮我们自动配置；@EnableAutoConfiguration告诉SpringBoot开启自动配置功能；这样自动配置才能生效； 123@AutoConfigurationPackage@Import(EnableAutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123; ​ @AutoConfigurationPackage：自动配置包 ​ @Import(AutoConfigurationPackages.Registrar.class)： ​ Spring的底层注解@Import，给容器中导入一个组件；导入的组件由AutoConfigurationPackages.Registrar.class； ==将主配置类（@SpringBootApplication标注的类）的所在包及下面所有子包里面的所有组件扫描到Spring容器；== ​ @Import(EnableAutoConfigurationImportSelector.class)； ​ 给容器中导入组件？ ​ EnableAutoConfigurationImportSelector：导入哪些组件的选择器； ​ 将所有需要导入的组件以全类名的方式返回；这些组件就会被添加到容器中； ​ 会给容器中导入非常多的自动配置类（xxxAutoConfiguration）；就是给容器中导入这个场景需要的所有组件，并配置好这些组件； 有了自动配置类，免去了我们手动编写配置注入功能组件等的工作； ​ SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class,classLoader)； ==Spring Boot在启动的时候从类路径下的META-INF/spring.factories中获取EnableAutoConfiguration指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置工作；==以前我们需要自己配置的东西，自动配置类都帮我们； J2EE的整体整合解决方案和自动配置都在spring-boot-autoconfigure-1.5.9.RELEASE.jar； ​ ==Spring注解版（谷粒学院）== 6、使用Spring Initializer快速创建Spring Boot项目1、IDEA：使用 Spring Initializer快速创建项目IDE都支持使用Spring的项目创建向导快速创建一个Spring Boot项目； 选择我们需要的模块；向导会联网创建Spring Boot项目； 默认生成的Spring Boot项目； 主程序已经生成好了，我们只需要我们自己的逻辑 resources文件夹中目录结构 static：保存所有的静态资源； js css images； templates：保存所有的模板页面；（Spring Boot默认jar包使用嵌入式的Tomcat，默认不支持JSP页面）；可以使用模板引擎（freemarker、thymeleaf）； application.properties：Spring Boot应用的配置文件；可以修改一些默认设置； 2、STS使用 Spring Starter Project快速创建项目 二、配置文件1、配置文件SpringBoot使用一个全局的配置文件，配置文件名是固定的； •application.properties •application.yml 配置文件的作用：修改SpringBoot自动配置的默认值；SpringBoot在底层都给我们自动配置好； YAML（YAML Ain’t Markup Language） ​ YAML A Markup Language：是一个标记语言 ​ YAML isn’t Markup Language：不是一个标记语言； 标记语言： ​ 以前的配置文件；大多都使用的是 xxxx.xml文件； ​ YAML：以数据为中心，比json、xml等更适合做配置文件； ​ YAML：配置例子 12server: port: 8081 ​ XML： 123&lt;server&gt; &lt;port&gt;8081&lt;/port&gt;&lt;/server&gt; 2、YAML语法：1、基本语法k:(空格)v：表示一对键值对（空格必须有）； 以空格的缩进来控制层级关系；只要是左对齐的一列数据，都是同一个层级的 123server: port: 8081 path: /hello 属性和值也是大小写敏感； 2、值的写法字面量：普通的值（数字，字符串，布尔）​ k: v：字面直接来写； ​ 字符串默认不用加上单引号或者双引号； ​ “”：双引号；不会转义字符串里面的特殊字符；特殊字符会作为本身想表示的意思 ​ name: “zhangsan \n lisi”：输出；zhangsan 换行 lisi ​ ‘’：单引号；会转义特殊字符，特殊字符最终只是一个普通的字符串数据 ​ name: ‘zhangsan \n lisi’：输出；zhangsan \n lisi 对象、Map（属性和值）（键值对）：​ k: v：在下一行来写对象的属性和值的关系；注意缩进 ​ 对象还是k: v的方式 123friends: lastName: zhangsan age: 20 行内写法： 1friends: &#123;lastName: zhangsan,age: 18&#125; 数组（List、Set）：用- 值表示数组中的一个元素 1234pets: - cat - dog - pig 行内写法 1pets: [cat,dog,pig] 3、配置文件值注入配置文件 123456789101112person: lastName: hello age: 18 boss: false birth: 2017/12/12 maps: &#123;k1: v1,k2: 12&#125; lists: - lisi - zhaoliu dog: name: 小狗 age: 12 javaBean： 1234567891011121314151617181920/** * 将配置文件中配置的每一个属性的值，映射到这个组件中 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定； * prefix = "person"：配置文件中哪个下面的所有属性进行一一映射 * * 只有这个组件是容器中的组件，才能容器提供的@ConfigurationProperties功能； * */@Component@ConfigurationProperties(prefix = "person")public class Person &#123; private String lastName; private Integer age; private Boolean boss; private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog; 我们可以导入配置文件处理器，以后编写配置就有提示了 123456&lt;!--导入配置文件处理器，配置文件进行绑定就会有提示--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; 1、properties配置文件在idea中默认utf-8可能会乱码调整乱码 2、@Value获取值和@ConfigurationProperties获取值比较 @ConfigurationProperties @Value 功能 批量注入配置文件中的属性 一个个指定 松散绑定（松散语法） 支持 不支持 SpEL 不支持 支持 JSR303数据校验 支持 不支持 复杂类型封装 支持 不支持 配置文件yml还是properties他们都能获取到值； 如果说，我们只是在某个业务逻辑中需要获取一下配置文件中的某项值，使用@Value； 如果说，我们专门编写了一个javaBean来和配置文件进行映射，我们就直接使用@ConfigurationProperties； 3、配置文件注入值数据校验123456789101112131415161718192021222324@Component@ConfigurationProperties(prefix = "person")@Validatedpublic class Person &#123; /** * &lt;bean class="Person"&gt; * &lt;property name="lastName" value="字面量/$&#123;key&#125;从环境变量、配置文件中获取值/#&#123;SpEL&#125;"&gt;&lt;/property&gt; * &lt;bean/&gt; */ //lastName必须是邮箱格式 @Email //@Value("$&#123;person.last-name&#125;") private String lastName; //@Value("#&#123;11*2&#125;") private Integer age; //@Value("true") private Boolean boss; private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog; 4、@PropertySource&amp;@ImportResource&amp;@Bean@PropertySource：加载指定的配置文件； 1234567891011121314151617181920212223242526272829/** * 将配置文件中配置的每一个属性的值，映射到这个组件中 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定； * prefix = "person"：配置文件中哪个下面的所有属性进行一一映射 * * 只有这个组件是容器中的组件，才能容器提供的@ConfigurationProperties功能； * @ConfigurationProperties(prefix = "person")默认从全局配置文件中获取值； * */@PropertySource(value = &#123;"classpath:person.properties"&#125;)@Component@ConfigurationProperties(prefix = "person")//@Validatedpublic class Person &#123; /** * &lt;bean class="Person"&gt; * &lt;property name="lastName" value="字面量/$&#123;key&#125;从环境变量、配置文件中获取值/#&#123;SpEL&#125;"&gt;&lt;/property&gt; * &lt;bean/&gt; */ //lastName必须是邮箱格式 // @Email //@Value("$&#123;person.last-name&#125;") private String lastName; //@Value("#&#123;11*2&#125;") private Integer age; //@Value("true") private Boolean boss; @ImportResource：导入Spring的配置文件，让配置文件里面的内容生效； Spring Boot里面没有Spring的配置文件，我们自己编写的配置文件，也不能自动识别； 想让Spring的配置文件生效，加载进来；@ImportResource标注在一个配置类上 12@ImportResource(locations = &#123;"classpath:beans.xml"&#125;)导入Spring的配置文件让其生效 不来编写Spring的配置文件 12345678&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="helloService" class="com.atguigu.springboot.service.HelloService"&gt;&lt;/bean&gt;&lt;/beans&gt; SpringBoot推荐给容器中添加组件的方式；推荐使用全注解的方式 1、配置类@Configuration——&gt;Spring配置文件 2、使用@Bean给容器中添加组件 12345678910111213141516/** * @Configuration：指明当前类是一个配置类；就是来替代之前的Spring配置文件 * * 在配置文件中用&lt;bean&gt;&lt;bean/&gt;标签添加组件 * */@Configurationpublic class MyAppConfig &#123; //将方法的返回值添加到容器中；容器中这个组件默认的id就是方法名 @Bean public HelloService helloService02()&#123; System.out.println("配置类@Bean给容器中添加组件了..."); return new HelloService(); &#125;&#125; ##4、配置文件占位符 1、随机数12$&#123;random.value&#125;、$&#123;random.int&#125;、$&#123;random.long&#125;$&#123;random.int(10)&#125;、$&#123;random.int[1024,65536]&#125; 2、占位符获取之前配置的值，如果没有可以是用:指定默认值123456789person.last-name=张三$&#123;random.uuid&#125;person.age=$&#123;random.int&#125;person.birth=2017/12/15person.boss=falseperson.maps.k1=v1person.maps.k2=14person.lists=a,b,cperson.dog.name=$&#123;person.hello:hello&#125;_dogperson.dog.age=15 5、Profile(spring.profile.active={profile},指定运行哪一个配置文件) 1、多Profile文件我们在主配置文件编写的时候，文件名可以是 application-{profile}.properties/yml{profile}是作为标识，可以自定义名称 默认使用application.properties的配置； 2、yml支持多文档块方式12345678910111213141516171819server: port: 8081spring: profiles: active: prod---server: port: 8083spring: profiles: dev---server: port: 8084spring: profiles: prod #指定属于哪个环境 3、激活指定profile​ 1、在配置文件中指定 spring.profiles.active=dev ​ 2、命令项目内可以Edit Configuration中添加命令行运行可以直接在测试的时候，配置传入命令行参数maven打包好jar包后在cmd中​ java -jar spring-boot-02-config-0.0.1-SNAPSHOT.jar –spring.profiles.active=dev；​ ​ 3、虚拟机参数；上图的VM option中 ​ -Dspring.profiles.active=dev 6、配置文件加载位置springboot 启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置 项目内 读取配置文件的优先级别是，springBoot都会读取，同样的配置按优先级使用，不同的配置会互补使用 根目录下config文件夹下的配置文件 根目录下的配置文件 resource下的config下的配置文件 resource 下的配置文件 与上述一一对应– file:./config/– file:./– classpath:/config/– classpath:/ 优先级由高到底，高优先级的配置会覆盖低优先级的配置；SpringBoot会从这四个位置全部加载主配置文件；互补配置； ==我们还可以通过spring.config.location来改变默认的配置文件位置== 项目打包好以后，我们可以使用命令行参数的形式，启动项目的时候来指定配置文件的新位置；指定配置文件和默认加载的这些配置文件共同起作用形成互补配置； java -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar –spring.config.location=G:/application.properties 7、外部配置加载顺序==SpringBoot也可以从以下位置加载配置； 优先级从高到低；高优先级的配置覆盖低优先级的配置，所有的配置会形成互补配置== 1.命令行参数 所有的配置都可以在命令行上进行指定 java -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar –server.port=8087 –server.context-path=/abc 多个配置用空格分开； –配置项=值 2.来自java:comp/env的JNDI属性 3.Java系统属性（System.getProperties()） 4.操作系统环境变量 5.RandomValuePropertySource配置的random.*属性值 ==由jar包外向jar包内进行寻找；== ==优先加载带profile== 6.jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件 7.jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件 ==再来加载不带profile== 8.jar包外部的application.properties或application.yml(不带spring.profile)配置文件 9.jar包内部的application.properties或application.yml(不带spring.profile)配置文件 10.@Configuration注解类上的@PropertySource 11.通过SpringApplication.setDefaultProperties指定的默认属性 所有支持的配置加载来源； 参考官方文档 8、自动配置原理配置文件到底能写什么？怎么写？自动配置原理； 配置文件能配置的属性参照 1、自动配置原理：1）、SpringBoot启动的时候加载主配置类，开启了自动配置功能 ==@EnableAutoConfiguration== 2）、@EnableAutoConfiguration 作用： 利用EnableAutoConfigurationImportSelector给容器中导入一些组件？ 可以查看selectImports()方法的内容； List configurations = getCandidateConfigurations(annotationMetadata, attributes);获取候选的配置 1234SpringFactoriesLoader.loadFactoryNames()扫描所有jar包类路径下 META-INF/spring.factories把扫描到的这些文件的内容包装成properties对象从properties中获取到EnableAutoConfiguration.class类（类名）对应的值，然后把他们添加在容器中 ​ ==将 类路径下 META-INF/spring.factories 里面配置的所有EnableAutoConfiguration的值加入到了容器中；== 也就是说自动配置就是在运行时，由于加入了@SpringBootApplication注解，该注解里面的的底层里有一个方法，就是扫描所有jar包寻找EnableAutoConfiguration.class类（类名）对应的值，且把他们添加到容器中运行，从而达到了自动配置的效果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\org.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\org.springframework.boot.autoconfigure.dao.PersistenceExceptionTranslationAutoConfiguration,\org.springframework.boot.autoconfigure.data.cassandra.CassandraDataAutoConfiguration,\org.springframework.boot.autoconfigure.data.cassandra.CassandraRepositoriesAutoConfiguration,\org.springframework.boot.autoconfigure.data.couchbase.CouchbaseDataAutoConfiguration,\org.springframework.boot.autoconfigure.data.couchbase.CouchbaseRepositoriesAutoConfiguration,\org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchAutoConfiguration,\org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchDataAutoConfiguration,\org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchRepositoriesAutoConfiguration,\org.springframework.boot.autoconfigure.data.jpa.JpaRepositoriesAutoConfiguration,\org.springframework.boot.autoconfigure.data.ldap.LdapDataAutoConfiguration,\org.springframework.boot.autoconfigure.data.ldap.LdapRepositoriesAutoConfiguration,\org.springframework.boot.autoconfigure.data.mongo.MongoDataAutoConfiguration,\org.springframework.boot.autoconfigure.data.mongo.MongoRepositoriesAutoConfiguration,\org.springframework.boot.autoconfigure.data.neo4j.Neo4jDataAutoConfiguration,\org.springframework.boot.autoconfigure.data.neo4j.Neo4jRepositoriesAutoConfiguration,\org.springframework.boot.autoconfigure.data.solr.SolrRepositoriesAutoConfiguration,\org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration,\org.springframework.boot.autoconfigure.data.redis.RedisRepositoriesAutoConfiguration,\org.springframework.boot.autoconfigure.data.rest.RepositoryRestMvcAutoConfiguration,\org.springframework.boot.autoconfigure.data.web.SpringDataWebAutoConfiguration,\org.springframework.boot.autoconfigure.elasticsearch.jest.JestAutoConfiguration,\org.springframework.boot.autoconfigure.freemarker.FreeMarkerAutoConfiguration,\org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration,\org.springframework.boot.autoconfigure.h2.H2ConsoleAutoConfiguration,\org.springframework.boot.autoconfigure.hateoas.HypermediaAutoConfiguration,\org.springframework.boot.autoconfigure.hazelcast.HazelcastAutoConfiguration,\org.springframework.boot.autoconfigure.hazelcast.HazelcastJpaDependencyAutoConfiguration,\org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration,\org.springframework.boot.autoconfigure.integration.IntegrationAutoConfiguration,\org.springframework.boot.autoconfigure.jackson.JacksonAutoConfiguration,\org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\org.springframework.boot.autoconfigure.jdbc.JdbcTemplateAutoConfiguration,\org.springframework.boot.autoconfigure.jdbc.JndiDataSourceAutoConfiguration,\org.springframework.boot.autoconfigure.jdbc.XADataSourceAutoConfiguration,\org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,\org.springframework.boot.autoconfigure.jms.JmsAutoConfiguration,\org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration,\org.springframework.boot.autoconfigure.jms.JndiConnectionFactoryAutoConfiguration,\org.springframework.boot.autoconfigure.jms.activemq.ActiveMQAutoConfiguration,\org.springframework.boot.autoconfigure.jms.artemis.ArtemisAutoConfiguration,\org.springframework.boot.autoconfigure.flyway.FlywayAutoConfiguration,\org.springframework.boot.autoconfigure.groovy.template.GroovyTemplateAutoConfiguration,\org.springframework.boot.autoconfigure.jersey.JerseyAutoConfiguration,\org.springframework.boot.autoconfigure.jooq.JooqAutoConfiguration,\org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration,\org.springframework.boot.autoconfigure.ldap.embedded.EmbeddedLdapAutoConfiguration,\org.springframework.boot.autoconfigure.ldap.LdapAutoConfiguration,\org.springframework.boot.autoconfigure.liquibase.LiquibaseAutoConfiguration,\org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration,\org.springframework.boot.autoconfigure.mail.MailSenderValidatorAutoConfiguration,\org.springframework.boot.autoconfigure.mobile.DeviceResolverAutoConfiguration,\org.springframework.boot.autoconfigure.mobile.DeviceDelegatingViewResolverAutoConfiguration,\org.springframework.boot.autoconfigure.mobile.SitePreferenceAutoConfiguration,\org.springframework.boot.autoconfigure.mongo.embedded.EmbeddedMongoAutoConfiguration,\org.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\org.springframework.boot.autoconfigure.mustache.MustacheAutoConfiguration,\org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration,\org.springframework.boot.autoconfigure.reactor.ReactorAutoConfiguration,\org.springframework.boot.autoconfigure.security.SecurityAutoConfiguration,\org.springframework.boot.autoconfigure.security.SecurityFilterAutoConfiguration,\org.springframework.boot.autoconfigure.security.FallbackWebSecurityAutoConfiguration,\org.springframework.boot.autoconfigure.security.oauth2.OAuth2AutoConfiguration,\org.springframework.boot.autoconfigure.sendgrid.SendGridAutoConfiguration,\org.springframework.boot.autoconfigure.session.SessionAutoConfiguration,\org.springframework.boot.autoconfigure.social.SocialWebAutoConfiguration,\org.springframework.boot.autoconfigure.social.FacebookAutoConfiguration,\org.springframework.boot.autoconfigure.social.LinkedInAutoConfiguration,\org.springframework.boot.autoconfigure.social.TwitterAutoConfiguration,\org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration,\org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration,\org.springframework.boot.autoconfigure.transaction.TransactionAutoConfiguration,\org.springframework.boot.autoconfigure.transaction.jta.JtaAutoConfiguration,\org.springframework.boot.autoconfigure.validation.ValidationAutoConfiguration,\org.springframework.boot.autoconfigure.web.DispatcherServletAutoConfiguration,\org.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfiguration,\org.springframework.boot.autoconfigure.web.ErrorMvcAutoConfiguration,\org.springframework.boot.autoconfigure.web.HttpEncodingAutoConfiguration,\org.springframework.boot.autoconfigure.web.HttpMessageConvertersAutoConfiguration,\org.springframework.boot.autoconfigure.web.MultipartAutoConfiguration,\org.springframework.boot.autoconfigure.web.ServerPropertiesAutoConfiguration,\org.springframework.boot.autoconfigure.web.WebClientAutoConfiguration,\org.springframework.boot.autoconfigure.web.WebMvcAutoConfiguration,\org.springframework.boot.autoconfigure.websocket.WebSocketAutoConfiguration,\org.springframework.boot.autoconfigure.websocket.WebSocketMessagingAutoConfiguration,\org.springframework.boot.autoconfigure.webservices.WebServicesAutoConfiguration 每一个这样的 xxxAutoConfiguration类都是容器中的一个组件，都加入到容器中；用他们来做自动配置； 3）、每一个自动配置类进行自动配置功能； 4）、以HttpEncodingAutoConfiguration（Http编码自动配置）为例解释自动配置原理； 12345678910111213141516171819202122232425262728@Configuration //表示这是一个配置类，以前编写的配置文件一样，也可以给容器中添加组件@EnableConfigurationProperties(HttpEncodingProperties.class) //启动指定类的ConfigurationProperties功能；将配置文件中对应的值和HttpEncodingProperties绑定起来；并把HttpEncodingProperties加入到ioc容器中@ConditionalOnWebApplication //Spring底层@Conditional注解（Spring注解版），根据不同的条件，如果满足指定的条件，整个配置类里面的配置就会生效； 判断当前应用是否是web应用，如果是，当前配置类生效@ConditionalOnClass(CharacterEncodingFilter.class) //判断当前项目有没有这个类CharacterEncodingFilter；SpringMVC中进行乱码解决的过滤器；@ConditionalOnProperty(prefix = "spring.http.encoding", value = "enabled", matchIfMissing = true) //判断配置文件中是否存在某个配置 spring.http.encoding.enabled；如果不存在，判断也是成立的//即使我们配置文件中不配置pring.http.encoding.enabled=true，也是默认生效的；public class HttpEncodingAutoConfiguration &#123; //他已经和SpringBoot的配置文件映射了 private final HttpEncodingProperties properties; //只有一个有参构造器的情况下，参数的值就会从容器中拿 public HttpEncodingAutoConfiguration(HttpEncodingProperties properties) &#123; this.properties = properties; &#125; @Bean //给容器中添加一个组件，这个组件的某些值需要从properties中获取 @ConditionalOnMissingBean(CharacterEncodingFilter.class) //判断容器没有这个组件？ public CharacterEncodingFilter characterEncodingFilter() &#123; CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter(); filter.setEncoding(this.properties.getCharset().name()); filter.setForceRequestEncoding(this.properties.shouldForce(Type.REQUEST)); filter.setForceResponseEncoding(this.properties.shouldForce(Type.RESPONSE)); return filter; &#125; 根据当前不同的条件判断，决定这个配置类是否生效？ 一但这个配置类生效；这个配置类就会给容器中添加各种组件；这些组件的属性是从对应的properties类中获取的，这些类里面的每一个属性又是和配置文件绑定的； 5）、所有在配置文件中能配置的属性都是在xxxxProperties类中封装者‘；配置文件能配置什么就可以参照某个功能对应的这个属性类 1234@ConfigurationProperties(prefix = "spring.http.encoding") //从配置文件中获取指定的值和bean的属性进行绑定public class HttpEncodingProperties &#123; public static final Charset DEFAULT_CHARSET = Charset.forName("UTF-8"); 精髓： ​ 1）、SpringBoot启动会加载大量的自动配置类 ​ 2）、我们看我们需要的功能有没有SpringBoot默认写好的自动配置类； ​ 3）、我们再来看这个自动配置类中到底配置了哪些组件；（只要我们要用的组件有，我们就不需要再来配置了） ​ 4）、给容器中自动配置类添加组件的时候，会从properties类中获取某些属性。我们就可以在配置文件中指定这些属性的值； xxxxAutoConfigurartion：自动配置类； 给容器中添加组件 xxxxProperties:封装配置文件中相关属性； 2、细节1、@Conditional派生注解（Spring注解版原生的@Conditional作用）作用：必须是@Conditional指定的条件成立，才给容器中添加组件，配置配里面的所有内容才生效； @Conditional扩展注解 作用（判断是否满足当前指定条件） @ConditionalOnJava 系统的java版本是否符合要求 @ConditionalOnBean 容器中存在指定Bean； @ConditionalOnMissingBean 容器中不存在指定Bean； @ConditionalOnExpression 满足SpEL表达式指定 @ConditionalOnClass 系统中有指定的类 @ConditionalOnMissingClass 系统中没有指定的类 @ConditionalOnSingleCandidate 容器中只有一个指定的Bean，或者这个Bean是首选Bean @ConditionalOnProperty 系统中指定的属性是否有指定的值 @ConditionalOnResource 类路径下是否存在指定资源文件 @ConditionalOnWebApplication 当前是web环境 @ConditionalOnNotWebApplication 当前不是web环境 @ConditionalOnJndi JNDI存在指定项 自动配置类必须在一定的条件下才能生效； 我们怎么知道哪些自动配置类生效； ==我们可以通过在配置文件加一句 debug=true属性；运行后，来让控制台打印自动配置报告==，这样我们就可以很方便的知道哪些自动配置类生效； 1234567891011121314151617181920212223=========================AUTO-CONFIGURATION REPO=========================Positive matches:（自动配置类启用的）----------------- DispatcherServletAutoConfiguration matched: - @ConditionalOnClass found required class 'org.springframework.web.servlet.DispatcherServlet'; @ConditionalOnMissingClass did not find unwanted class (OnClassCondition) - @ConditionalOnWebApplication (required) found StandardServletEnvironment (OnWebApplicationCondition) Negative matches:（没有启动，没有匹配成功的自动配置类）----------------- ActiveMQAutoConfiguration: Did not match: - @ConditionalOnClass did not find required classes 'javax.jms.ConnectionFactory', 'org.apache.activemq.ActiveMQConnectionFactory' (OnClassCondition) AopAutoConfiguration: Did not match: - @ConditionalOnClass did not find required classes 'org.aspectj.lang.annotation.Aspect', 'org.aspectj.lang.reflect.Advice' (OnClassCondition) 三、日志1、日志框架 小张；开发一个大型系统； ​ 1、System.out.println(“”)；将关键数据打印在控制台；去掉？写在一个文件？ ​ 2、框架来记录系统的一些运行时信息；日志框架 ； zhanglogging.jar； ​ 3、高大上的几个功能？异步模式？自动归档？xxxx？ zhanglogging-good.jar？ ​ 4、将以前框架卸下来？换上新的框架，重新修改之前相关的API；zhanglogging-prefect.jar； ​ 5、JDBC—数据库驱动； ​ 写了一个统一的接口层；日志门面（日志的一个抽象层）；logging-abstract.jar； ​ 给项目中导入具体的日志实现就行了；我们之前的日志框架都是实现的抽象层； 市面上的日志框架； JUL、JCL、Jboss-logging、logback、log4j、log4j2、slf4j…. 日志门面 （日志的抽象层） 日志实现 JCL（Jakarta Commons Logging） SLF4j（Simple Logging Facade for Java） jboss-logging Log4j JUL（java.util.logging） Log4j2 Logback 左边选一个门面（抽象层）、右边来选一个实现； 日志门面： （抽象层） SLF4J； 日志实现：Logback； SpringBoot：底层是Spring框架，Spring框架默认是用JCL；‘ ​ *==SpringBoot选用 SLF4j和logback；== 2、SLF4j使用1、如何在系统中使用SLF4j https://www.slf4j.org以后开发的时候，日志记录方法的调用，不应该来直接调用日志的实现类，而是调用日志抽象层里面的方法； 给系统里面导入slf4j的jar和 logback的实现jar 123456789import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class HelloWorld &#123; public static void main(String[] args) &#123; Logger logger = LoggerFactory.getLogger(HelloWorld.class); logger.info("Hello World"); &#125;&#125; 图示SLF$4J与其他日志实现的关系图 每一个日志的实现框架都有自己的配置文件。使用slf4j以后，配置文件还是做成日志实现框架自己本身的配置文件，连接的话不能直接连接可以使用适配层进行中间件连接； 2、遗留问题a（slf4j+logback）: Spring（commons-logging）、Hibernate（jboss-logging）、MyBatis、xxxx 统一日志记录，即使是别的框架和我一起统一使用slf4j进行输出？ 如何让系统中所有的日志都统一到slf4j； ==1、将系统中其他日志框架先排除出去；== ==2、用中间包来替换原有的日志框架；== ==3、我们导入slf4j要使用的其他的日志实现（如使用的日志实现是log4j其他框架的就都使用log4j的ja）== 3、SpringBoot日志关系1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; SpringBoot使用它来做日志功能； 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/dependency&gt; 底层依赖关系 总结： ​ 1）、SpringBoot底层也是使用slf4j+logback的方式进行日志记录 ​ 2）、SpringBoot也把其他的日志都替换成了slf4j； ​ 3）、中间替换包？ 以下为举例，就是中间包的替换，虽然包名看上去是common.logging的，但里面类的实却是 LogFactory logFactory = new SLF4JLogFactory(); 123456@SuppressWarnings("rawtypes")public abstract class LogFactory &#123; static String UNSUPPORTED_OPERATION_IN_JCL_OVER_SLF4J = "http://www.slf4j.org/codes.html#unsupported_operation_in_jcl_over_slf4j"; static LogFactory logFactory = new SLF4JLogFactory(); ​ 4）、如果我们要引入其他框架？一定要把这个框架的默认日志依赖移除掉？ ​ Spring框架用的是commons-logging； 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; ==SpringBoot能自动适配所有的日志，而且底层使用slf4j+logback的方式记录日志，引入其他框架的时候，只需要把这个框架依赖的日志框架排除掉即可；== 4、日志使用；1、默认配置SpringBoot默认帮我们配置好了日志； 12345678910111213141516171819202122232425//记录器Logger logger = LoggerFactory.getLogger(getClass());@Testpublic void contextLoads() &#123; //System.out.println(); //日志的级别； //由低到高 trace&lt;debug&lt;info&lt;warn&lt;error //可以调整输出的日志级别；日志就只会个级别以以后的高级别生效 // 跟踪轨迹-如果运行时需要跟踪信息的时候使用 logger.trace("这是trace日志..."); //调试 logger.debug("这是debug日志..."); //SpringBoot默认给我们使用的是info级别的，没有指定级别的就用SpringBoot默认规定的级别；root级别 // 默认使用的是info级别，就是说比info高，包括info的才会输出 //logging.level.com.example.sgg_springboot=trace 设置了就会以此为级别，其与其更高的才回输出 //自定义 logger.info("这是info日志..."); //警告-返回值不是预期的就可以警告 logger.warn("这是warn日志..."); //错误-方法捕获异常，用日志记录下 logger.error("这是error日志...");&#125; 日志输出格式： %d表示日期时间， %thread表示线程名， %-5level：级别从左显示5个字符宽度 %logger{50} 表示logger名字最长50个字符，否则按照句点分割。 %msg：日志消息， %n是换行符 --&gt; %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%nSpringBoot修改日志的默认配置 1234567891011121314logging.level.com.atguigu=trace# 生成springboot.log日志# 可以指定完整的路径；#logging.file=G:/springboot.log# 在当前磁盘的根路径下创建spring文件夹和里面的log文件夹；以及再里面的 spring.log(默认)logging.path=/spring/log# 在控制台输出的日志的格式logging.pattern.console=%d&#123;yyyy-MM-dd&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n# 指定文件中日志输出的格式logging.pattern.file=%d&#123;yyyy-MM-dd&#125; === [%thread] === %-5level === %logger&#123;50&#125; ==== %msg%n logging.file logging.path Example Description (none) (none) 只在控制台输出 指定文件名 (none) my.log 输出日志到my.log文件 (none) 指定目录 /var/log 输出到指定目录的 spring.log 文件中 2、指定配置给类路径下放上每个日志框架自己的配置文件即可；SpringBoot就不使用他默认配置的了 Logging System Customization Logback logback-spring.xml, logback-spring.groovy, logback.xml or logback.groovy Log4j2 log4j2-spring.xml or log4j2.xml JDK (Java Util Logging) logging.properties logback.xml：直接就被日志框架识别了； logback-spring.xml：日志框架就不直接加载日志的配置项，由SpringBoot解析加载日志配置，虽然不直接识别了，但可以使用SpringBoot的高级Profile功能 1234&lt;springProfile name="staging"&gt; &lt;!-- configuration to be enabled when the "staging" profile is active --&gt; 可以指定某段配置只在某个环境下生效&lt;/springProfile&gt; 如： 12345678910111213141516171819&lt;appender name="stdout" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;!-- 日志输出格式： %d表示日期时间， %thread表示线程名， %-5level：级别从左显示5个字符宽度 %logger&#123;50&#125; 表示logger名字最长50个字符，否则按照句点分割。 %msg：日志消息， %n是换行符 --&gt; &lt;layout class="ch.qos.logback.classic.PatternLayout"&gt; &lt;springProfile name="dev"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; ----&gt; [%thread] ---&gt; %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;/springProfile&gt; &lt;springProfile name="!dev"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; ==== [%thread] ==== %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;/springProfile&gt; &lt;/layout&gt; &lt;/appender&gt; 如果使用logback.xml作为日志配置文件，还要使用profile功能，会有以下错误 no applicable action for [springProfile] 5、切换日志框架可以按照slf4j的日志适配图，进行相关的切换； slf4j+log4j的方式；（不推荐） 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;&lt;/dependency&gt; 切换为log4j2 123456789101112131415 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt;&lt;/dependency&gt; 四、Web开发1、简介使用SpringBoot； 1）、创建SpringBoot应用，选中我们需要的模块； 2）、SpringBoot已经默认将这些场景配置好了，只需要在配置文件中指定少量配置就可以运行起来 3）、自己编写业务代码； 自动配置原理？ 这个场景SpringBoot帮我们配置了什么？能不能修改？能修改哪些配置？能不能扩展？xxx 12xxxxAutoConfiguration：帮我们给容器中自动配置组件；xxxxProperties:配置类来封装配置文件的内容； 2、SpringBoot对静态资源的映射规则；123@ConfigurationProperties(prefix = "spring.resources", ignoreUnknownFields = false)public class ResourceProperties implements ResourceLoaderAware &#123; //可以设置和静态资源有关的参数，缓存时间等 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364WebMvcAuotConfiguration： @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; if (!this.resourceProperties.isAddMappings()) &#123; logger.debug("Default resource handling disabled"); return; &#125; Integer cachePeriod = this.resourceProperties.getCachePeriod(); if (!registry.hasMappingForPattern("/webjars/**")) &#123; customizeResourceHandlerRegistration( registry.addResourceHandler("/webjars/**") .addResourceLocations( "classpath:/META-INF/resources/webjars/") .setCachePeriod(cachePeriod)); &#125; String staticPathPattern = this.mvcProperties.getStaticPathPattern(); //静态资源文件夹映射 if (!registry.hasMappingForPattern(staticPathPattern)) &#123; customizeResourceHandlerRegistration( registry.addResourceHandler(staticPathPattern) .addResourceLocations( this.resourceProperties.getStaticLocations()) .setCachePeriod(cachePeriod)); &#125; &#125; //配置欢迎页映射 @Bean public WelcomePageHandlerMapping welcomePageHandlerMapping( ResourceProperties resourceProperties) &#123; return new WelcomePageHandlerMapping(resourceProperties.getWelcomePage(), this.mvcProperties.getStaticPathPattern()); &#125; //配置喜欢的图标 @Configuration @ConditionalOnProperty(value = "spring.mvc.favicon.enabled", matchIfMissing = true) public static class FaviconConfiguration &#123; private final ResourceProperties resourceProperties; public FaviconConfiguration(ResourceProperties resourceProperties) &#123; this.resourceProperties = resourceProperties; &#125; @Bean public SimpleUrlHandlerMapping faviconHandlerMapping() &#123; SimpleUrlHandlerMapping mapping = new SimpleUrlHandlerMapping(); mapping.setOrder(Ordered.HIGHEST_PRECEDENCE + 1); //所有 **/favicon.ico mapping.setUrlMap(Collections.singletonMap("**/favicon.ico", faviconRequestHandler())); return mapping; &#125; @Bean public ResourceHttpRequestHandler faviconRequestHandler() &#123; ResourceHttpRequestHandler requestHandler = new ResourceHttpRequestHandler(); requestHandler .setLocations(this.resourceProperties.getFaviconLocations()); return requestHandler; &#125; &#125; ==1）、所有 /webjars/** ，都去 classpath:/META-INF/resources/webjars/ 找资源；== ​ webjars：以jar包的方式引入静态资源； http://www.webjars.org/ localhost:8080/webjars/jquery/3.3.1/jquery.js 123456&lt;!--引入jquery-webjar--&gt;在访问的时候只需要写webjars下面资源的名称即可 &lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt; &lt;/dependency&gt; ==2）、”/**” 访问当前项目的任何资源，都去（静态资源的文件夹）找映射== 12345&quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;,&quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; &quot;/&quot;：当前项目的根路径 以上都是静态文件夹路径localhost:8080/abc === 去静态资源文件夹里面找abc ==3）、欢迎页； 静态资源文件夹下的所有index.html页面；被”/**”映射；== ​ localhost:8080/ 找index页面 ==4）、所有的 **/favicon.ico 都是在静态资源文件下找（如果页面无法显示图标就按Ctrl+F5 强制刷新就出现了，再不行再清除缓存）；== 也可以自定义静态文件夹的路径（但一旦定义了自定义的，默认的public.resources就不再是静态文件夹了，无法默认直接访问了） 3、模板引擎JSP、Velocity、Freemarker、Thymeleaf SpringBoot推荐的Thymeleaf； 语法更简单，功能更强大； 1、引入thymeleaf；123456789101112 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; 2.1.6 &lt;/dependency&gt;切换thymeleaf版本&lt;properties&gt; &lt;thymeleaf.version&gt;3.0.9.RELEASE&lt;/thymeleaf.version&gt; &lt;!-- 布局功能的支持程序 thymeleaf3主程序 layout2以上版本 --&gt; &lt;!-- thymeleaf2 layout1--&gt; &lt;thymeleaf-layout-dialect.version&gt;2.2.2&lt;/thymeleaf-layout-dialect.version&gt; &lt;/properties&gt; 2、Thymeleaf使用1234567891011@ConfigurationProperties(prefix = "spring.thymeleaf")public class ThymeleafProperties &#123; private static final Charset DEFAULT_ENCODING = Charset.forName("UTF-8"); private static final MimeType DEFAULT_CONTENT_TYPE = MimeType.valueOf("text/html"); public static final String DEFAULT_PREFIX = "classpath:/templates/"; public static final String DEFAULT_SUFFIX = ".html"; // 只要我们把HTML页面放在classpath:/templates/，thymeleaf就能自动渲染； 使用： 1、导入thymeleaf的名称空间 1&lt;html lang="en" xmlns:th="http://www.thymeleaf.org"&gt; 2、使用thymeleaf语法； 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang="en" xmlns:th="http://www.thymeleaf.org"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;成功！&lt;/h1&gt; &lt;!--th:text 将div里面的文本内容设置为 --&gt; &lt;div th:text="$&#123;hello&#125;"&gt;这是显示欢迎信息&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3、语法规则1）、th:text；改变当前元素里面的文本内容； ​ th：任意html属性；来替换原生属性的值 2）、表达式？ 123456789101112131415161718192021222324252627282930Simple expressions:（表达式语法） Variable Expressions: $&#123;...&#125;：获取变量值；OGNL； 1）、获取对象的属性、调用方法 2）、使用内置的基本对象： #ctx : the context object. #vars: the context variables. #locale : the context locale. #request : (only in Web Contexts) the HttpServletRequest object. #response : (only in Web Contexts) the HttpServletResponse object. #session : (only in Web Contexts) the HttpSession object. #servletContext : (only in Web Contexts) the ServletContext object. $&#123;session.foo&#125; 3）、内置的一些工具对象：#execInfo : information about the template being processed.#messages : methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #&#123;…&#125; syntax.#uris : methods for escaping parts of URLs/URIs#conversions : methods for executing the configured conversion service (if any).#dates : methods for java.util.Date objects: formatting, component extraction, etc.#calendars : analogous to #dates , but for java.util.Calendar objects.#numbers : methods for formatting numeric objects.#strings : methods for String objects: contains, startsWith, prepending/appending, etc.#objects : methods for objects in general.#bools : methods for boolean evaluation.#arrays : methods for arrays.#lists : methods for lists.#sets : methods for sets.#maps : methods for maps.#aggregates : methods for creating aggregates on arrays or collections.#ids : methods for dealing with id attributes that might be repeated (for example, as a result of an iteration). 1234567891011121314151617181920212223242526272829303132333435363738 Selection Variable Expressions: *&#123;...&#125;：选择表达式：和$&#123;&#125;在功能上是一样； #补充：配合 th:object="$&#123;session.user&#125;： &lt;div th:object="$&#123;session.user&#125;"&gt; &lt;p&gt;Name: &lt;span th:text="*&#123;firstName&#125;"&gt;Sebastian&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Surname: &lt;span th:text="*&#123;lastName&#125;"&gt;Pepper&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Nationality: &lt;span th:text="*&#123;nationality&#125;"&gt;Saturn&lt;/span&gt;.&lt;/p&gt; &lt;/div&gt; Message Expressions: #&#123;...&#125;：获取国际化内容 Link URL Expressions: @&#123;...&#125;：定义URL； @&#123;/order/process(execId=$&#123;execId&#125;,execType='FAST')&#125; Fragment Expressions: ~&#123;...&#125;：片段引用表达式 &lt;div th:insert="~&#123;commons :: main&#125;"&gt;...&lt;/div&gt; Literals（字面量） Text literals: 'one text' , 'Another one!' ,… Number literals: 0 , 34 , 3.0 , 12.3 ,… Boolean literals: true , false Null literal: null Literal tokens: one , sometext , main ,…Text operations:（文本操作） String concatenation: + Literal substitutions: |The name is $&#123;name&#125;|Arithmetic operations:（数学运算） Binary operators: + , - , * , / , % Minus sign (unary operator): -Boolean operations:（布尔运算） Binary operators: and , or Boolean negation (unary operator): ! , notComparisons and equality:（比较运算） Comparators: &gt; , &lt; , &gt;= , &lt;= ( gt , lt , ge , le ) Equality operators: == , != ( eq , ne )Conditional operators:条件运算（三元运算符） If-then: (if) ? (then) If-then-else: (if) ? (then) : (else) Default: (value) ?: (defaultvalue)Special tokens: No-Operation: _ 4、SpringMVC自动配置https://docs.spring.io/spring-boot/docs/1.5.10.RELEASE/reference/htmlsingle/#boot-features-developing-web-applications 1. Spring MVC auto-configurationSpring Boot 自动配置好了SpringMVC 以下是SpringBoot对SpringMVC的默认配置:==（WebMvcAutoConfiguration）== Inclusion of ContentNegotiatingViewResolver and BeanNameViewResolver beans. 自动配置了ViewResolver（视图解析器：根据方法的返回值得到视图对象（View），视图对象决定如何渲染（转发？重定向？）） ContentNegotiatingViewResolver：组合所有的视图解析器的； ==如何定制：我们可以自己给容器中添加一个视图解析器；自动的将其组合进来；== Support for serving static resources, including support for WebJars (see below).静态资源文件夹路径,webjars Static index.html support. 静态首页访问 Custom Favicon support (see below). favicon.ico ​ 自动注册了 of Converter, GenericConverter, Formatter beans. Converter：转换器； public String hello(User user)：类型转换使用Converter Formatter 格式化器； 2017.12.17===Date； 12345@Bean@ConditionalOnProperty(prefix = "spring.mvc", name = "date-format")//在文件中配置日期格式化的规则public Formatter&lt;Date&gt; dateFormatter() &#123; return new DateFormatter(this.mvcProperties.getDateFormat());//日期格式化组件&#125; ​ ==自己添加的格式化器转换器，我们只需要放在容器中即可== Support for HttpMessageConverters (see below). HttpMessageConverter：SpringMVC用来转换Http请求和响应的；User—Json； HttpMessageConverters 是从容器中确定；获取所有的HttpMessageConverter； ==自己给容器中添加HttpMessageConverter，只需要将自己的组件注册容器中（@Bean,@Component）== ​ Automatic registration of MessageCodesResolver (see below).定义错误代码生成规则 Automatic use of a ConfigurableWebBindingInitializer bean (see below). ==我们可以配置一个ConfigurableWebBindingInitializer来替换默认的；（添加到容器）== 12初始化WebDataBinder；请求数据=====JavaBean； org.springframework.boot.autoconfigure.web：web的所有自动场景； If you want to keep Spring Boot MVC features, and you just want to add additional MVC configuration (interceptors, formatters, view controllers etc.) you can add your own @Configuration class of type WebMvcConfigurerAdapter, but without @EnableWebMvc. If you wish to provide custom instances of RequestMappingHandlerMapping, RequestMappingHandlerAdapter or ExceptionHandlerExceptionResolver you can declare a WebMvcRegistrationsAdapter instance providing such components. If you want to take complete control of Spring MVC, you can add your own @Configuration annotated with @EnableWebMvc. 2、扩展SpringMVC1234567&lt;mvc:view-controller path="/hello" view-name="success"/&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path="/hello"/&gt; &lt;bean&gt;&lt;/bean&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; ==编写一个配置类（@Configuration），是WebMvcConfigurerAdapter类型；不能标注@EnableWebMvc==; 既保留了所有的自动配置，也能用我们扩展的配置； 1234567891011//使用WebMvcConfigurerAdapter可以来扩展SpringMVC的功能@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; // super.addViewControllers(registry); //浏览器发送 /atguigu 请求来到 success registry.addViewController("/atguigu").setViewName("success"); &#125;&#125; 原理： ​ 1）、WebMvcAutoConfiguration是SpringMVC的自动配置类 ​ 2）、在做其他自动配置时会导入；@Import(EnableWebMvcConfiguration.class) 123456789101112131415161718 @Configurationpublic static class EnableWebMvcConfiguration extends DelegatingWebMvcConfiguration &#123; private final WebMvcConfigurerComposite configurers = new WebMvcConfigurerComposite(); //从容器中获取所有的WebMvcConfigurer @Autowired(required = false) public void setConfigurers(List&lt;WebMvcConfigurer&gt; configurers) &#123; if (!CollectionUtils.isEmpty(configurers)) &#123; this.configurers.addWebMvcConfigurers(configurers); //一个参考实现；将所有的WebMvcConfigurer相关配置都来一起调用； @Override // public void addViewControllers(ViewControllerRegistry registry) &#123; // for (WebMvcConfigurer delegate : this.delegates) &#123; // delegate.addViewControllers(registry); // &#125; &#125; &#125;&#125; ​ 3）、容器中所有的WebMvcConfigurer都会一起起作用； ​ 4）、我们的配置类也会被调用； ​ 效果：SpringMVC的自动配置和我们的扩展配置都会起作用； 3、全面接管SpringMVC；SpringBoot对SpringMVC的自动配置不需要了，所有都是我们自己配置；所有的SpringMVC的自动配置都失效了 我们需要在配置类中添加@EnableWebMvc即可； 123456789101112//使用WebMvcConfigurerAdapter可以来扩展SpringMVC的功能@EnableWebMvc@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; // super.addViewControllers(registry); //浏览器发送 /atguigu 请求来到 success registry.addViewController("/atguigu").setViewName("success"); &#125;&#125; 原理： 为什么@EnableWebMvc自动配置就失效了； 1）@EnableWebMvc的核心 12@Import(DelegatingWebMvcConfiguration.class)public @interface EnableWebMvc &#123; 2）、 12@Configurationpublic class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport &#123; 3）、 12345678910@Configuration@ConditionalOnWebApplication@ConditionalOnClass(&#123; Servlet.class, DispatcherServlet.class, WebMvcConfigurerAdapter.class &#125;)//容器中没有这个组件的时候，这个自动配置类才生效@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10)@AutoConfigureAfter(&#123; DispatcherServletAutoConfiguration.class, ValidationAutoConfiguration.class &#125;)public class WebMvcAutoConfiguration &#123; 4）、@EnableWebMvc将WebMvcConfigurationSupport组件导入进来； 5）、导入的WebMvcConfigurationSupport只是SpringMVC最基本的功能； 5、如何修改SpringBoot的默认配置模式： ​ 1）、SpringBoot在自动配置很多组件的时候，先看容器中有没有用户自己配置的（@Bean、@Component）如果有就用用户配置的，如果没有，才自动配置；如果有些组件可以有多个（ViewResolver）将用户配置的和自己默认的组合起来； ​ 2）、在SpringBoot中会有非常多的xxxConfigurer帮助我们进行扩展配置 ​ 3）、在SpringBoot中会有很多的xxxCustomizer帮助我们进行定制配置 6、RestfulCRUD1）、默认访问首页1234567891011121314151617181920212223242526//使用WebMvcConfigurerAdapter可以来扩展SpringMVC的功能//@EnableWebMvc 不要接管SpringMVC@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; // super.addViewControllers(registry); //浏览器发送 /atguigu 请求来到 success registry.addViewController("/atguigu").setViewName("success"); &#125; //所有的WebMvcConfigurerAdapter组件都会一起起作用 @Bean //将组件注册在容器 public WebMvcConfigurerAdapter webMvcConfigurerAdapter()&#123; WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController("/").setViewName("login"); registry.addViewController("/index.html").setViewName("login"); &#125; &#125;; return adapter; &#125;&#125; 2）、国际化1）、编写国际化配置文件； 2）、使用ResourceBundleMessageSource管理国际化资源文件 3）、在页面使用fmt:message取出国际化内容 步骤： 1）、编写国际化配置文件，抽取页面需要显示的国际化消息 2）、SpringBoot自动配置好了管理国际化资源文件的组件； 12345678910111213141516171819202122232425262728@ConfigurationProperties(prefix = "spring.messages")public class MessageSourceAutoConfiguration &#123; /** * Comma-separated list of basenames (essentially a fully-qualified classpath * location), each following the ResourceBundle convention with relaxed support for * slash based locations. If it doesn't contain a package qualifier (such as * "org.mypackage"), it will be resolved from the classpath root. */ private String basename = "messages"; //我们的配置文件可以直接放在类路径下叫messages.properties； @Bean public MessageSource messageSource() &#123; ResourceBundleMessageSource messageSource = new ResourceBundleMessageSource(); if (StringUtils.hasText(this.basename)) &#123; //设置国际化资源文件的基础名（去掉语言国家代码的） messageSource.setBasenames(StringUtils.commaDelimitedListToStringArray( StringUtils.trimAllWhitespace(this.basename))); &#125; if (this.encoding != null) &#123; messageSource.setDefaultEncoding(this.encoding.name()); &#125; messageSource.setFallbackToSystemLocale(this.fallbackToSystemLocale); messageSource.setCacheSeconds(this.cacheSeconds); messageSource.setAlwaysUseMessageFormat(this.alwaysUseMessageFormat); return messageSource; &#125; 3）、去页面获取国际化的值； 123456789101112131415161718192021222324252627282930313233343536&lt;!DOCTYPE html&gt;&lt;html lang="en" xmlns:th="http://www.thymeleaf.org"&gt; &lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"&gt; &lt;meta name="description" content=""&gt; &lt;meta name="author" content=""&gt; &lt;title&gt;Signin Template for Bootstrap&lt;/title&gt; &lt;!-- Bootstrap core CSS --&gt; &lt;link href="asserts/css/bootstrap.min.css" th:href="@&#123;/webjars/bootstrap/4.0.0/css/bootstrap.css&#125;" rel="stylesheet"&gt; &lt;!-- Custom styles for this template --&gt; &lt;link href="asserts/css/signin.css" th:href="@&#123;/asserts/css/signin.css&#125;" rel="stylesheet"&gt; &lt;/head&gt; &lt;body class="text-center"&gt; &lt;form class="form-signin" action="dashboard.html"&gt; &lt;img class="mb-4" th:src="@&#123;/asserts/img/bootstrap-solid.svg&#125;" src="asserts/img/bootstrap-solid.svg" alt="" width="72" height="72"&gt; &lt;h1 class="h3 mb-3 font-weight-normal" th:text="#&#123;login.tip&#125;"&gt;Please sign in&lt;/h1&gt; &lt;label class="sr-only" th:text="#&#123;login.username&#125;"&gt;Username&lt;/label&gt; &lt;input type="text" class="form-control" placeholder="Username" th:placeholder="#&#123;login.username&#125;" required="" autofocus=""&gt; &lt;label class="sr-only" th:text="#&#123;login.password&#125;"&gt;Password&lt;/label&gt; &lt;input type="password" class="form-control" placeholder="Password" th:placeholder="#&#123;login.password&#125;" required=""&gt; &lt;div class="checkbox mb-3"&gt; &lt;label&gt; &lt;input type="checkbox" value="remember-me"/&gt; [[#&#123;login.remember&#125;]] &lt;/label&gt; &lt;/div&gt; &lt;button class="btn btn-lg btn-primary btn-block" type="submit" th:text="#&#123;login.btn&#125;"&gt;Sign in&lt;/button&gt; &lt;p class="mt-5 mb-3 text-muted"&gt;© 2017-2018&lt;/p&gt; &lt;a class="btn btn-sm"&gt;中文&lt;/a&gt; &lt;a class="btn btn-sm"&gt;English&lt;/a&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 效果：根据浏览器语言设置的信息切换了国际化； 原理： ​ 国际化Locale（区域信息对象）；LocaleResolver（获取区域信息对象）； 123456789101112@Bean@ConditionalOnMissingBean@ConditionalOnProperty(prefix = "spring.mvc", name = "locale")public LocaleResolver localeResolver() &#123; if (this.mvcProperties .getLocaleResolver() == WebMvcProperties.LocaleResolver.FIXED) &#123; return new FixedLocaleResolver(this.mvcProperties.getLocale()); &#125; AcceptHeaderLocaleResolver localeResolver = new AcceptHeaderLocaleResolver(); localeResolver.setDefaultLocale(this.mvcProperties.getLocale()); return localeResolver;&#125; 默认的就是根据请求头带来的区域信息获取Locale进行国际化4）、点击链接切换国际化 12345678910111213141516171819202122232425262728/** * 可以在连接上携带区域信息 */public class MyLocaleResolver implements LocaleResolver &#123; @Override public Locale resolveLocale(HttpServletRequest request) &#123; String l = request.getParameter("l"); Locale locale = Locale.getDefault(); if(!StringUtils.isEmpty(l))&#123; String[] split = l.split("_"); locale = new Locale(split[0],split[1]); &#125; return locale; &#125; @Override public void setLocale(HttpServletRequest request, HttpServletResponse response, Locale locale) &#123; &#125;&#125; @Bean public LocaleResolver localeResolver()&#123; return new MyLocaleResolver(); &#125;&#125; 3）、登陆开发期间模板引擎页面修改以后，要实时生效 1）、禁用模板引擎的缓存 12# 禁用缓存spring.thymeleaf.cache=false 2）、页面修改完成以后ctrl+f9：重新编译； 登陆错误消息的显示 1&lt;p style="color: red" th:text="$&#123;msg&#125;" th:if="$&#123;not #strings.isEmpty(msg)&#125;"&gt;&lt;/p&gt; 4）、拦截器进行登陆检查拦截器 12345678910111213141516171819202122232425262728293031/** * 登陆检查， */public class LoginHandlerInterceptor implements HandlerInterceptor &#123; //目标方法执行之前 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; Object user = request.getSession().getAttribute("loginUser"); if(user == null)&#123; //未登陆，返回登陆页面 request.setAttribute("msg","没有权限请先登陆"); request.getRequestDispatcher("/index.html").forward(request,response); return false; &#125;else&#123; //已登陆，放行请求 return true; &#125; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; &#125;&#125; 注册拦截器 1234567891011121314151617181920212223//所有的WebMvcConfigurerAdapter组件都会一起起作用 @Bean //将组件注册在容器 public WebMvcConfigurerAdapter webMvcConfigurerAdapter()&#123; WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController("/").setViewName("login"); registry.addViewController("/index.html").setViewName("login"); registry.addViewController("/main.html").setViewName("dashboard"); &#125; //注册拦截器 @Override public void addInterceptors(InterceptorRegistry registry) &#123; //super.addInterceptors(registry); //静态资源； *.css , *.js //SpringBoot已经做好了静态资源映射 registry.addInterceptor(new LoginHandlerInterceptor()).addPathPatterns("/**") .excludePathPatterns("/index.html","/","/user/login"); &#125; &#125;; return adapter; &#125; 5）、CRUD-员工列表实验要求： 1）、RestfulCRUD：CRUD满足Rest风格； URI： /资源名称/资源标识 HTTP请求方式区分对资源CRUD操作 普通CRUD（uri来区分操作） RestfulCRUD 查询 getEmp emp—GET 添加 addEmp?xxx emp—POST 修改 updateEmp?id=xxx&amp;xxx=xx emp/{id}—PUT 删除 deleteEmp?id=1 emp/{id}—DELETE 2）、实验的请求架构; 实验功能 请求URI 请求方式 查询所有员工 emps GET 查询某个员工(来到修改页面) emp/1 GET 来到添加页面 emp GET 添加员工 emp POST 来到修改页面（查出员工进行信息回显） emp/1 GET 修改员工 emp PUT 删除员工 emp/1 DELETE 3）、员工列表： thymeleaf公共页面元素抽取12345678910111213141、抽取公共片段&lt;div th:fragment="copy"&gt;&amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/div&gt;2、引入公共片段&lt;div th:insert="~&#123;footer :: copy&#125;"&gt;&lt;/div&gt;~&#123;templatename::selector&#125;：模板名::选择器（选择的是公共片段的id名）（这种方式可以不在公共片段加东西 只要有id 就可以直接在需求页面增加公共片段：推荐！！！）~&#123;templatename::fragmentname&#125;:模板名::片段名3、默认效果：insert的公共片段在div标签中如果使用th:insert等属性进行引入，可以不用写~&#123;&#125;：行内写法可以加上：[[~&#123;&#125;]];[(~&#123;&#125;)]； 三种引入公共片段的th属性： th:insert：将公共片段整个插入到声明引入的元素中 th:replace：将声明引入的元素替换为公共片段 th:include：将被引入的片段的内容包含进这个标签中 1234567891011121314151617181920212223&lt;footer th:fragment="copy"&gt;&amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;引入方式&lt;div th:insert="footer :: copy"&gt;&lt;/div&gt; &lt;!--第一个属于在div里插入了公共片段以及最外部标签footer--&gt;&lt;div th:replace="footer :: copy"&gt;&lt;/div&gt;&lt;!--第二个是将div替换成footer，然后再引入其公共片段--&gt;&lt;div th:include="footer :: copy"&gt;&lt;/div&gt;&lt;!--d第三是将footer元素去掉，用div来包含公共片段，摒弃公共片段的自身外部标签--&gt;效果&lt;div&gt; &lt;footer&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery &lt;/footer&gt;&lt;/div&gt;&lt;footer&gt;&amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;&lt;div&gt;&amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/div&gt; 引入片段的时候传入参数： 123456789101112131415161718&lt;nav class="col-md-2 d-none d-md-block bg-light sidebar" id="sidebar"&gt; &lt;div class="sidebar-sticky"&gt; &lt;ul class="nav flex-column"&gt; &lt;li class="nav-item"&gt; &lt;a class="nav-link active" th:class="$&#123;activeUri=='main.html'?'nav-link active':'nav-link'&#125;" href="#" th:href="@&#123;/main.html&#125;"&gt; &lt;svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-home"&gt; &lt;path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"&gt;&lt;/path&gt; &lt;polyline points="9 22 9 12 15 12 15 22"&gt;&lt;/polyline&gt; &lt;/svg&gt; Dashboard &lt;span class="sr-only"&gt;(current)&lt;/span&gt; &lt;/a&gt; &lt;/li&gt;&lt;!--引入侧边栏;传入参数--&gt;&lt;div th:replace="commons/bar::#sidebar(activeUri='emps')"&gt;&lt;/div&gt; 6）、CRUD-员工添加添加页面 123456789101112131415161718192021222324252627282930313233343536&lt;form&gt; &lt;div class="form-group"&gt; &lt;label&gt;LastName&lt;/label&gt; &lt;input type="text" class="form-control" placeholder="zhangsan"&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label&gt;Email&lt;/label&gt; &lt;input type="email" class="form-control" placeholder="zhangsan@atguigu.com"&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label&gt;Gender&lt;/label&gt;&lt;br/&gt; &lt;div class="form-check form-check-inline"&gt; &lt;input class="form-check-input" type="radio" name="gender" value="1"&gt; &lt;label class="form-check-label"&gt;男&lt;/label&gt; &lt;/div&gt; &lt;div class="form-check form-check-inline"&gt; &lt;input class="form-check-input" type="radio" name="gender" value="0"&gt; &lt;label class="form-check-label"&gt;女&lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label&gt;department&lt;/label&gt; &lt;select class="form-control"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label&gt;Birth&lt;/label&gt; &lt;input type="text" class="form-control" placeholder="zhangsan"&gt; &lt;/div&gt; &lt;button type="submit" class="btn btn-primary"&gt;添加&lt;/button&gt;&lt;/form&gt; 提交的数据格式不对：生日：日期； 2017-12-12；2017/12/12；2017.12.12； 日期的格式化；SpringMVC将页面提交的值需要转换为指定的类型; 2017-12-12—Date； 类型转换，格式化; 默认日期是按照/的方式； 7）、CRUD-员工修改修改添加二合一表单 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!--需要区分是员工修改还是添加；--&gt;&lt;form th:action="@&#123;/emp&#125;" method="post"&gt; &lt;!--发送put请求修改员工数据--&gt; &lt;!--1、SpringMVC中配置HiddenHttpMethodFilter;（SpringBoot自动配置好的）2、页面创建一个post表单3、创建一个input项，name="_method";值就是我们指定的请求方式--&gt; &lt;input type="hidden" name="_method" value="put" th:if="$&#123;emp!=null&#125;"/&gt; &lt;input type="hidden" name="id" th:if="$&#123;emp!=null&#125;" th:value="$&#123;emp.id&#125;"&gt; &lt;div class="form-group"&gt; &lt;label&gt;LastName&lt;/label&gt; &lt;input name="lastName" type="text" class="form-control" placeholder="zhangsan" th:value="$&#123;emp!=null&#125;?$&#123;emp.lastName&#125;"&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label&gt;Email&lt;/label&gt; &lt;input name="email" type="email" class="form-control" placeholder="zhangsan@atguigu.com" th:value="$&#123;emp!=null&#125;?$&#123;emp.email&#125;"&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label&gt;Gender&lt;/label&gt;&lt;br/&gt; &lt;div class="form-check form-check-inline"&gt; &lt;input class="form-check-input" type="radio" name="gender" value="1" th:checked="$&#123;emp!=null&#125;?$&#123;emp.gender==1&#125;"&gt; &lt;label class="form-check-label"&gt;男&lt;/label&gt; &lt;/div&gt; &lt;div class="form-check form-check-inline"&gt; &lt;input class="form-check-input" type="radio" name="gender" value="0" th:checked="$&#123;emp!=null&#125;?$&#123;emp.gender==0&#125;"&gt; &lt;label class="form-check-label"&gt;女&lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label&gt;department&lt;/label&gt; &lt;!--提交的是部门的id--&gt; &lt;select class="form-control" name="department.id"&gt; &lt;option th:selected="$&#123;emp!=null&#125;?$&#123;dept.id == emp.department.id&#125;" th:value="$&#123;dept.id&#125;" th:each="dept:$&#123;depts&#125;" th:text="$&#123;dept.departmentName&#125;"&gt;1&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label&gt;Birth&lt;/label&gt; &lt;input name="birth" type="text" class="form-control" placeholder="zhangsan" th:value="$&#123;emp!=null&#125;?$&#123;#dates.format(emp.birth, 'yyyy-MM-dd HH:mm')&#125;"&gt; &lt;/div&gt; &lt;button type="submit" class="btn btn-primary" th:text="$&#123;emp!=null&#125;?'修改':'添加'"&gt;添加&lt;/button&gt;&lt;/form&gt; 8）、CRUD-员工删除123456789101112131415161718192021&lt;tr th:each="emp:$&#123;emps&#125;"&gt; &lt;td th:text="$&#123;emp.id&#125;"&gt;&lt;/td&gt; &lt;td&gt;[[$&#123;emp.lastName&#125;]]&lt;/td&gt; &lt;td th:text="$&#123;emp.email&#125;"&gt;&lt;/td&gt; &lt;td th:text="$&#123;emp.gender&#125;==0?'女':'男'"&gt;&lt;/td&gt; &lt;td th:text="$&#123;emp.department.departmentName&#125;"&gt;&lt;/td&gt; &lt;td th:text="$&#123;#dates.format(emp.birth, 'yyyy-MM-dd HH:mm')&#125;"&gt;&lt;/td&gt; &lt;td&gt; &lt;a class="btn btn-sm btn-primary" th:href="@&#123;/emp/&#125;+$&#123;emp.id&#125;"&gt;编辑&lt;/a&gt; &lt;button th:attr="del_uri=@&#123;/emp/&#125;+$&#123;emp.id&#125;" class="btn btn-sm btn-danger deleteBtn"&gt;删除&lt;/button&gt; &lt;/td&gt;&lt;/tr&gt;&lt;script&gt; $(".deleteBtn").click(function()&#123; //删除当前员工的 $("#deleteEmpForm").attr("action",$(this).attr("del_uri")).submit(); return false; &#125;);&lt;/script&gt; 7、错误处理机制1）、SpringBoot默认的错误处理机制默认效果： ​ 1）、浏览器，返回一个默认的错误页面 浏览器发送请求的请求头： ​ 2）、如果是其他客户端，默认响应一个json数据 ​ 原理： ​ 可以参照ErrorMvcAutoConfiguration；错误处理的自动配置； 给容器中添加了以下组件​ 1、DefaultErrorAttributes： 1234567891011帮我们在页面共享信息；@Override public Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; Map&lt;String, Object&gt; errorAttributes = new LinkedHashMap&lt;String, Object&gt;(); errorAttributes.put("timestamp", new Date()); addStatus(errorAttributes, requestAttributes); addErrorDetails(errorAttributes, requestAttributes, includeStackTrace); addPath(errorAttributes, requestAttributes); return errorAttributes; &#125; ​ 2、BasicErrorController：处理默认/error请求 12345678910111213141516171819202122232425@Controller@RequestMapping("$&#123;server.error.path:$&#123;error.path:/error&#125;&#125;")public class BasicErrorController extends AbstractErrorController &#123; @RequestMapping(produces = "text/html")//产生html类型的数据；浏览器发送的请求来到这个方法处理 public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) &#123; HttpStatus status = getStatus(request); Map&lt;String, Object&gt; model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); //去哪个页面作为错误页面；包含页面地址和页面内容 ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView == null ? new ModelAndView("error", model) : modelAndView); &#125; @RequestMapping @ResponseBody //产生json数据，其他客户端来到这个方法处理； public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; body = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.ALL)); HttpStatus status = getStatus(request); return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(body, status); &#125; ​ 3、ErrorPageCustomizer： 12@Value("$&#123;error.path:/error&#125;")private String path = "/error"; 系统出现错误以后来到error请求进行处理；（web.xml注册的错误页面规则） ​ 4、DefaultErrorViewResolver： 123456789101112131415161718192021222324@Override public ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map&lt;String, Object&gt; model) &#123; ModelAndView modelAndView = resolve(String.valueOf(status), model); if (modelAndView == null &amp;&amp; SERIES_VIEWS.containsKey(status.series())) &#123; modelAndView = resolve(SERIES_VIEWS.get(status.series()), model); &#125; return modelAndView; &#125; private ModelAndView resolve(String viewName, Map&lt;String, Object&gt; model) &#123; //默认SpringBoot可以去找到一个页面？ error/404 String errorViewName = "error/" + viewName; //模板引擎可以解析这个页面地址就用模板引擎解析 TemplateAvailabilityProvider provider = this.templateAvailabilityProviders .getProvider(errorViewName, this.applicationContext); if (provider != null) &#123; //模板引擎可用的情况下返回到errorViewName指定的视图地址 return new ModelAndView(errorViewName, model); &#125; //模板引擎不可用，就在静态资源文件夹下找errorViewName对应的页面 error/404.html return resolveResource(errorViewName, model); &#125; ​ 步骤： ​ 一但系统出现4xx或者5xx之类的错误；ErrorPageCustomizer就会生效（定制错误的响应规则）；就会来到/error请求；就会被BasicErrorController处理； ​ 1）响应页面；去哪个页面是由DefaultErrorViewResolver解析得到的； 1234567891011protected ModelAndView resolveErrorView(HttpServletRequest request, HttpServletResponse response, HttpStatus status, Map&lt;String, Object&gt; model) &#123; //所有的ErrorViewResolver得到ModelAndView for (ErrorViewResolver resolver : this.errorViewResolvers) &#123; ModelAndView modelAndView = resolver.resolveErrorView(request, status, model); if (modelAndView != null) &#123; return modelAndView; &#125; &#125; return null;&#125; 2）、如果定制错误响应：1）、如何定制错误的页面；​ 1）、有模板引擎的情况下；error/状态码; 【将错误页面命名为 错误状态码.html 放在模板引擎文件夹里面的 error文件夹下】，发生此状态码的错误就会来到 对应的页面； ​ 我们可以使用4xx和5xx作为错误页面的文件名来匹配这种类型的所有错误，精确优先（优先寻找精确的状态码.html）； ​ 页面能获取的信息； ​ timestamp：时间戳 ​ status：状态码 ​ error：错误提示 ​ exception：异常对象 ​ message：异常消息 ​ errors：JSR303数据校验的错误都在这里 ​ 2）、没有模板引擎（模板引擎找不到这个错误页面），静态资源文件夹下找； ​ 3）、以上都没有错误页面，就是默认来到SpringBoot默认的错误提示页面； 2）、如何定制错误的json数据；​ 1）、自定义异常处理&amp;返回定制json数据； 12345678910111213@ControllerAdvicepublic class MyExceptionHandler &#123; @ResponseBody @ExceptionHandler(UserNotExistException.class) public Map&lt;String,Object&gt; handleException(Exception e)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put("code","user.notexist"); map.put("message",e.getMessage()); return map; &#125;&#125;//没有自适应效果... ​ 2）、转发到/error进行自适应响应效果处理 1234567891011121314@ExceptionHandler(UserNotExistException.class) public String handleException(Exception e, HttpServletRequest request)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); //传入我们自己的错误状态码 4xx 5xx，否则就不会进入定制错误页面的解析流程 /** * Integer statusCode = (Integer) request .getAttribute("javax.servlet.error.status_code"); */ request.setAttribute("javax.servlet.error.status_code",500); map.put("code","user.notexist"); map.put("message",e.getMessage()); //转发到/error return "forward:/error"; &#125; 3）、将我们的定制数据携带出去；出现错误以后，会来到/error请求，会被BasicErrorController处理，响应出去可以获取的数据是由getErrorAttributes得到的（是AbstractErrorController（ErrorController）规定的方法）； ​ 1、完全来编写一个ErrorController的实现类【或者是编写AbstractErrorController的子类】，放在容器中； ​ 2、页面上能用的数据，或者是json返回能用的数据都是通过errorAttributes.getErrorAttributes得到； ​ 容器中DefaultErrorAttributes.getErrorAttributes()；默认进行数据处理的； 自定义ErrorAttributes 1234567891011//给容器中加入我们自己定义的ErrorAttributes@Componentpublic class MyErrorAttributes extends DefaultErrorAttributes &#123; @Override public Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; Map&lt;String, Object&gt; map = super.getErrorAttributes(requestAttributes, includeStackTrace); map.put("company","atguigu"); return map; &#125;&#125; 最终的效果：响应是自适应的，可以通过定制ErrorAttributes改变需要返回的内容， 8、配置嵌入式Servlet容器SpringBoot默认使用Tomcat作为嵌入式的Servlet容器； 问题？ 1）、如何定制和修改Servlet容器的相关配置；1、修改和server有关的配置（ServerProperties【也是EmbeddedServletContainerCustomizer】）； 123456789server.port=8081server.context-path=/crudserver.tomcat.uri-encoding=UTF-8//通用的Servlet容器设置server.xxx//Tomcat的设置server.tomcat.xxx 2、编写一个EmbeddedServletContainerCustomizer：嵌入式的Servlet容器的定制器；来修改Servlet容器的配置 1234567891011@Bean //一定要将这个定制器加入到容器中public EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer()&#123; return new EmbeddedServletContainerCustomizer() &#123; //定制嵌入式的Servlet容器相关的规则 @Override public void customize(ConfigurableEmbeddedServletContainer container) &#123; container.setPort(8083); &#125; &#125;;&#125; 2）、注册Servlet三大组件【Servlet、Filter、Listener】由于SpringBoot默认是以jar包的方式启动嵌入式的Servlet容器来启动SpringBoot的web应用，没有web.xml文件。 注册三大组件用以下方式 ServletRegistrationBean 123456//注册三大组件@Beanpublic ServletRegistrationBean myServlet()&#123; ServletRegistrationBean registrationBean = new ServletRegistrationBean(new MyServlet(),"/myServlet"); return registrationBean;&#125; FilterRegistrationBean 1234567@Beanpublic FilterRegistrationBean myFilter()&#123; FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(new MyFilter()); registrationBean.setUrlPatterns(Arrays.asList("/hello","/myServlet")); return registrationBean;&#125; ServletListenerRegistrationBean 12345@Beanpublic ServletListenerRegistrationBean myListener()&#123; ServletListenerRegistrationBean&lt;MyListener&gt; registrationBean = new ServletListenerRegistrationBean&lt;&gt;(new MyListener()); return registrationBean;&#125; SpringBoot帮我们自动SpringMVC的时候，自动的注册SpringMVC的前端控制器；DIspatcherServlet； DispatcherServletAutoConfiguration中： 1234567891011121314151617@Bean(name = DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME)@ConditionalOnBean(value = DispatcherServlet.class, name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)public ServletRegistrationBean dispatcherServletRegistration( DispatcherServlet dispatcherServlet) &#123; ServletRegistrationBean registration = new ServletRegistrationBean( dispatcherServlet, this.serverProperties.getServletMapping()); //默认拦截： / 所有请求；包静态资源，但是不拦截jsp请求； /*会拦截jsp //可以通过server.servletPath来修改SpringMVC前端控制器默认拦截的请求路径 registration.setName(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME); registration.setLoadOnStartup( this.webMvcProperties.getServlet().getLoadOnStartup()); if (this.multipartConfig != null) &#123; registration.setMultipartConfig(this.multipartConfig); &#125; return registration;&#125; 2）、SpringBoot能不能支持其他的Servlet容器； 3）、替换为其他嵌入式Servlet容器 默认支持： Tomcat（默认使用） 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; 引入web模块默认就是使用嵌入式的Tomcat作为Servlet容器；&lt;/dependency&gt; Jetty 1234567891011121314151617&lt;!-- 引入web模块 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!--引入其他的Servlet容器--&gt;&lt;dependency&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;/dependency&gt; Undertow 1234567891011121314151617&lt;!-- 引入web模块 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!--引入其他的Servlet容器--&gt;&lt;dependency&gt; &lt;artifactId&gt;spring-boot-starter-undertow&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;/dependency&gt; 4）、嵌入式Servlet容器自动配置原理；EmbeddedServletContainerAutoConfiguration：嵌入式的Servlet容器自动配置？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE)@Configuration@ConditionalOnWebApplication@Import(BeanPostProcessorsRegistrar.class)//导入BeanPostProcessorsRegistrar：Spring注解版；给容器中导入一些组件//导入了EmbeddedServletContainerCustomizerBeanPostProcessor：//后置处理器：bean初始化前后（创建完对象，还没赋值赋值）执行初始化工作public class EmbeddedServletContainerAutoConfiguration &#123; @Configuration @ConditionalOnClass(&#123; Servlet.class, Tomcat.class &#125;)//判断当前是否引入了Tomcat依赖； @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT)//判断当前容器没有用户自己定义EmbeddedServletContainerFactory：嵌入式的Servlet容器工厂；作用：创建嵌入式的Servlet容器 public static class EmbeddedTomcat &#123; @Bean public TomcatEmbeddedServletContainerFactory tomcatEmbeddedServletContainerFactory() &#123; return new TomcatEmbeddedServletContainerFactory(); &#125; &#125; /** * Nested configuration if Jetty is being used. */ @Configuration @ConditionalOnClass(&#123; Servlet.class, Server.class, Loader.class, WebAppContext.class &#125;) @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT) public static class EmbeddedJetty &#123; @Bean public JettyEmbeddedServletContainerFactory jettyEmbeddedServletContainerFactory() &#123; return new JettyEmbeddedServletContainerFactory(); &#125; &#125; /** * Nested configuration if Undertow is being used. */ @Configuration @ConditionalOnClass(&#123; Servlet.class, Undertow.class, SslClientAuthMode.class &#125;) @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT) public static class EmbeddedUndertow &#123; @Bean public UndertowEmbeddedServletContainerFactory undertowEmbeddedServletContainerFactory() &#123; return new UndertowEmbeddedServletContainerFactory(); &#125; &#125; 1）、EmbeddedServletContainerFactory（嵌入式Servlet容器工厂） 1234567public interface EmbeddedServletContainerFactory &#123; //获取嵌入式的Servlet容器 EmbeddedServletContainer getEmbeddedServletContainer( ServletContextInitializer... initializers);&#125; 2）、EmbeddedServletContainer：（嵌入式的Servlet容器） 3）、以TomcatEmbeddedServletContainerFactory为例 123456789101112131415161718192021222324@Overridepublic EmbeddedServletContainer getEmbeddedServletContainer( ServletContextInitializer... initializers) &#123; //创建一个Tomcat Tomcat tomcat = new Tomcat(); //配置Tomcat的基本环节 File baseDir = (this.baseDirectory != null ? this.baseDirectory : createTempDir("tomcat")); tomcat.setBaseDir(baseDir.getAbsolutePath()); Connector connector = new Connector(this.protocol); tomcat.getService().addConnector(connector); customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); configureEngine(tomcat.getEngine()); for (Connector additionalConnector : this.additionalTomcatConnectors) &#123; tomcat.getService().addConnector(additionalConnector); &#125; prepareContext(tomcat.getHost(), initializers); //将配置好的Tomcat传入进去，返回一个EmbeddedServletContainer；并且启动Tomcat服务器 return getTomcatEmbeddedServletContainer(tomcat);&#125; 4）、我们对嵌入式容器的配置修改是怎么生效？ 1ServerProperties、EmbeddedServletContainerCustomizer EmbeddedServletContainerCustomizer：定制器帮我们修改了Servlet容器的配置？ 怎么修改的原理？ 5）、容器中导入了EmbeddedServletContainerCustomizerBeanPostProcessor 12345678910111213141516171819202122232425262728293031323334353637//初始化之前@Overridepublic Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; //如果当前初始化的是一个ConfigurableEmbeddedServletContainer类型的组件 if (bean instanceof ConfigurableEmbeddedServletContainer) &#123; // postProcessBeforeInitialization((ConfigurableEmbeddedServletContainer) bean); &#125; return bean;&#125;private void postProcessBeforeInitialization( ConfigurableEmbeddedServletContainer bean) &#123; //获取所有的定制器，调用每一个定制器的customize方法来给Servlet容器进行属性赋值； for (EmbeddedServletContainerCustomizer customizer : getCustomizers()) &#123; customizer.customize(bean); &#125;&#125;private Collection&lt;EmbeddedServletContainerCustomizer&gt; getCustomizers() &#123; if (this.customizers == null) &#123; // Look up does not include the parent context this.customizers = new ArrayList&lt;EmbeddedServletContainerCustomizer&gt;( this.beanFactory //从容器中获取所有这葛类型的组件：EmbeddedServletContainerCustomizer //定制Servlet容器，给容器中可以添加一个EmbeddedServletContainerCustomizer类型的组件 .getBeansOfType(EmbeddedServletContainerCustomizer.class, false, false) .values()); Collections.sort(this.customizers, AnnotationAwareOrderComparator.INSTANCE); this.customizers = Collections.unmodifiableList(this.customizers); &#125; return this.customizers;&#125;ServerProperties也是定制器 步骤： 1）、SpringBoot根据导入的依赖情况，给容器中添加相应的EmbeddedServletContainerFactory【TomcatEmbeddedServletContainerFactory】 2）、容器中某个组件要创建对象就会惊动后置处理器；EmbeddedServletContainerCustomizerBeanPostProcessor； 只要是嵌入式的Servlet容器工厂，后置处理器就工作； 3）、后置处理器，从容器中获取所有的EmbeddedServletContainerCustomizer，调用定制器的定制方法 ###5）、嵌入式Servlet容器启动原理； 什么时候创建嵌入式的Servlet容器工厂？什么时候获取嵌入式的Servlet容器并启动Tomcat； 获取嵌入式的Servlet容器工厂： 1）、SpringBoot应用启动运行run方法 2）、refreshContext(context);SpringBoot刷新IOC容器【创建IOC容器对象，并初始化容器，创建容器中的每一个组件】；如果是web应用创建AnnotationConfigEmbeddedWebApplicationContext，否则：AnnotationConfigApplicationContext 3）、refresh(context);刷新刚才创建好的ioc容器； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 4）、 onRefresh(); web的ioc容器重写了onRefresh方法 5）、webioc容器会创建嵌入式的Servlet容器；createEmbeddedServletContainer(); 6）、获取嵌入式的Servlet容器工厂： EmbeddedServletContainerFactory containerFactory = getEmbeddedServletContainerFactory(); ​ 从ioc容器中获取EmbeddedServletContainerFactory 组件；TomcatEmbeddedServletContainerFactory创建对象，后置处理器一看是这个对象，就获取所有的定制器来先定制Servlet容器的相关配置； 7）、使用容器工厂获取嵌入式的Servlet容器：this.embeddedServletContainer = containerFactory .getEmbeddedServletContainer(getSelfInitializer()); 8）、嵌入式的Servlet容器创建对象并启动Servlet容器； 先启动嵌入式的Servlet容器，再将ioc容器中剩下没有创建出的对象获取出来； ==IOC容器启动创建嵌入式的Servlet容器== 9、使用外置的Servlet容器嵌入式Servlet容器：应用打成可执行的jar ​ 优点：简单、便携； ​ 缺点：默认不支持JSP、优化定制比较复杂（使用定制器【ServerProperties、自定义EmbeddedServletContainerCustomizer】，自己编写嵌入式Servlet容器的创建工厂【EmbeddedServletContainerFactory】）； 外置的Servlet容器：外面安装Tomcat—应用war包的方式打包； 步骤1）、必须创建一个war项目；（利用idea创建好目录结构） 2）、将嵌入式的Tomcat指定为provided； 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 3）、必须编写一个SpringBootServletInitializer的子类，并调用configure方法 123456789public class ServletInitializer extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) &#123; //传入SpringBoot应用的主程序 return application.sources(SpringBoot04WebJspApplication.class); &#125;&#125; 4）、启动服务器就可以使用； 原理jar包：执行SpringBoot主类的main方法，启动ioc容器，创建嵌入式的Servlet容器； war包：启动服务器，服务器启动SpringBoot应用【SpringBootServletInitializer】，启动ioc容器； servlet3.0（Spring注解版）： 8.2.4 Shared libraries / runtimes pluggability： 规则： ​ 1）、服务器启动（web应用启动）会创建当前web应用里面每一个jar包里面ServletContainerInitializer实例： ​ 2）、ServletContainerInitializer的实现放在jar包的META-INF/services文件夹下，有一个名为javax.servlet.ServletContainerInitializer的文件，内容就是ServletContainerInitializer的实现类的全类名 ​ 3）、还可以使用@HandlesTypes，在应用启动的时候加载我们感兴趣的类； 流程： 1）、启动Tomcat 2）、org\springframework\spring-web\4.3.14.RELEASE\spring-web-4.3.14.RELEASE.jar!\META-INF\services\javax.servlet.ServletContainerInitializer： Spring的web模块里面有这个文件：org.springframework.web.SpringServletContainerInitializer 3）、SpringServletContainerInitializer将@HandlesTypes(WebApplicationInitializer.class)标注的所有这个类型的类都传入到onStartup方法的Set&lt;Class&lt;?&gt;&gt;；为这些WebApplicationInitializer类型的类创建实例； 4）、每一个WebApplicationInitializer都调用自己的onStartup； 5）、相当于我们的SpringBootServletInitializer的类会被创建对象，并执行onStartup方法 6）、SpringBootServletInitializer实例执行onStartup的时候会createRootApplicationContext；创建容器 1234567891011121314151617181920212223242526272829303132333435363738protected WebApplicationContext createRootApplicationContext( ServletContext servletContext) &#123; //1、创建SpringApplicationBuilder SpringApplicationBuilder builder = createSpringApplicationBuilder(); StandardServletEnvironment environment = new StandardServletEnvironment(); environment.initPropertySources(servletContext, null); builder.environment(environment); builder.main(getClass()); ApplicationContext parent = getExistingRootWebApplicationContext(servletContext); if (parent != null) &#123; this.logger.info("Root context already created (using as parent)."); servletContext.setAttribute( WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, null); builder.initializers(new ParentContextApplicationContextInitializer(parent)); &#125; builder.initializers( new ServletContextApplicationContextInitializer(servletContext)); builder.contextClass(AnnotationConfigEmbeddedWebApplicationContext.class); //调用configure方法，子类重写了这个方法，将SpringBoot的主程序类传入了进来 builder = configure(builder); //使用builder创建一个Spring应用 SpringApplication application = builder.build(); if (application.getSources().isEmpty() &amp;&amp; AnnotationUtils .findAnnotation(getClass(), Configuration.class) != null) &#123; application.getSources().add(getClass()); &#125; Assert.state(!application.getSources().isEmpty(), "No SpringApplication sources have been defined. Either override the " + "configure method or add an @Configuration annotation"); // Ensure error pages are registered if (this.registerErrorPageFilter) &#123; application.getSources().add(ErrorPageFilterConfiguration.class); &#125; //启动Spring应用 return run(application);&#125; 7）、Spring的应用就启动并且创建IOC容器 1234567891011121314151617181920212223242526272829303132333435public ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; FailureAnalyzers analyzers = null; configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try &#123; ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); Banner printedBanner = printBanner(environment); context = createApplicationContext(); analyzers = new FailureAnalyzers(context); prepareContext(context, environment, listeners, applicationArguments, printedBanner); //刷新IOC容器 refreshContext(context); afterRefresh(context, applicationArguments); listeners.finished(context, null); stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); &#125; return context; &#125; catch (Throwable ex) &#123; handleRunFailure(context, listeners, analyzers, ex); throw new IllegalStateException(ex); &#125;&#125; ==启动Servlet容器，再启动SpringBoot应用== 五、Docker1、简介Docker是一个开源的应用容器引擎；是一个轻量级容器技术； Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像； 运行中的这个镜像称为容器，容器启动是非常快速的。 2、核心概念docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）； docker客户端(Client)：连接docker主机进行操作； docker仓库(Registry)：用来保存各种打包好的软件镜像； docker镜像(Images)：软件打包好的镜像；放在docker仓库中； docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用 使用Docker的步骤： 1）、安装Docker 2）、去Docker仓库找到这个软件对应的镜像； 3）、使用Docker运行这个镜像，这个镜像就会生成一个Docker容器； 4）、对容器的启动停止就是对软件的启动停止； 3、安装Docker1）、安装linux虚拟机​ 1）、VMWare、VirtualBox（安装）； ​ 2）、导入虚拟机文件centos7-atguigu.ova； ​ 3）、双击启动linux虚拟机;使用 root/ 123456登陆 ​ 4）、使用客户端连接linux服务器进行命令操作； ​ 5）、设置虚拟机网络； ​ 桥接网络===选好网卡====接入网线； ​ 6）、设置好网络以后使用命令重启虚拟机的网络 1service network restart ​ 7）、查看linux的ip地址 1ip addr ​ 8）、使用客户端连接linux； 2）、在linux虚拟机上安装docker步骤： 12345678910111213141、检查内核版本，必须是3.10及以上uname -r2、安装dockeryum install docker3、输入y确认安装4、启动docker[root@localhost ~]# systemctl start docker[root@localhost ~]# docker -vDocker version 1.12.6, build 3e8e77d/1.12.65、开机启动docker[root@localhost ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.6、停止dockersystemctl stop docker 4、Docker常用命令&amp;操作1）、镜像操作 操作 命令 说明 检索 docker search 关键字 eg：docker search redis 我们经常去docker hub上检索镜像的详细信息，如镜像的TAG。 拉取 docker pull 镜像名:tag :tag是可选的，tag表示标签，多为软件的版本，默认是latest 列表 docker images 查看所有本地镜像 删除 docker rmi image-id 删除指定的本地镜像 https://hub.docker.com/ 2）、容器操作软件镜像（QQ安装程序）—-运行镜像—-产生一个容器（正在运行的软件，运行的QQ）； 步骤： 1234567891011121314151617181920212223242526272829301、搜索镜像[root@localhost ~]# docker search tomcat2、拉取镜像[root@localhost ~]# docker pull tomcat3、根据镜像启动容器docker run --name mytomcat -d tomcat:latest4、docker ps 查看运行中的容器5、 停止运行中的容器docker stop 容器的id6、查看所有的容器docker ps -a7、启动容器docker start 容器id8、删除一个容器 docker rm 容器id9、启动一个做了端口映射的tomcat[root@localhost ~]# docker run -d -p 8888:8080 tomcat-d：后台运行-p: 将主机的端口映射到容器的一个端口 主机端口:容器内部的端口10、为了演示简单关闭了linux的防火墙service firewalld status ；查看防火墙状态service firewalld stop：关闭防火墙11、查看容器的日志docker logs container-name/container-id更多命令参看https://docs.docker.com/engine/reference/commandline/docker/可以参考每一个镜像的文档 3）、安装MySQL示例1docker pull mysql 错误的启动 1234567891011121314151617[root@localhost ~]# docker run --name mysql01 -d mysql42f09819908bb72dd99ae19e792e0a5d03c48638421fa64cce5f8ba0f40f5846mysql退出了[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES42f09819908b mysql "docker-entrypoint.sh" 34 seconds ago Exited (1) 33 seconds ago mysql01538bde63e500 tomcat "catalina.sh run" About an hour ago Exited (143) About an hour ago compassionate_goldstinec4f1ac60b3fc tomcat "catalina.sh run" About an hour ago Exited (143) About an hour ago lonely_fermi81ec743a5271 tomcat "catalina.sh run" About an hour ago Exited (143) About an hour ago sick_ramanujan//错误日志[root@localhost ~]# docker logs 42f09819908berror: database is uninitialized and password option is not specified You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD；这个三个参数必须指定一个 正确的启动 12345[root@localhost ~]# docker run --name mysql01 -e MYSQL_ROOT_PASSWORD=123456 -d mysqlb874c56bec49fb43024b3805ab51e9097da779f2f572c22c695305dedd684c5f[root@localhost ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb874c56bec49 mysql "docker-entrypoint.sh" 4 seconds ago Up 3 seconds 3306/tcp mysql01 做了端口映射 12345[root@localhost ~]# docker run -p 3306:3306 --name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysqlad10e4bc5c6a0f61cbad43898de71d366117d120e39db651844c0e73863b9434[root@localhost ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESad10e4bc5c6a mysql "docker-entrypoint.sh" 4 seconds ago Up 2 seconds 0.0.0.0:3306-&gt;3306/tcp mysql02 几个其他的高级操作 123456docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag把主机的/conf/mysql文件夹挂载到 mysqldocker容器的/etc/mysql/conf.d文件夹里面改mysql的配置文件就只需要把mysql配置文件放在自定义的文件夹下（/conf/mysql）docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci指定mysql的一些配置参数 六、SpringBoot与数据访问1、JDBC123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; 123456spring: datasource: username: root password: 123456 url: jdbc:mysql://192.168.15.22:3306/jdbc driver-class-name: com.mysql.jdbc.Driver 效果： ​ 默认是用org.apache.tomcat.jdbc.pool.DataSource作为数据源； ​ 数据源的相关配置都在DataSourceProperties里面； 自动配置原理： org.springframework.boot.autoconfigure.jdbc： 1、参考DataSourceConfiguration，根据配置创建数据源，默认使用Tomcat连接池；可以使用spring.datasource.type指定自定义的数据源类型； 2、SpringBoot默认可以支持； 1org.apache.tomcat.jdbc.pool.DataSource、HikariDataSource、BasicDataSource、 3、自定义数据源类型 1234567891011121314/** * Generic DataSource configuration. */@ConditionalOnMissingBean(DataSource.class)@ConditionalOnProperty(name = "spring.datasource.type")static class Generic &#123; @Bean public DataSource dataSource(DataSourceProperties properties) &#123; //使用DataSourceBuilder创建数据源，利用反射创建响应type的数据源，并且绑定相关属性 return properties.initializeDataSourceBuilder().build(); &#125;&#125; 4、DataSourceInitializer：ApplicationListener； ​ 作用： ​ 1）、runSchemaScripts();运行建表语句； ​ 2）、runDataScripts();运行插入数据的sql语句； 默认只需要将文件命名为： 123456schema-*.sql、data-*.sql默认规则：schema.sql，schema-all.sql；可以使用 schema: - classpath:department.sql 指定位置 5、操作数据库：自动配置了JdbcTemplate操作数据库 2、整合Druid数据源12345678910111213141516171819202122232425262728293031323334353637383940414243导入druid数据源@Configurationpublic class DruidConfig &#123; @ConfigurationProperties(prefix = "spring.datasource") @Bean public DataSource druid()&#123; return new DruidDataSource(); &#125; //配置Druid的监控 //1、配置一个管理后台的Servlet @Bean public ServletRegistrationBean statViewServlet()&#123; ServletRegistrationBean bean = new ServletRegistrationBean(new StatViewServlet(), "/druid/*"); Map&lt;String,String&gt; initParams = new HashMap&lt;&gt;(); initParams.put("loginUsername","admin"); initParams.put("loginPassword","123456"); initParams.put("allow","");//默认就是允许所有访问 initParams.put("deny","192.168.15.21"); bean.setInitParameters(initParams); return bean; &#125; //2、配置一个web监控的filter @Bean public FilterRegistrationBean webStatFilter()&#123; FilterRegistrationBean bean = new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; initParams = new HashMap&lt;&gt;(); initParams.put("exclusions","*.js,*.css,/druid/*"); bean.setInitParameters(initParams); bean.setUrlPatterns(Arrays.asList("/*")); return bean; &#125;&#125; 3、整合MyBatis12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 步骤： ​ 1）、配置数据源相关属性（见上一节Druid） ​ 2）、给数据库建表 ​ 3）、创建JavaBean 4）、注解版1234567891011121314151617//指定这是一个操作数据库的mapper@Mapperpublic interface DepartmentMapper &#123; @Select("select * from department where id=#&#123;id&#125;") public Department getDeptById(Integer id); @Delete("delete from department where id=#&#123;id&#125;") public int deleteDeptById(Integer id); @Options(useGeneratedKeys = true,keyProperty = "id") @Insert("insert into department(departmentName) values(#&#123;departmentName&#125;)") public int insertDept(Department department); @Update("update department set departmentName=#&#123;departmentName&#125; where id=#&#123;id&#125;") public int updateDept(Department department);&#125; 问题： 自定义MyBatis的配置规则；给容器中添加一个ConfigurationCustomizer； 1234567891011121314@org.springframework.context.annotation.Configurationpublic class MyBatisConfig &#123; @Bean public ConfigurationCustomizer configurationCustomizer()&#123; return new ConfigurationCustomizer()&#123; @Override public void customize(Configuration configuration) &#123; configuration.setMapUnderscoreToCamelCase(true); &#125; &#125;; &#125;&#125; 123456789使用MapperScan批量扫描所有的Mapper接口；@MapperScan(value = "com.atguigu.springboot.mapper")@SpringBootApplicationpublic class SpringBoot06DataMybatisApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBoot06DataMybatisApplication.class, args); &#125;&#125; 5）、配置文件版123mybatis: config-location: classpath:mybatis/mybatis-config.xml 指定全局配置文件的位置 mapper-locations: classpath:mybatis/mapper/*.xml 指定sql映射文件的位置 更多使用参照 http://www.mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/ 4、整合SpringData JPA1）、SpringData简介 2）、整合SpringData JPAJPA:ORM（Object Relational Mapping）； 1）、编写一个实体类（bean）和数据表进行映射，并且配置好映射关系； 12345678910111213//使用JPA注解配置映射关系@Entity //告诉JPA这是一个实体类（和数据表映射的类）@Table(name = "tbl_user") //@Table来指定和哪个数据表对应;如果省略默认表名就是user；public class User &#123; @Id //这是一个主键 @GeneratedValue(strategy = GenerationType.IDENTITY)//自增主键 private Integer id; @Column(name = "last_name",length = 50) //这是和数据表对应的一个列 private String lastName; @Column //省略默认列名就是属性名 private String email; 2）、编写一个Dao接口来操作实体类对应的数据表（Repository） 123//继承JpaRepository来完成对数据库的操作public interface UserRepository extends JpaRepository&lt;User,Integer&gt; &#123;&#125; 3）、基本的配置JpaProperties 1234567spring: jpa: hibernate:# 更新或者创建数据表结构 ddl-auto: update# 控制台显示SQL show-sql: true 七、启动配置原理几个重要的事件回调机制 配置在META-INF/spring.factories ApplicationContextInitializer SpringApplicationRunListener 只需要放在ioc容器中 ApplicationRunner CommandLineRunner 启动流程： 1、创建SpringApplication对象12345678910111213141516initialize(sources);private void initialize(Object[] sources) &#123; //保存主配置类 if (sources != null &amp;&amp; sources.length &gt; 0) &#123; this.sources.addAll(Arrays.asList(sources)); &#125; //判断当前是否一个web应用 this.webEnvironment = deduceWebEnvironment(); //从类路径下找到META-INF/spring.factories配置的所有ApplicationContextInitializer；然后保存起来 setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); //从类路径下找到ETA-INF/spring.factories配置的所有ApplicationListener setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); //从多个配置类中找到有main方法的主配置类 this.mainApplicationClass = deduceMainApplicationClass();&#125; 2、运行run方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; FailureAnalyzers analyzers = null; configureHeadlessProperty(); //获取SpringApplicationRunListeners；从类路径下META-INF/spring.factories SpringApplicationRunListeners listeners = getRunListeners(args); //回调所有的获取SpringApplicationRunListener.starting()方法 listeners.starting(); try &#123; //封装命令行参数 ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); //准备环境 ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); //创建环境完成后回调SpringApplicationRunListener.environmentPrepared()；表示环境准备完成 Banner printedBanner = printBanner(environment); //创建ApplicationContext；决定创建web的ioc还是普通的ioc context = createApplicationContext(); analyzers = new FailureAnalyzers(context); //准备上下文环境;将environment保存到ioc中；而且applyInitializers()； //applyInitializers()：回调之前保存的所有的ApplicationContextInitializer的initialize方法 //回调所有的SpringApplicationRunListener的contextPrepared()； // prepareContext(context, environment, listeners, applicationArguments, printedBanner); //prepareContext运行完成以后回调所有的SpringApplicationRunListener的contextLoaded（）； //s刷新容器；ioc容器初始化（如果是web应用还会创建嵌入式的Tomcat）；Spring注解版 //扫描，创建，加载所有组件的地方；（配置类，组件，自动配置） refreshContext(context); //从ioc容器中获取所有的ApplicationRunner和CommandLineRunner进行回调 //ApplicationRunner先回调，CommandLineRunner再回调 afterRefresh(context, applicationArguments); //所有的SpringApplicationRunListener回调finished方法 listeners.finished(context, null); stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); &#125; //整个SpringBoot应用启动完成以后返回启动的ioc容器； return context; &#125; catch (Throwable ex) &#123; handleRunFailure(context, listeners, analyzers, ex); throw new IllegalStateException(ex); &#125;&#125; 3、事件监听机制配置在META-INF/spring.factories ApplicationContextInitializer 123456public class HelloApplicationContextInitializer implements ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; &#123; @Override public void initialize(ConfigurableApplicationContext applicationContext) &#123; System.out.println("ApplicationContextInitializer...initialize..."+applicationContext); &#125;&#125; SpringApplicationRunListener 123456789101112131415161718192021222324252627282930313233public class HelloSpringApplicationRunListener implements SpringApplicationRunListener &#123; //必须有的构造器 public HelloSpringApplicationRunListener(SpringApplication application, String[] args)&#123; &#125; @Override public void starting() &#123; System.out.println("SpringApplicationRunListener...starting..."); &#125; @Override public void environmentPrepared(ConfigurableEnvironment environment) &#123; Object o = environment.getSystemProperties().get("os.name"); System.out.println("SpringApplicationRunListener...environmentPrepared.."+o); &#125; @Override public void contextPrepared(ConfigurableApplicationContext context) &#123; System.out.println("SpringApplicationRunListener...contextPrepared..."); &#125; @Override public void contextLoaded(ConfigurableApplicationContext context) &#123; System.out.println("SpringApplicationRunListener...contextLoaded..."); &#125; @Override public void finished(ConfigurableApplicationContext context, Throwable exception) &#123; System.out.println("SpringApplicationRunListener...finished..."); &#125;&#125; 配置（META-INF/spring.factories） 12345org.springframework.context.ApplicationContextInitializer=\com.atguigu.springboot.listener.HelloApplicationContextInitializerorg.springframework.boot.SpringApplicationRunListener=\com.atguigu.springboot.listener.HelloSpringApplicationRunListener 只需要放在ioc容器中 ApplicationRunner 1234567@Componentpublic class HelloApplicationRunner implements ApplicationRunner &#123; @Override public void run(ApplicationArguments args) throws Exception &#123; System.out.println("ApplicationRunner...run...."); &#125;&#125; CommandLineRunner 1234567@Componentpublic class HelloCommandLineRunner implements CommandLineRunner &#123; @Override public void run(String... args) throws Exception &#123; System.out.println("CommandLineRunner...run..."+ Arrays.asList(args)); &#125;&#125; 八、自定义starterstarter： ​ 1、这个场景需要使用到的依赖是什么？ ​ 2、如何编写自动配置 12345678910111213@Configuration //指定这个类是一个配置类@ConditionalOnXXX //在指定条件成立的情况下自动配置类生效@AutoConfigureAfter //指定自动配置类的顺序@Bean //给容器中添加组件@ConfigurationPropertie结合相关xxxProperties类来绑定相关的配置@EnableConfigurationProperties //让xxxProperties生效加入到容器中自动配置类要能加载将需要启动就加载的自动配置类，配置在META-INF/spring.factoriesorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\ ​ 3、模式： 启动器只用来做依赖导入； 专门来写一个自动配置模块； 启动器依赖自动配置；别人只需要引入启动器（starter） mybatis-spring-boot-starter；自定义启动器名-spring-boot-starter 步骤： 1）、启动器模块 12345678910111213141516171819202122&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.atguigu.starter&lt;/groupId&gt; &lt;artifactId&gt;atguigu-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!--启动器--&gt; &lt;dependencies&gt; &lt;!--引入自动配置模块--&gt; &lt;dependency&gt; &lt;groupId&gt;com.atguigu.starter&lt;/groupId&gt; &lt;artifactId&gt;atguigu-spring-boot-starter-autoconfigurer&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2）、自动配置模块 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.atguigu.starter&lt;/groupId&gt; &lt;artifactId&gt;atguigu-spring-boot-starter-autoconfigurer&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;atguigu-spring-boot-starter-autoconfigurer&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.10.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!--引入spring-boot-starter；所有starter的基本配置--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 1234567891011121314151617181920212223242526package com.atguigu.starter;import org.springframework.boot.context.properties.ConfigurationProperties;@ConfigurationProperties(prefix = "atguigu.hello")public class HelloProperties &#123; private String prefix; private String suffix; public String getPrefix() &#123; return prefix; &#125; public void setPrefix(String prefix) &#123; this.prefix = prefix; &#125; public String getSuffix() &#123; return suffix; &#125; public void setSuffix(String suffix) &#123; this.suffix = suffix; &#125;&#125; 123456789101112131415161718package com.atguigu.starter;public class HelloService &#123; HelloProperties helloProperties; public HelloProperties getHelloProperties() &#123; return helloProperties; &#125; public void setHelloProperties(HelloProperties helloProperties) &#123; this.helloProperties = helloProperties; &#125; public String sayHellAtguigu(String name)&#123; return helloProperties.getPrefix()+"-" +name + helloProperties.getSuffix(); &#125;&#125; 12345678910111213141516171819202122package com.atguigu.starter;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.autoconfigure.condition.ConditionalOnWebApplication;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configuration@ConditionalOnWebApplication //web应用才生效@EnableConfigurationProperties(HelloProperties.class)public class HelloServiceAutoConfiguration &#123; @Autowired HelloProperties helloProperties; @Bean public HelloService helloService()&#123; HelloService service = new HelloService(); service.setHelloProperties(helloProperties); return service; &#125;&#125; 更多SpringBoot整合示例https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples]]></content>
      <categories>
        <category>Java框架</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat]]></title>
    <url>%2F2019%2F04%2F25%2F1_Tomcat%2F</url>
    <content type="text"><![CDATA[Tomcat一、WEB概述 原始年代1990-1992:1990年，HTML标记语言的出现标志Web开发时代的到来.B/S架构开始在之后的岁月中不断的发展壮大，攻城略地蚕食传统C/S的领域。如同所有的新生事物一样，在web的史前岁月，web的开发技术在在html标记诞生后，无论是在服务端还客户端都缓慢的发展着，在相当长的一个时间内，它并未像今天这样辉煌，甚至于只是静态的文本标识.关键字：HTML 技术特性：静态文本显示，表现力和交互能力不足。如：[hao123官网][1] 封建诸侯年代1993-1996:1993年，NCSA提出了CGI1.0草案。Web开发终于迎来了它的第二次重大飞跃，伴随着CGI，带来Web的动态处理能力，CGI就是这个时代的国王。(服务器端动态生成内容)1994年，PHP1996年，ASP关键字：CGI(Common Gateway Interface )(Perl&amp;&amp;C&amp;&amp;Python)技术特性：实现了客户端和服务器端的动态交互，在程序代码中写html标记，是面向过程的开发方式，用多进程运行。注： CGI是Web服务器端组件，都能产生Web动态页面输出 工业文明1996-1999:1997年，Sun公司推出了Servlet规范。Java阵营终于迎来了自己的web英雄。1998年JSP技术诞生，也是基于Servlet，可以写Java代码，可以使用Servlet+JSP+JavaBean实现JavaWeb开发1998年，Sun发布了EJB1.0标准（重量级JavaEE规范，目前学习的为轻量级的规范）1999年，Sun正式发布了J2EE的第一个版本，紧接着，遵循J2EE，为企业级应用提供支持平台的各类应用服务器争先恐后的涌现出来（WebSphere，WebLogic，JBoss），（同时2001微软发布了ASP.NET技术）##二、CS和BS的区别 CS和BS是软件架构模式: C/S: Client/Server :客户端/服务端架构: B/S: Browser/Server:浏览器/服务器架构: C/S:VB,Delphi,VC++,C#,Java awt/swing:比如桌面QQ,扫雷,拱猪等运行在桌面的程序.特点: 在服务端主要就是一个数据库,把所有业务逻辑以及界面的渲染操作交给客户端完成.优点: 较安全,用户界面很丰富,用户体验不错等缺点: 每次升级都需要重新安装,针对于不同的操作系统开发,可移植性很差. B/S:JSP,ASP,PHP:基于浏览器访问的应用,把业务逻辑交给服务端完成,客户端仅仅只做界面渲染和数据交换.特点: BS是特殊的CS,此时浏览器充当了客户端.基于HTTP协议的.优点: 只开发服务端,可以跨平台,移植性很强等.缺点: 安全性较低,用户体验较差等. 现在的应用综合了BS和CS的优点:部分应用不再是单纯BS.富客户端技术: RIA(Rich Internet Applications), 客户端会处理部分的业务逻辑, 也会做界面的渲染和数据交互. 界面丰富好比是CS.EasyUI,Flex,Extjs,Java FX等 瘦客户端技术: 基于传统的html界面,客户端只界面的渲染和数据交互.(传统的BS) 三、服务器 服务器:软件服务器:就是一个软件.硬件服务器:安装了软件服务器的主机，如果是做运维就会接触到，只做开发接触较少 分类: 1.http服务器：专门处理静态页面的，用的比较少 2.JavaWeb服务器： 比如：Tomcat等. 仅仅实现了JavaEE 13 种规范中的几（6）个规范.(Servlet容器) 3.应用服务器: 实现了JavaEE13种规范.WebSphere(IBM),WebLogic(Oracle),JBoss(red hat) 不同版本服务器所对应的Servlet/JSP规范1. WebSocketWebSocket是HTML5开始提供的一种在单个 TCP 连接上进行全双工通讯的协议。在WebSocket API中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送。2. EL表达式EL（Expression Language） 是为了使JSP写起来更加简单。表达式语言的灵感来自于ECMAScript 和 XPath 表达式语言，它提供了在 JSP 中简化表达式的方法，让Jsp的代码更加简化。语法结构: ${expression}3. 个版本对比 [参考链接][2] Servlet JSP EL WebSocket Tomcat Java 4.0 2.3 3.0 1.1 9.0.x 8 and later 3.1 2.3 3.0 1.1 8.5.x 7 and later 3.1 2.3 3.0 1.1 8.0.x 7 and later 3.0 2.2 2.2 1.1 7.0.x 6 and later (WebSocket 1.0 requires 7) 2.5 2.1 2.1 N/A 6.0.x 5 and later 2.4 2.0 N/A N/A 5.5.x 1.4 and later 2.3 1.2 N/A N/A 4.1.x 1.3 and later 2.2 1.1 N/A N/A 3.3.x 1.1 and later ## 四、Tomcat服务器 ### 4.1 Tomcat的安装 Tomcat是使用Java语言编写的一个服务器(程序),要运行Tomcat,必须得有jre. 安装启动: 版本 32位的JDK —&gt;32位的Eclipse—&gt;32位Tomcat 64位的JDK —&gt;64位的Eclipse—&gt;64位Tomcat 安装目录不能使中文的，并且安装路径不允许出现空格。 如: D:\SoftWare\Tomcat\apache-tomcat-8.5.29:我们把该路径称之为Tomcat的根路径 启动Tomcat服务器: Tomcat根/bin/startup.bat 注意: 必须先配置JAVA_HOME或者JRE_HOME的环境变量: 一般的我们只配置JAVA_HOME:配置为JDK的根路径（即JDK的安装路径） JAVA_HOME=D:\SoftWare\Java\jdk1.8.0_152 配置好之后,再点击Tomcat根路径/bin/startup.bat:知道控制台没有打印重大的错误,Exception,没有一闪而过,就表示启动成功。 Tomcat的默认端口是8080: 访问: 打开浏览器: http://服务器所在主机的IP:服务器的端口号/资源名字 http://服务器所在主机的名字:服务器的端口号/资源名字 若服务在本机: http://本机的IP:服务器的端口号/资源名字 http://127.0.0.1:服务器的端口号/资源名字 http://localhost:服务器的端口号/资源名字 Tomcat根下的目录: bin:存放了启动/关闭Tomcat的等应用程序工具. conf:存放了Tomcat软件的一些配置文件. lib:存放了Tomcat软件启动运行的依赖的jar文件. logs:存放Tomcat 日志记录(成功,失败) temp:临时目录,比如把上传的大文件存放于临时目录 webapps:里面存放需要部署的 JavaWeb 项目目录. work:工作目录,存放了jsp翻译成Servlet的java文件以及字节码文件. 4.2 Tomcat常见问题 未配置JAVA_HOME![image_1c9oqm1g58f01oalo25ud4dnkm.png-25.4kB][3] 版本不对应![image_1c9o5kcbem5v1n9qie311ok1nagm.png-25.4kB][4]原因在于64为的jdk安装了32为的Tomcat 出现404的页面![image_1c9o5qum7c1al011cgrhp316jn1g.png-28.6kB][5]我们自己把资源的路径写错了,自己检查,如访问了一个不存在的页面! Tomcat还未关闭,又再次重新启动Tomcat![image_1c9o603331sbbhlsld01p53bg62n.png-20.2kB][6]出现:java.net.BindException: Address already in use: JVM_Bind异常该程序的端口以及被其他程序所占用:注意:出错之后,要习惯去查看日志信息:Tomcat根/logs/catalina.2018-03-29.log Tomcat下的配置文件的结构不能乱改1229-Mar-2018 14:13:12.745 严重 [main] org.apache.tomcat.util.digester.Digester.fatalError Parse Fatal Error at line 165 column 7: 元素类型 "Host" 必须由匹配的结束标记 "&lt;/Host&gt;" 终止。 org.xml.sax.SAXParseException; systemId: file:/D:/SoftWare/Tomcat/apache-tomcat-8.5.29/conf/server.xml; lineNumber: 165; columnNumber: 7; 元素类型 "Host" 必须由匹配的结束标记 "&lt;/Host&gt;" 终止。 要保证XML内容编码和文件编码相同,若有中文,建议使用UTF-8,否则不能使用中文1229-Mar-2018 14:24:21.195 警告 [main] org.apache.catalina.startup.Catalina.load Catalina.start using conf/server.xml: com.sun.org.apache.xerces.internal.impl.io.MalformedByteSequenceException: 1 字节的 UTF-8 序列的字节 1 无效。 4.3 常见配置 端口号的配置Tomcat的默认端口是8080,HTTP协议的默认端口是80; 步骤: 1.1 进入Tomcat根/conf/找到server.xml文件 1.2 默认是在第70行,Connector元素的 port属性: 1.3 配置为80端口(80端口是http协议的默认端口):![image_1c9o9be6d1i21nakmhu1ts719a25b.png-12.6kB][7] 虚拟主机的配置![image_1c9o9a9511od4eem1ojl18kt2u24u.png-33.7kB][8] 五、HTTP协议HTTP：特点:无状态, 默认端口就是80https: 收费，比HTTP安全 什么是HTTPWEB浏览器与WEB服务器之间的一问一答的交互过程必须遵循一定的规则，这个规则就是HTTP协议。HTTP是hypertext transfer protocol（超文本传输协议）的简写，它是TCP/IP协议之上的一个应用层协议，用于定义WEB浏览器与WEB服务器之间交换数据的过程以及数据本身的格式。 HTTP协议到底约束了什么![image_1c9q96t8hko31teplk21uru1oit16.png-26.7kB][9] 约束了浏览器以何种格式向服务端发送数据: 约束了服务器应该以何种格式来接收客户端发送的数据: 约束了服务器应该以何种格式来反馈数据给浏览器; 约束了浏览器应该以何种格式来接收服务器反馈的数据. 请求与响应 浏览器给服务器发送数据:一次请求 服务器给浏览器反馈数据:一次响应 HTTP协议的版本 HTTP/1.0 方式： 若请求的有N个资源,得建立N次连接,发送N次请求,接收N次响应,关闭N次连接. 每次请求的之间都要建立单独的连接,请求,响应,响应完关闭该次连接: 缺点: 每请求一个资源都要单独的建立新的连接,请求完并关闭连接. 解决方案: 能在一次连接之间,多次请求,多次响应,响应完之后再关闭连接. HTTP1.1规范: 方式： 能在一次连接之间,多次请求,多次响应,响应完之后再关闭连接. 使用浏览器查看响应和请求头信息 请求消息的结构：一个请求行、若干请求头、以请求体，其中的一些请求头和实体内容都是可选的，请求头和实体内容之间要用空行隔开。 请求行:GET /books/java.html HTTP/1.1请求行信息分3段:请求方式: GET请求资源:/books/java.html协议的版本:HTTP/1.1 请求头：![image_1c9qr2ts7l7o16vr1fppcel1n111m.png-39.3kB][10]常见的请求头: Accept：浏览器可接受的MIME类型（Tomcat安装目录/conf/web.xml中查找） Accept-Charset：告知服务器，客户端支持哪种字符集 Accept-Encoding：浏览器能够进行解码的数据编码方式 Accept-Language：浏览器支持的语言。 Referer：当前页面由哪个页面访问过来的。 Content-Type：通知服务器，请求正文的MIME类型。取值：application/x-www-form-urlencoded默认值.对应的是form表单的enctype属性MIME的英文全称是”Multipurpose Internet Mail Extensions” 多用途互联网邮件扩展，它是一个互联网标准，在1992年最早应用于电子邮件系统，但后来也应用到浏览器。服务器会将它们发送的多媒体数据的类型告诉浏览器，文件内容的类型: MIME类型就是设定某种扩展名的文件用一种应用程序来打开的方式类型 Content-Length：请求正文的长度 If-Modified-Since:通知服务器，缓存的文件的最后修改时间。 User-Agent:通知服务器，浏览器类型. Connection:表示是否需要持久连接。如果服务器看到这里的值为“Keep-Alive”，或者看到请求使用的是HTTP 1.1（HTTP 1.1默认进行持久连接 ) Cookie:这是最重要的请求头信息之一（会话有关） 请求体: 使用POST请求的时候才有 响应消息结构：一个状态行、若干响应头、以及实体内容，其中的一些消息头和实体内容都是可选的，响应头和实体内容之间要用空行隔开。 状态行HTTP/1.1 200 OKHTTP/1.1 404 Not Found常见的响应状态码: 200:表示完全正常,OK: 404:表示客户端有问题,就是我们的请求资源的路径不存在. 500:表示服务端有问题,不是说是服务器有问题,而是我们的后台Java代码有问题. 响应头![image_1c9qsj1g9hal8p414b1vr4n552t.png-52.6kB][11] Location：制定转发的地址。需与302/307响应码一同使用 Server：告知客户端服务器使用的容器类型 Content-Encoding：告知客户端服务器发送的数据所采用的压缩格式 Content-Length：告知客户端正文的长度 Content-Type：告知客户端正文的MIME类型 Conent-Type:text/html;charset=UTF-8 Refresh：定期刷新。还可以刷新到其他资源 Refresh:3;URL=otherurl 3秒后刷新到otherurl这个页面 Content-Disposition：指示客户端以下载的方式保存文件。 Content-Disposition：attachment;filename=2.jpg Expires：网页的有效时间。单位是毫秒(等于-1时表示页面立即过期) Cache-Control：no-cache Pragma：no-cache 控制客户端不要缓存 Set-Cookie:SS=Q0=5Lb_nQ; path=/search服务器端发送的Cookie（会话有关） GET和POST的请求区别 GRT和POST都是请求方式: GET:浏览器地址栏:http://localhost/form.html?name=neld&amp;age=18请求行:GET /form.html?name=neld&amp;age=18 HTTP/1.1请求方式是:GET请求资源是: /form.html?name=neld&amp;age=18请求资源包括请求参数:第一个参数使用?和资源连接,其他的参数使用&amp;符号连接.缺点:暴露请求信息,不安全. 请求信息不超过1kb.这样就限定了GET方式不能做图片上传.（受限于url长度，具体的数值取决于浏览器和服务器的限制）直接在地里输入地址,回车请求,超链接的请求都属于GET方式.表单中的method=get POST:浏览器地址栏:http://localhost/form.html#不再有请求信息:请求行:POST /form.html HTTP/1.1POST方式的参数全部在请求的实体中:隐藏了请求信息,较安全:POST方式没有限制请求的数据大小,可以做图片上传.目前HTTP的请求方式只有两种:GET/POST:doPost/doGet ![1]( https://www.hao123.com/index.html![2]( http://tomcat.apache.org/whichversion.html]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm]]></title>
    <url>%2F2019%2F04%2F25%2F1_Storm%2F</url>
    <content type="text"><![CDATA[Storm为何使用Storm？Apache Storm是一个免费的开源分布式实时计算系统。Storm可以轻松可靠地处理无限数据流，实时处理Hadoop为批处理所做的工作。Storm非常简单，可以与任何编程语言一起使用，并且使用起来很有趣！ Storm有许多用例：实时分析，在线机器学习，连续计算，分布式RPC，ETL等。Storm很快：一个基准测试表示每个节点每秒处理超过一百万个元组。它具有可扩展性，容错性，可确保您的数据得到处理，并且易于设置和操作。 Storm集成了您已经使用的排队和数据库技术。Storm拓扑消耗数据流并以任意复杂的方式处理这些流，然后在计算的每个阶段之间重新划分流。阅读教程中的更多内容。 Storm架构 Nimbus：master节点：监控topo的运行状态，分发task给supervisor supervisor：从节点。每个这个节点都会有多个worker进程，负责代理task给worker worker：执行特定的task，但是本身不执行任务，创建一个executor，然executor执行 task：实际要处理的业务逻辑 executor：运行task Storm与Hadoop的区别 storm Hadoop 实时流处理 批处理 无状态 有状态 使用zookeeper协同的主从架构 不需要zookeeper的主从架构 每秒处理数万消息 MR好几分钟甚至好几个小时 不会主动停止 终有完成的时候 storm的优点 跨平台 可伸缩 低延迟，秒级 高容错 核心概念http://storm.apache.org/releases/1.2.2/Concepts.html Topologies实时应用程序的逻辑被打包到Storm拓扑中。Storm拓扑类似于MapReduce作业。一个关键的区别是MapReduce作业最终完成，而拓扑结构永远运行（当然，直到你杀死它）。拓扑是与流分组连接的spouts 和Bolts的图形。形成一个有向图，边就是stream Tuple（元组）主要的数据结构，有序元素的列表 StreamTuple的序列。流是Storm中的核心抽象。流是无限的元组序列，以分布式方式并行处理和创建。流定义了一个模式，该模式命名流的元组中的字段。默认情况下，元组可以包含整数，长整数，短整数，字节，字符串，双精度数，浮点数，布尔值和字节数组。您还可以定义自己的序列化程序，以便可以在元组中本机使用自定义类型。 Spouts数据流源头，可以读取kafka队列消息，可以自定义。通常，spouts将从外部源读取元组并将它们发送到拓扑中（例如，Kestrel队列或Twitter API）。Spouts可以发出多个流。 Bolts转接头，逻辑处理单元。spout的数据传递给bolt，bolt计算完成后产生新的数据。可以执行过滤，函数，聚合，连接，与数据库交互等操作。 Task每个Spouts或Bolts在整个集群中执行任意数量的Task。执行实际的任务处理。 WorkersTopologies在一个或多个worker 进程中执行。每个worker 进程都是物理JVM，并执行Topologies的所有Task的子集。例如，如果Topologies的组合并行度为300且分配了50个worker，则每个worker将执行6个任务（作为工作线程中的线程）。Storm试图在所有worker之间平均分配任务。 Nimbusmaster节点：监控topo的运行状态，分发task给supervisor，分析top并收集需要运行的task。本身无状态，依靠ZK获取集群的状态 supervisor从节点。每个这个节点都会有多个worker进程，负责代理task给worker，worker再孵化执行线程最终运行task，storm使用内部消息系统在nimbus和supervisor之间进行通信。接受nimbus指令，管理worker进程完成task派发。 Executor本质上由worker进行孵化出来的一个进程而已，Executor运行任务都属于同一个Spout或bolt Storm工作流程一个工作的Storm集群应该有一个Nimbus和一个或多个supervisors。另一个重要的节点是Apache ZooKeeper，它将用于nimbus和supervisors之间的协调。现在让我们仔细看看Apache Storm的工作流程 最初，nimbus将等待“Storm拓扑”提交给它。 一旦提交拓扑，它将处理拓扑并收集要执行的所有任务和任务将被执行的顺序。 然后，nimbus将任务均匀分配给所有可用的supervisors。 在特定的时间间隔，所有supervisor将向nimbus发送心跳以通知它们仍然活着。 当supervisor终止并且不向心跳发送心跳时，则nimbus将任务分配给另一个supervisor。 当nimbus本身终止时，supervisor将在没有任何问题的情况下对已经分配的任务进行工作。说白了就是继续执行自己的任务 一旦所有的任务都完成后，supervisor将等待新的任务进去。 同时，终止nimbus将由服务监控工具自动重新启动。 重新启动的网络将从停止的地方继续。同样，终止supervisor也可以自动重新启动。由于网络管理程序和supervisor都可以自动重新启动，并且两者将像以前一样继续，因此Storm保证至少处理所有任务一次。 一旦处理了所有拓扑，则nimbus等待新的拓扑到达，并且类似地，管理器等待新的任务。 默认情况下，Storm集群中有两种模式： 本地模式此模式用于开发，测试和调试，因为它是查看所有拓扑组件协同工作的最简单方法。在这种模式下，我们可以调整参数，使我们能够看到我们的拓扑如何在不同的Storm配置环境中运行。在本地模式下，storm拓扑在本地机器上在单个JVM中运行。 生产模式在这种模式下，我们将拓扑提交到工作Storm集群，该集群由许多进程组成，通常运行在不同的机器上。如在storm的工作流中所讨论的，工作集群将无限地运行，直到它被关闭。 Storm工作实例移动呼叫日志分析器 移动呼叫及其持续时间将作为对Storm的输入，Storm将处理和分组在相同呼叫者和接收者之间的呼叫及其呼叫总数。 在我们的场景中，我们需要收集呼叫日志详细信息。呼叫日志的信息包含。 主叫号码 接收号码 持续时间 由于我们没有呼叫日志的实时信息，我们将生成假呼叫日志。假信息将使用Random类创建。完整的程序代码如下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import java.util.*;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Values;import backtype.storm.topology.IRichSpout;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.spout.SpoutOutputCollector;import backtype.storm.task.TopologyContext;/** * Spout类，负责产生数据流 */public class CallLogReaderSpout implements IRichSpout &#123; //Spout输出收集器 private SpoutOutputCollector collector; //是否完成 private boolean completed = false; //上下文 private TopologyContext context; //随机数生成器 private Random random = new Random(); private Integer idx = 0; @Override public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; this.context = context; this.collector = collector; &#125; //下一个元组 @Override public void nextTuple() &#123; if(this.idx &lt;= 1000) &#123; List&lt;String&gt; mobileNumbers = new ArrayList&lt;String&gt;(); mobileNumbers.add("101"); mobileNumbers.add("102"); mobileNumbers.add("103"); mobileNumbers.add("104"); Integer localIdx = 0; while(localIdx++ &lt; 100 &amp;&amp; this.idx++ &lt; 1000) &#123; //主叫 String caller = mobileNumbers.get(random.nextInt(4)); //被叫 String callee = mobileNumbers.get(random.nextInt(4)); while(caller == callee) &#123; callee = mobileNumbers.get(random.nextInt(4)); &#125; //模拟通话时长 Integer duration = randomGenerator.nextInt(60); //输出的元组数据 this.collector.emit(new Values(caller, callee, duration)); &#125; &#125; &#125; //定义输出的字段名称 @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("from", "to", "duration")); &#125; // @Override public void close() &#123;&#125; public boolean isDistributed() &#123; return false; &#125; @Override public void activate() &#123;&#125; @Override public void deactivate() &#123;&#125; @Override public void ack(Object msgId) &#123;&#125; @Override public void fail(Object msgId) &#123;&#125; @Override public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.HashMap;import java.util.Map;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Values;import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.IRichBolt;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.tuple.Tuple;//创建通话记录的boltpublic class CallLogCreatorBolt implements IRichBolt &#123; //创建OutputCollector实例，收集并发出元组以产生输出 private OutputCollector collector; @Override public void prepare(Map conf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; @Override public void execute(Tuple tuple) &#123; //获取主叫 String from = tuple.getString(0); //获取被叫 String to = tuple.getString(1); //获取通话时长 Integer duration = tuple.getInteger(2); //产生新的元组 collector.emit(new Values(from + " - " + to, duration)); &#125; @Override public void cleanup() &#123;&#125; //重新定义输出字段的名称 @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("call", "duration")); &#125; @Override public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.util.HashMap;import java.util.Map;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Values;import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.IRichBolt;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.tuple.Tuple;//通话记录计数器public class CallLogCounterBolt implements IRichBolt &#123; Map&lt;String, Integer&gt; counterMap; private OutputCollector collector; @Override public void prepare(Map conf, TopologyContext context, OutputCollector collector) &#123; this.counterMap = new HashMap&lt;String, Integer&gt;(); this.collector = collector; &#125; @Override public void execute(Tuple tuple) &#123; //通话记录： String call = tuple.getString(0); //本次通话时长 Integer duration = tuple.getInteger(1); if(!counterMap.containsKey(call))&#123; counterMap.put(call, 1); &#125;else&#123; Integer c = counterMap.get(call) + 1; counterMap.put(call, c); &#125; //消息被确认 collector.ack(tuple); &#125; @Override public void cleanup() &#123; for(Map.Entry&lt;String, Integer&gt; entry:counterMap.entrySet())&#123; System.out.println(entry.getKey()+" : " + entry.getValue()); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("call")); &#125; @Override public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125; &#125; 123456789101112131415161718192021222324252627public class App&#123; public static void main(String[] args)&#123; TopologyBuilder builder = new TopologyBuilder(); //设置spout builder.setSpout("spout", new CallLogReaderSpout()); //设置bolt builder.setBolt("creator-bolt", new CallLogCreatorBolt()).shuffleGrouping("spout"); builder.setBolt("counter-bolt", new CallLogCounterBolt()).fieldsGrouping("creator-bolt", new Fields("call")); Config config = new Config(); config.setDebug(true); //本地模式运行 LocalCluster cluster = new LocalCluster(); //集群模式 StormSubmitter.submitTopology("mytopo", conf, builder.createTopology()); cluster.submitTopology("LogAnalyserStorm", config, builder.createTopology()); Thread.sleep(10000); //停止集群（类似于kill） cluster.shutdown(); &#125;&#125; 在集群上部署topology，导出jar 1storm jar path/to/allmycode.jar org.me.MyTopology arg1 arg2 arg3 Storm 实现wordcountStream Grouping详解Storm里面有7种类型的stream grouping Shuffle Grouping: 随机分组， 随机派发stream里面的tuple，保证每个bolt接收到的tuple数目大致相同。 Fields Grouping：按字段分组，比如按userid来分组，具有同样userid的tuple会被分到相同的Bolts里的一个task，而不同的userid则会被分配到不同的bolts里的task。 All Grouping：广播发送，对于每一个tuple，所有的bolts都会收到。 Global Grouping：全局分组， 这个tuple被分配到storm中的一个bolt的其中一个task。再具体一点就是分配给id值最低的那个task。 Non Grouping：不分组，这stream grouping个分组的意思是说stream不关心到底谁会收到它的tuple。目前这种分组和Shuffle grouping是一样的效果， 有一点不同的是storm会把这个bolt放到这个bolt的订阅者同一个线程里面去执行。 Direct Grouping： 直接分组， 这是一种比较特别的分组方法，用这种分组意味着消息的发送者指定由消息接收者的哪个task处理这个消息。只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的task的id （OutputCollector.emit方法也会返回task的id）。 Local or shuffle grouping：如果目标bolt有一个或者多个task在同一个工作进程中，tuple将会被随机发生给这些tasks。否则，和普通的Shuffle Grouping行为一致。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;figure&gt;&lt;svg version="1.1" id="anim1" width="100%" height="100%" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 552 350" style="enable-background:new 0 0 552 350;" xml:space="preserve"&gt;&lt;style type="text/css"&gt; #anim1 .st2&#123;opacity:0.5502;filter:url(#filter);enable-background:new ;&#125; #anim1 .st3&#123;fill:#FFFFFF;&#125; #anim1 .st4&#123;fill:none;stroke:#E6E6E6;stroke-width:2;stroke-linecap:round;&#125; #anim1 .st5&#123;fill:none;stroke:url(#green-path-1);stroke-width:2;stroke-linecap:round;&#125; #anim1 .st6&#123;fill:none;stroke:url(#green-path-2);stroke-width:2;stroke-linecap:round;&#125; #anim1 .st7&#123;fill:none;stroke:url(#green-path-3);stroke-width:2;stroke-linecap:round;&#125; #anim1 .st8&#123;fill:none;stroke:url(#green-path-4);stroke-width:2;stroke-linecap:round;&#125; #anim1 .st9&#123;fill:#FFFFFF;stroke:#E6E6E6;stroke-width:2;&#125; #anim1 .st10&#123;fill:none;stroke:#E6E6E6;stroke-width:2;&#125; #anim1 .st20&#123;fill:none;stroke:#999999;stroke-width:3;stroke-linecap:round;stroke-linejoin:round;&#125;&lt;/style&gt;&lt;filter width="200%" height="200%" id="filter" filterUnits="objectBoundingBox" y="-50%" x="-50%"&gt; &lt;feGaussianBlur in="SourceGraphic" stdDeviation="18.8378906"&gt;&lt;/feGaussianBlur&gt;&lt;/filter&gt;&lt;g id="a2"&gt;&lt;path id="white-circle-bg-1" class="st3" d="M7.5,65.3A50,50 0,1,1 107.5,65.3A50,50 0,1,1 7.5,65.3" style="stroke-dasharray: 315, 355; stroke-dashoffset: 0;"&gt;&lt;/path&gt;&lt;path id="white-circle-bg-2" class="st3" d="M154.5,65.3A50,50 0,1,1 254.5,65.3A50,50 0,1,1 154.5,65.3" style="stroke-dasharray: 315, 355; stroke-dashoffset: 0;"&gt;&lt;/path&gt;&lt;path id="white-circle-bg-3" class="st3" d="M300.5,65.3A50,50 0,1,1 400.5,65.3A50,50 0,1,1 300.5,65.3" style="stroke-dasharray: 315, 355; stroke-dashoffset: 0;"&gt;&lt;/path&gt;&lt;path id="white-circle-bg-4" class="st3" d="M443.5,65.3A50,50 0,1,1 543.5,65.3A50,50 0,1,1 443.5,65.3" style="stroke-dasharray: 315, 355; stroke-dashoffset: 0;"&gt;&lt;/path&gt;&lt;path id="gray-outline-1" class="st4" d="M56.9,114.6C29.5,114.3,7.5,92,7.5,64.6c0-27.6,22.4-50,50-50s50,22.4,50,50c0,27.6-22.4,50-50,50v15c0,5.5,4.5,10,10,10h15l148-0.1" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="gray-outline-2" class="st4" d="M203.5,114.6c-27.2-0.5-49-22.7-49-50c0-27.6,22.4-50,50-50s50,22.4,50,50c0,27.6-22.4,50-50,50v15c0,5.5,4.5,10,10,10h15h38c5.5,0,10,4.5,10,10v15" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="gray-outline-3" class="st4" d="M349.7,114.6c-27.3-0.4-49.2-22.6-49.2-50c0-27.6,22.4-50,50-50s50,22.4,50,50c0,27.6-22.4,50-50,50v15c0,5.5-4.5,10-10,10h-15h-38c-5.5,0-10,4.5-10,10v15" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="gray-outline-4" class="st4" d="M494,114.6c-0.2,0-0.4,0-0.5,0c-27.6,0-50-22.4-50-50c0-27.6,22.4-50,50-50s50,22.4,50,50c0,27.3-21.8,49.5-49,50v15c0,5.5-4.5,10-10,10h-15h-87.3h-47.7" data-ignore="none"&gt;&lt;/path&gt;&lt;/g&gt;&lt;g id="a1"&gt;&lt;linearGradient id="green-path-1" gradientUnits="userSpaceOnUse" x1="-93.7909" y1="220.2818" x2="-93.7909" y2="219.1985" gradientTransform="matrix(-100 0 0 100 -9260.0879 -21896.832)"&gt;&lt;stop offset="0" style="stop-color:#00BFB3"&gt;&lt;/stop&gt;&lt;stop offset="1" style="stop-color:#31C8FA"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;path id="green-path-1b" class="st5" d="M56.9,114.6C29.5,114.3,7.5,92,7.5,64.6c0-27.6,22.4-50,50-50s50,22.4,50,50c0,27.6-22.4,50-50,50v15c0,5.5,4.5,10,10,10h15l148-0.1" style="stroke-dasharray: 508, 548; stroke-dashoffset: 0;"&gt;&lt;/path&gt;&lt;image id="Bitmap-1" opacity="1" x="20" y="25" width="80" height="80" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/cn/assets/blt96a73ceabd7b6203/beats-01.svg"&gt;&lt;/image&gt;&lt;linearGradient id="green-path-2" gradientUnits="userSpaceOnUse" x1="-93.2909" y1="220.4485" x2="-93.2909" y2="219.2818" gradientTransform="matrix(-100 0 0 100 -9113.0879 -21896.832)"&gt;&lt;stop offset="0" style="stop-color:#00BFB3"&gt;&lt;/stop&gt;&lt;stop offset="1" style="stop-color:#31C8FA"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;path id="green-path-2b" class="st6" d="M203.5,114.6c-27.2-0.5-49-22.7-49-50c0-27.6,22.4-50,50-50s50,22.4,50,50c0,27.6-22.4,50-50,50v15c0,5.5,4.5,10,10,10h15h38c5.5,0,10,4.5,10,10v15" style="stroke-dasharray: 428, 468; stroke-dashoffset: 0;"&gt;&lt;/path&gt;&lt;image id="Bitmap-2" opacity="1" x="165" y="25" width="80" height="80" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/cn/assets/bltf7e873716e4478cb/GitHub-01.svg" style="display: inline; opacity: 0.129832;"&gt;&lt;/image&gt;&lt;linearGradient id="green-path-3" gradientUnits="userSpaceOnUse" x1="-91.8022" y1="220.4485" x2="-91.8022" y2="219.2818" gradientTransform="matrix(100 0 0 100 9519.2246 -21896.832)"&gt;&lt;stop offset="0" style="stop-color:#00BFB3"&gt;&lt;/stop&gt;&lt;stop offset="1" style="stop-color:#31C8FA"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;path id="green-path-3b" class="st7" d="M349.7,114.6c-27.3-0.4-49.2-22.6-49.2-50c0-27.6,22.4-50,50-50s50,22.4,50,50c0,27.6-22.4,50-50,50v15c0,5.5-4.5,10-10,10h-15h-38c-5.5,0-10,4.5-10,10v15" style="stroke-dasharray: 428, 468; stroke-dashoffset: 0;"&gt;&lt;/path&gt;&lt;image id="Bitmap-3" opacity="1" x="310" y="25" width="80" height="80" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/cn/assets/blt9be07c84e4c2bc10/http-01.svg" style="display: inline; opacity: 0.137673;"&gt;&lt;/image&gt;&lt;linearGradient id="green-path-4" gradientUnits="userSpaceOnUse" x1="-92.2322" y1="220.2818" x2="-92.2322" y2="219.1985" gradientTransform="matrix(100 0 0 100 9662.2246 -21896.832)"&gt;&lt;stop offset="0" style="stop-color:#00BFB3"&gt;&lt;/stop&gt;&lt;stop offset="1" style="stop-color:#31C8FA"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;path id="green-path-4b" class="st8" d="M494,114.6c-0.2,0-0.4,0-0.5,0c-27.6,0-50-22.4-50-50c0-27.6,22.4-50,50-50s50,22.4,50,50c0,27.3-21.8,49.5-49,50v15c0,5.5-4.5,10-10,10h-15h-87.3h-47.7" style="stroke-dasharray: 495, 535; stroke-dashoffset: 0;"&gt;&lt;/path&gt;&lt;image id="Bitmap-4" opacity="1" x="455" y="25" width="80" height="80" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/cn/assets/bltb4167870b42fce56/IRC-01.svg" style="display: inline; opacity: 0.143497;"&gt;&lt;/image&gt;&lt;/g&gt;&lt;g id="a3"&gt;&lt;path id="gray-circle-1" class="st9" d="M128.5,348.3c19.3,0,35-15.7,35-35s-15.7-35-35-35s-35,15.7-35,35S109.2,348.3,128.5,348.3z" data-ignore="none"&gt;&lt;/path&gt;&lt;image id="Bitmap-5" opacity="1" x="103" y="288" width="50" height="50" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.elastic.co/cn/assets/bltb5d092d864eb080d/Elasticsearch-02.svg"&gt;&lt;/image&gt;&lt;path id="gray-circle-2" class="st9" d="M228.5,348.3c19.3,0,35-15.7,35-35s-15.7-35-35-35s-35,15.7-35,35S209.2,348.3,228.5,348.3z" data-ignore="none"&gt;&lt;/path&gt;&lt;image id="Bitmap-6" opacity="1" x="203" y="288" width="50" height="50" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.elastic.co/cn/assets/blt08e6eee450ecd75e/Email-02.svg"&gt;&lt;/image&gt;&lt;path id="gray-circle-3" class="st9" d="M326.5,348.3c19.3,0,35-15.7,35-35s-15.7-35-35-35s-35,15.7-35,35S307.2,348.3,326.5,348.3z" data-ignore="none"&gt;&lt;/path&gt;&lt;image id="Bitmap-7" opacity="1" x="302" y="288" width="50" height="50" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/cn/assets/bltf361d26f5edc8763/S3-02.svg"&gt;&lt;/image&gt;&lt;path id="gray-circle-4" class="st9" d="M428.5,348.3c19.3,0,35-15.7,35-35s-15.7-35-35-35s-35,15.7-35,35S409.2,348.3,428.5,348.3z" data-ignore="none"&gt;&lt;/path&gt;&lt;image id="Bitmap-8" opacity="1" x="400" y="283" width="60" height="60" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.elastic.co/cn/products/logstash/cn/assets/blt3076bd904482ae55/HDFS-02.svg"&gt;&lt;/image&gt;&lt;path id="gray-circle-path-1" class="st10" d="M248.5,254.3h-105h-5c-5.5,0-10,4.5-10,10v15" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="gray-circle-path-2" class="st10" d="M277.5,229.3v15c0,5.5-4.5,10-10,10h-24h-5c-5.5,0-10,4.5-10,10v15" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="gray-circle-path-3" class="st10" d="M308.3,254.3h105.2h5c5.5,0,10,4.5,10,10v15" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="gray-circle-path-4" class="st10" d="M277.5,229.3v15c0,5.5,4.5,10,10,10h24h5c5.5,0,10,4.5,10,10v15" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="center-sq" class="st9" d="M313.5,172.3c0-5.5-4.5-10-10-10h-50c-5.5,0-10,4.5-10,10v50c0,5.5,4.5,10,10,10h50c5.5,0,10-4.5,10-10V172.3z" data-ignore="none"&gt;&lt;/path&gt;&lt;g id="Group-4" transform="translate(515.000000, 306.000000)"&gt;&lt;path id="Stroke-1_1_" class="st20" d="M-221.5-88.5v-11" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="Stroke-3_1_" class="st20" d="M-231.5-88.5v-11" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="Stroke-5" class="st20" d="M-241.5-88.5v-11" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="Stroke-7" class="st20" d="M-251.5-88.5v-11" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="Stroke-9" class="st20" d="M-251.5-107.5h30" data-ignore="none"&gt;&lt;/path&gt;&lt;path id="Stroke-11" class="st20" d="M-249.5-128.5h26c1.1,0,2,0.9,2,2v9c0,1.1-0.9,2-2,2h-26c-1.1,0-2-0.9-2-2v-9C-251.5-127.6-250.6-128.5-249.5-128.5L-249.5-128.5z" data-ignore="none"&gt;&lt;/path&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/figure&gt;]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 第四天之Spark Streaming]]></title>
    <url>%2F2019%2F04%2F25%2F1_Spark%20%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8BSpark%20Streaming%2F</url>
    <content type="text"><![CDATA[Spark 第四天之Spark Streaming1. Spark Streaming概述1.1 什么是Spark Streaming![image_1cml8qbgv17na10dm1sip1oirpu59.png-66.3kB][1]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming类似于Apache Storm，用于流式数据的处理。根据其官方文档介绍，Spark Streaming有高吞吐量和容错能力强等特点。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象语言的语法如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。 ![image_1cml9cufv12ds1pfbuac9f98bt44.png-74kB][2]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此 得名“离散化”)。![image_1cml9dhl91th01e7i1pthras1atk4h.png-29.5kB][3]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DStream 可以从各种输入源创建，比如 Flume、Kafka 或者 HDFS。创建出来的DStream 支持两种操作，一种是转化操作(transformation)，会生成一个新的DStream，另一种是输出操作(output operation)，可以把数据写入外部系统中。DStream 提供了许多与 RDD 所支持的操作相类似的操作支持，还增加了与时间相关的新操作，比如滑动窗口。 1.2 Spark Streaming的特点 易用![image_1cml974rq5e01i91bqq1tj51t0g2t.png-60.1kB][4] 容错![image_1cml97ol8s7l3ked0o1tig2p23a.png-70.6kB][5] 易整合到Spark体系![image_1cml98ft71an9i9qmmtjr1rlb3n.png-65.1kB][6] 1.3 Spark 与 Storm 对比1.3.1 对比 对比点 Storm Spark Streaming 实时计算模型 纯实时，来一条数据，处理一条数据 准实时，对一个时间段内的数据收集起来，作为一个RDD，再处理 实时计算延迟度 毫秒级 秒级 吞吐量 低 高 事务机制 支持完善 支持，但不够完善 健壮性 / 容错性 ZooKeeper，Acker，非常强 Checkpoint，WAL，一般 动态调整并行度 支持 不支持 1.3.2 Spark Streaming与Storm的应用场景Storm: 建议在那种需要纯实时，不能忍受1秒以上延迟的场景下使用，比如实时金融系统，要求纯实时进行金融交易和分析 此外，如果对于实时计算的功能中，要求可靠的事务机制和可靠性机制，即数据的处理完全精准，一条也不能多，一条也不能少，也可以考虑使用Storm 如果还需要针对高峰低峰时间段，动态调整实时计算程序的并行度，以最大限度利用集群资源（通常是在小型公司，集群资源紧张的情况），也可以考虑用Storm 如果一个大数据应用系统，它就是纯粹的实时计算，不需要在中间执行SQL交互式查询、复杂的transformation算子等，那么用Storm是比较好的选择 Spark Streaming: 如果对上述适用于Storm的三点，一条都不满足的实时场景，即，不要求纯实时，不要求强大可靠的事务机制，不要求动态调整并行度，那么可以考虑使用Spark Streaming 考虑使用Spark Streaming最主要的一个因素，应该是针对整个项目进行宏观的考虑，即，如果一个项目除了实时计算之外，还包括了离线批处理、交互式查询等业务功能，而且实时计算中，可能还会牵扯到高延迟批处理、交互式查询等功能，那么就应该首选Spark生态，用Spark Core开发离线批处理，用Spark SQL开发交互式查询，用Spark Streaming开发实时计算，三者可以无缝整合，给系统提供非常高的可扩展性 1.3.3 Spark Streaming与Storm的优劣分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实上，Spark Streaming绝对谈不上比Storm优秀。这两个框架在实时计算领域中，都很优秀，只是擅长的细分场景并不相同。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming仅仅在吞吐量上比Storm要优秀，而吞吐量这一点，也是历来挺Spark Streaming，贬Storm的人着重强调的。但是问题是，是不是在所有的实时计算场景下，都那么注重吞吐量？不尽然。因此，通过吞吐量说Spark Streaming强于Storm，不靠谱。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实上，Storm在实时延迟度上，比Spark Streaming就好多了，前者是纯实时，后者是准实时。而且，Storm的事务机制、健壮性 / 容错性、动态调整并行度等特性，都要比Spark Streaming更加优秀。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming，有一点是Storm绝对比不上的，就是：它位于Spark生态技术栈中，因此Spark Streaming可以和Spark Core、Spark SQL无缝整合，也就意味着，我们可以对实时处理出来的中间数据，立即在程序中无缝进行延迟批处理、交互式查询等操作。这个特点大大增强了Spark Streaming的优势和功能。 1.4 Spark Streaming关键抽象&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Discretized Stream或DStream是Spark Streaming提供的基本抽象。它表示连续的数据流，可以是从源接收的输入数据流，也可以是通过转换输入流生成的已处理数据流。在内部，DStream由一系列连续的RDD表示，这是Spark对不可变分布式数据集的抽象。DStream中的每个RDD都包含来自特定时间间隔的数据，如下图所示。![image_1cmla1g621cnv66o1rgeqq71gvv4u.png-25.3kB][7]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;应用于DStream的任何操作都转换为底层RDD上的操作。例如，在先前将行流转换为字的示例中，flatMap操作应用于linesDStream中的每个RDD 以生成DStream的 wordsRDD。如下图所示。![image_1cmla1soocrm17rcn7u157v7sl5b.png-47kB][8]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming接收实时输入数据流并将数据分成批处理，然后由Spark引擎处理，以批量生成最终结果流。![image_1cmlaatu7cb28641eedcs7g905o.png-29.5kB][9] 1.5 Spark Streaming 架构&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming使用“微批次”的架构，把流式计算当作一系列连续的小规模批处理来对待。Spark Streaming从各种输入源中读取数据，并把数据分组为小的批次。新的批次按均匀的时间间隔创建出来。在每个时间区间开始的时候，一个新的批次就创建出来，在该区间内收到的数据都会被添加到这个批次中。在时间区间结束时，批次停止增长。时间区间的大小是由批次间隔这个参数决定的。批次间隔一般设在500毫秒到几秒之间，由应用开发者配置。每个输入批次都形成一个RDD，以 Spark 作业的方式处理并生成其他的 RDD。 处理的结果可以以批处理的方式传给外部系统。高层次的架构如图![image_1cmlail2212531ab3hkf16gb134h8s.png-28.5kB][10]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming在Spark的驱动器程序—工作节点的结构的执行过程如下图所示。Spark Streaming为每个输入源启动对 应的接收器。接收器以任务的形式运行在应用的执行器进程中，从输入源收集数据并保存为 RDD。它们收集到输入数据后会把数据复制到另一个执行器进程来保障容错性(默 认行为)。数据保存在执行器进程的内存中，和缓存 RDD 的方式一样。驱动器程序中的 StreamingContext 会周期性地运行 Spark 作业来处理这些数据，把数据与之前时间区间中的 RDD 进行整合。![image_1cmlahq4hhvbhsi1g44amm87b8f.png-108.6kB][11] 1.6 背压机制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现batch processing time &gt; batch interval的情况，其中batch processing time 为实际计算一个批次花费时间， batch interval为Streaming应用设置的批处理间隔。这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk, 则内存存放不下的数据会溢写至disk, 加大延迟）。Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。为了更好的协调数据接收速率与资源处理能力，Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;背压机制: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在原架构的基础上加上一个新的组件RateController,这个组件负责监听“OnBatchCompleted ”事件，然后从中抽取processingDelay 及schedulingDelay信息. Estimator依据这些信息估算出最大处理速度（rate），最后由基于Receiver的Input Stream将rate通过ReceiverTracker与ReceiverSupervisorImpl转发给BlockGenerator（继承自RateLimiter）.![image_1cmlbb14j7al1qkcnl715i918r89m.png-51.3kB][12] 流量控制点当Receiver开始接收数据时，会通过supervisor.pushSingle()方法将接收的数据存入currentBuffer等待BlockGenerator定时将数据取走，包装成block. 在将数据存放入currentBuffer之时，要获取许可（令牌）。如果获取到许可就可以将数据存入buffer, 否则将被阻塞，进而阻塞Receiver从数据源拉取数据。其令牌投放采用令牌桶机制进行， 原理如下图所示:![image_1cmlbcq4919t7g31e5q12kr8rcb0.png-70.1kB][13]令牌桶机制大小固定的令牌桶可自行以恒定的速率源源不断地产生令牌。如果令牌不被消耗，或者被消耗的速度小于产生的速度，令牌就会不断地增多，直到把桶填满。后面再产生的令牌就会从桶中溢出。最后桶中可以保存的最大令牌数永远不会超过桶的大小。当进行某操作时需要令牌时会从令牌桶中取出相应的令牌数，如果获取到则继续操作，否则阻塞。用完之后不用放回。2. Spark Streaming 简单应用2.1 安装Telnet向端口发送消息12 2.2 使用SparkStreaming监控端口数据展示到控制台3. DStream 的输入&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming原生支持一些不同的数据源。一些“核心”数据源已经被打包到Spark Streaming 的 Maven 工件中，而其他的一些则可以通过 spark-streaming-kafka 等附加工件获取。每个接收器都以 Spark 执行器程序中一个长期运行的任务的形式运行，因此会占据分配给应用的 CPU 核心。此外，我们还需要有可用的 CPU 核心来处理数据。这意味着如果要运行多个接收器，就必须至少有和接收器数目相同的核心数，还要加上用来完成计算所需要的核心数。例如，如果我们想要在流计算应用中运行 10 个接收器，那么至少需要为应用分配 11 个 CPU 核心。所以如果在本地模式运行，不要使用local或者local[1]。 ![image_1cmlcf8ikeptuu01f6b2djk6r9.png-30.1kB][14] 3.1 文件数据源文件数据流：能够读取所有HDFS API兼容的文件系统文件，通过fileStream方法进行读取&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark Streaming 将会监控 dataDirectory 目录并不断处理 mv 进来的文件，记住目前不支持嵌套目录。 文件需要有相同的数据格式 文件进入 dataDirectory的方式需要通过移动或者重命名来实现。 一旦文件移动进目录，则不能再修改，即便修改了也不会读取新数据。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果文件比较简单，则可以使用 streamingContext.textFileStream(dataDirectory)方法来读取文件。文件流不需要接收器，不需要单独分配CPU核。 案例实操: 12345678## 导入相应的jar包scala&gt; import org.apache.spark.streaming._## 创建StreamingContext操作对象scala&gt; val ssc = new StreamingContext(sc,Seconds(5))scala&gt; val lines = ssc.textFileStream("hdfs://master:9000/spark/data")scala&gt; val wordCount = lines.flatMap(_.split("\t")).map(x=&gt;(x,1)).reduceByKey(_+_)scala&gt; wordCount.printscala&gt; ssc.start 3.2 RDD队列可以通过使用streamingContext.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。监控一个队列，实时获取队列中的新增的RDD 案例实操12345678910111213141516171819202122232425262728293031323334353637package com.zhiyou100.sparkimport org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import scala.collection.mutableimport scala.util.Randomobject TestSparkStreamRddQueue extends App &#123; val conf = new SparkConf().setAppName("StreamRDD").setMaster("local[3]") val ssc = new StreamingContext(conf,Seconds(2)) //创建RDD队列 val rddQueue = new mutable.SynchronizedQueue[RDD[Int]] //创建DStream val dStream = ssc.queueStream(rddQueue) //处理DStream的数据——业务逻辑 //将数据叠加起来，计算总和 val result = dStream.map(x =&gt; (1,x)).reduceByKey(_+_) //结果输出 result.print //启动 ssc.start() //生产数据 for(i&lt;-1 to 1000)&#123; val r = ssc.sparkContext.makeRDD(1 to 100) r.collect() rddQueue += r Thread.sleep(3000) &#125; ssc.awaitTermination()&#125;3.3 自定义数据源通过继承Receiver，并实现onStart、onStop方法来自定义数据源采集。 案例实操123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.zhiyou100.sparkimport java.io.&#123;BufferedReader, InputStreamReader&#125;import java.net.Socketimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.receiver.Receiverclass TestSparkStreamCustomReceiver(host:String,port:Int) extends Receiver[String](StorageLevel.MEMORY_ONLY)&#123; //启动的时候调用 override def onStart(): Unit = &#123; println("启动了") //创建一个socket val socket = new Socket(host,port) val reader = new BufferedReader(new InputStreamReader(socket.getInputStream)) //创建一个变量去读取socket的输入流的数据 var line = reader.readLine() while(!isStopped() &amp;&amp; line != null)&#123; //如果接收到了数据，就使用父类的中store方法进行保存 store(line) //继续读取下一行数据 line = reader.readLine() &#125; &#125; //停止的时候调用 override def onStop(): Unit = &#123; println("停止了") &#125;&#125;object TestSparkStreamCustomReceiver extends App&#123; //配置对象 val conf = new SparkConf().setAppName("stream").setMaster("local[3]") //创建StreamingContext val ssc = new StreamingContext(conf,Seconds(5)) //从Socket接收数据 val lineDStream = ssc.receiverStream(new TestSparkStreamCustomReceiver("master",8888)) //统计词频 val result = lineDStream.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_) result.print() //启动 ssc.start() ssc.awaitTermination()&#125;3.4 KafkaSpark 与 Kafka集成指南![image_1cmnmm4da1h1agsh1p26j5a1auo9.png-121.5kB][15]![image_1cmldu0l61v555v59v8172210e4m.png-146kB][16]下面我们进行一个实例，演示SparkStreaming如何从Kafka读取消息，如果通过连接池方法把消息处理完成后再写会Kafka![image_1cmle3dk11olvbjjub4ge1avo13.png-44.6kB][17] 整合: 引入jar包依赖 1234567&lt;!--引入JAR包--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt; 代码编写 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100//Stream2Kafka import kafka.serializer.StringDecoder import org.apache.kafka.clients.consumer.ConsumerConfig import org.apache.kafka.clients.producer.ProducerRecord import org.apache.kafka.common.serialization.StringDeserializer import org.apache.spark.SparkConf import org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125; import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125; /** * */ object Stream2Kafka extends App &#123; //创建配置对象 val conf = new SparkConf().setAppName("kafka").setMaster("local[3]") //创建SparkStreaming操作对象 val ssc = new StreamingContext(conf,Seconds(5)) //连接Kafka就需要Topic //输入的topic val fromTopic = "source" //输出的Topic val toTopic = "target" //创建brokers的地址 val brokers = "master:9092,slave1:9092,slave3:9092,slave2:9092" //Kafka消费者配置对象 val kafkaParams = Map[String, Object]( //用于初始化链接到集群的地址 ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; brokers, //Key与VALUE的序列化类型 ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG-&gt;classOf[StringDeserializer], ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG-&gt;classOf[StringDeserializer], //用于标识这个消费者属于哪个消费团体 ConsumerConfig.GROUP_ID_CONFIG-&gt;"kafka", //如果没有初始化偏移量或者当前的偏移量不存在任何服务器上，可以使用这个配置属性 //可以使用这个配置，latest自动重置偏移量为最新的偏移量 ConsumerConfig.AUTO_OFFSET_RESET_CONFIG-&gt;"latest", //如果是true，则这个消费者的偏移量会在后台自动提交 ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG-&gt;(false: java.lang.Boolean) ) //创建DStream，连接到Kafka，返回接收到的输入数据 val inputStream = &#123; KafkaUtils.createDirectStream[String, String]( ssc, //位置策略（可用的Executor上均匀分配分区） LocationStrategies.PreferConsistent, //消费策略（订阅固定的主题集合） ConsumerStrategies.Subscribe[String, String](Array(fromTopic), kafkaParams)) &#125; inputStream.map&#123;record =&gt; "hehe--"+record.value&#125;.foreachRDD &#123; rdd =&gt; //在这里将RDD写回Kafka,需要使用Kafka连接池 rdd.foreachPartition &#123; items =&gt; val kafkaProxyPool = KafkaPool(brokers) val kafkaProxy = kafkaProxyPool.borrowObject() for (item &lt;- items) &#123; //使用这个连接池 kafkaProxy.kafkaClient.send(new ProducerRecord[String, String](toTopic, item)) &#125; kafkaProxyPool.returnObject(kafkaProxy) &#125; &#125; ssc.start() ssc.awaitTermination() &#125;//Kafka连接池 import org.apache.commons.pool2.impl.&#123;DefaultPooledObject, GenericObjectPool&#125; import org.apache.commons.pool2.&#123;BasePooledObjectFactory, PooledObject&#125; import org.apache.kafka.clients.producer.&#123;KafkaProducer, ProducerConfig&#125; import org.apache.kafka.common.serialization.StringSerializer //因为要将Scala的集合类型转换成Java的 import scala.collection.JavaConversions._ class KafkaProxy(broker:String)&#123; val conf = Map( //用于初始化链接到集群的地址 ProducerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; broker, //Key与VALUE的序列化类型 ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG-&gt;classOf[StringSerializer], ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG-&gt;classOf[StringSerializer] ) val kafkaClient = new KafkaProducer[String,String](conf) &#125; //创建一个创建KafkaProxy的工厂 class KafkaProxyFactory(broker:String) extends BasePooledObjectFactory[KafkaProxy]&#123; //创建实例 override def create(): KafkaProxy = new KafkaProxy(broker) //包装实例 override def wrap(t: KafkaProxy): PooledObject[KafkaProxy] = new DefaultPooledObject[KafkaProxy](t) &#125; object KafkaPool &#123; private var kafkaPool:GenericObjectPool[KafkaProxy]=null def apply(broker:String): GenericObjectPool[KafkaProxy] =&#123; if(kafkaPool == null)&#123; this.kafkaPool = new GenericObjectPool[KafkaProxy](new KafkaProxyFactory(broker)) &#125; kafkaPool &#125; &#125; 启动zookeeper 1 启动kafka 1kafka-server-start.sh /opt/apps/Kafka/kafka_2.11_2.0.0/config/server.properties &amp; 创建两个主题 123[root@master ~]# kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181,slave3:2181,slave4:2181 --replication-factor 2 --partitions 2 --topic source[root@master ~]# kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181,slave3:2181,slave4:2181 --replication-factor 2 --partitions 2 --topic target 启动producer 写入数据到source 1[root@master ~]# kafka-console-producer.sh --broker-list master:9092,slave1:9092,slave2:9092,slave3:9092,slave4:9092 --topic source 启动consumer 监听target的数据 1[root@master ~]# kafka-console-consumer.sh --bootstrap-server master:9092,slave1:9092,slave2:9092,slave3:9092,slave4:9092 --topic target 手动设置offset : ![image_1cmnv4jl11l7qmhkd7gtvgvgu9.png-55.9kB][18] 3.5 Flume-ng4. DStream 转换DStream上的语法与RDD的类似，分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的语法，如：updateStateByKey()、transform()以及各种Window相关的语法。|Transformation| Meaning||—|—||map(func)| 将源DStream中的每个元素通过一个函数func从而得到新的DStreams。||flatMap(func)| 和map类似，但是每个输入的项可以被映射为0或更多项。||filter(func)| 选择源DStream中函数func判为true的记录作为新DStreams||repartition(numPartitions) |通过创建更多或者更少的partition来改变此DStream的并行级别。||union(otherStream)| 联合源DStreams和其他DStreams来得到新DStream ||count()| 统计源DStreams中每个RDD所含元素的个数得到单元素RDD的新DStreams。||reduce(func)| 通过函数func(两个参数一个输出)来整合源DStreams中每个RDD元素得到单元素RDD的DStreams。这个函数需要关联从而可以被并行计算。||countByValue() |对于DStreams中元素类型为K调用此函数，得到包含(K,Long)对的新DStream，其中Long值表明相应的K在源DStream中每个RDD出现的频率。||reduceByKey(func, [numTasks]) | 对(K,V)对的DStream调用此函数，返回同样（K,V)对的新DStream，但是新DStream中的对应V为使用reduce函数整合而来。Note：默认情况下，这个操作使用Spark默认数量的并行任务（本地模式为2，集群模式中的数量取决于配置参数spark.default.parallelism）。你也可以传入可选的参数numTaska来设置不同数量的任务。 ||join(otherStream, [numTasks])| 两DStream分别为(K,V)和(K,W)对，返回(K,(V,W))对的新DStream。 ||cogroup(otherStream, [numTasks])| 两DStream分别为(K,V)和(K,W)对，返回(K,(Seq[V],Seq[W])对新DStreams ||transform(func) | 将RDD到RDD映射的函数func作用于源DStream中每个RDD上得到新DStream。这个可用于在DStream的RDD上做任意操作。 ||updateStateByKey(func)| 得到”状态”DStream，其中每个key状态的更新是通过将给定函数用于此key的上一个状态和新值而得到。这个可用于保存每个key值的任意状态数据。 | DStream 的转化操作可以分为无状态(stateless)和有状态(stateful)两种。 在无状态转化操作中，每个批次的处理不依赖于之前批次的数据。常见的 RDD 转化操作，例如 map()、filter()、reduceByKey() 等，都是无状态转化操作。 相对地，有状态转化操作需要使用之前批次的数据或者是中间结果来计算当前批次的数据。有状态转化操作包括基于滑动窗口的转化操作和追踪状态变化的转化操作。 4.1 无状态转化操作无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每一个 RDD。部分无状态转化操作列在了下表中。 注意，针对键值对的 DStream 转化操作(比如 reduceByKey())要添加import StreamingContext._ 才能在 Scala中使用。![image_1cmo3v7r81ggk1til14iodcl1r28m.png-118.5kB][19]需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部是由许多 RDD(批次)组成，且无状态转化操作是分别应用到每个 RDD 上的。例如， reduceByKey() 会归约每个时间区间中的数据，但不会归约不同区间之间的数据。举个例子，在之前的wordcount程序中，我们只会统计1秒内接收到的数据的单词个数，而不会累加。无状态转化操作也能在多个 DStream 间整合数据，不过也是在各个时间区间内。例如，键 值对 DStream 拥有和 RDD 一样的与连接相关的转化操作，也就是 cogroup()、join()、 leftOuterJoin() 等。我们可以在 DStream 上使用这些操作，这样就对每个批次分别执行了对应的 RDD 操作。我们还可以像在常规的 Spark 中一样使用 DStream 的 union() 操作将它和另一个 DStream 的内容合并起来，也可以使用 StreamingContext.union() 来合并多个流。 4.2 有状态转化操作4.2.1 追踪状态变化UpdateStateByKeyUpdateStateByKey原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加wordcount)。针对这种情况，updateStateByKey() 为我们提供了对一个状态变量的访问，用于键值对形式的 DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件 更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。updateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，你需要做下面两步： 定义状态，状态可以是一个任意的数据类型。 定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。使用updateStateByKey需要对检查点目录进行配置，会使用检查点来保存状态。12345678910111213141516171819202122232425262728293031package com.zhiyou100.sparkimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object TestStreamUpdate extends App &#123; //配置对象 val conf = new SparkConf().setAppName("stream").setMaster("local[3]") //创建StreamingContext val ssc = new StreamingContext(conf,Seconds(5)) //重要：检查点目录的配置 ssc.sparkContext.setCheckpointDir("F:\\cd\\data") //从Socket接收数据 val lineDStream = ssc.socketTextStream("master",8888) val words = lineDStream.flatMap(_.split(" ")).map(x=&gt;(x,1)) //使用updateStateByKey进行状态的更新，统计从运行开始以来的单词总数 val state = words.updateStateByKey[Int]((values:Seq[Int],state:Option[Int])=&gt;&#123; //values应该是新的数据 val currentCount = values.foldLeft(0)(_+_) val stateCount = state.getOrElse(0) Some(currentCount+stateCount) &#125;) state.print() //启动 ssc.start() ssc.awaitTermination()&#125; 4.2.2 WindowWindow Operations有点类似于Storm中的State，可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态。基于窗口的操作会在一个比 StreamingContext 的批次间隔更长的时间范围内，通过整合多个批次的结果，计算出整个窗口的结果。![image_1cmon3m5813v69t2upj1r6g2di13.png-33.1kB][20]所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长，两者都必须是 StreamContext 的批次间隔的整数倍。窗口时长控制每次计算最近的多少个批次的数据，其实就是最近的 windowDuration/batchInterval 个批次。如果有一个以 10 秒为批次间隔的源 DStream，要创建一个最近 30 秒的时间窗口(即最近 3 个批次)，就应当把 windowDuration 设为 30 秒。而滑动步长的默认值与批次间隔相等，用来控制对新的 DStream 进行计算的间隔。如果源 DStream 批次间隔为 10 秒，并且我们只希望每两个批次计算一次窗口结果， 就应该把滑动步长设置为 20 秒。假设，你想拓展前例从而每隔十秒对持续30秒的数据生成word count。为做到这个，我们需要在持续30秒数据的(word,1)对DStream上应用reduceByKey。使用操作reduceByKeyAndWindow. 123456789101112131415161718192021222324252627282930313233343536package com.zhiyou100.sparkimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object TestStreamWindow extends App&#123; //配置对象 val conf = new SparkConf().setAppName("stream").setMaster("local[3]") //创建StreamingContext val ssc = new StreamingContext(conf,Seconds(5)) //重要：检查点目录的配置 ssc.sparkContext.setCheckpointDir("F:\\cd\\data") //从Socket接收数据 val lineDStream = ssc.socketTextStream("master",8888) val words = lineDStream.flatMap(_.split(" ")).map(x=&gt;(x,1)) println("这是每隔5秒统计结果") words.reduceByKey(_+_).print() val window = words.reduceByKeyAndWindow( (a:Int,b:Int)=&gt;(a+b),//reduce函数：将值进行叠加 Seconds(20),//窗口的时间间隔，大小为3 Seconds(10)//窗口滑动的时间间隔，步长为2 ) window.print() //启动 ssc.start() ssc.awaitTermination()&#125; 5. DStream 输出]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark第二天之RDD]]></title>
    <url>%2F2019%2F04%2F25%2F1_Spark%20%E7%AC%AC%E4%BA%8C%E5%A4%A9%E4%B9%8BRDD%2F</url>
    <content type="text"><![CDATA[Spark 第二天之RDD1. RDD概念1.1 为什么要学习RDD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。MR中的迭代： ![image_1cmftqjpr1iv51ej5cvc2lg1hf89.png-27.9kB][1] Spark中的迭代：![image_1cmftrru117341tc9q2h10m9h8g16.png-21.7kB][2] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。 1.2 什么是RDD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。 1.3 RDD的属性![image_1cmfus9721d7iom1pllav1b8k3g.png-38.1kB][3] 一组分片（Partition）即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 一个Partitioner即RDD的分区函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。![image_1cmfuvvr5c8k1il2qor1pe6gnp3t.png-360.8kB][4] 1.4 RDD的弹性 自动进行内存和磁盘数据存储的切换 Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换 基于血统关系高效容错机制 在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。 任务失败会自动重试 RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。 数据分区的高度弹性 可以根据业务的特征，动态调整数据分区的个数，提升整体的应用执行效率。 RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)，它是一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等)，通过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，该DAG描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)一气呵成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记录被传入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建迭代型应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系统，简单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。 1.5 RDD的特点分区RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。只读但是RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了。如下图所示：![image_1cmfvpaiv10rcqhlh7e1jrr8hu4q.png-34.7kB][5]RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。![image_1cmfvq4lqq5p1mqo1coc1efdac657.png-79.7kB][6] 依赖RDD之间的转换操作需要一个关系信息，这些信息或者说这种血缘关系就成为一个依赖。依赖分为窄依赖和宽依赖。窄依赖：RDD的分区是一一对应的。宽依赖：下游的RDD的每个分区都与上游RDD的每个分区都有关系，说白了就是多对多的关系。缓存如果程序中需要多次使用一个RDD，可以将缓存的内容存储起来。checkpointcheckpoint可以将数据持久化到存储中。可以切断血缘关系，后续的可以直接从checkpoint中拿到RDD的信息包含以下内容 分区数，分区方式 上游RDD衍生而来的依赖关系 计算每个分区的数据- 如果这个数据被缓存了，那么就直接从内存读取数据 - 如果这个数据被checkpoint了，那么就从checkpoint恢复数据 - 根据依赖计算数据 2. RDD的方法在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。 2.1 RDD的创建在Spark中创建RDD的创建方式大概可以分为三种： 从集合中创建RDD 1234567## 创建RDD，其实底层还是parallelizescala&gt; sc.makeRDD(1 to 10)scala&gt; sc.parallelize(1 to 10)res2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:25scala&gt; res2.collectres3: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) 从外部存储创建RDD 从其他RDD创建。 2.2 RDD的类型 数值型RDD（单一值）RDD[Int]、 RDD[(Int,Int)] 、 RDD[(Int,(Int,Int))] RDD.scala 键值对RDDRDD[Int,Int] RDD[(Int,(Int,Int))] PairRDDFunctions.scala [所有键值对RDD都可以使用数据型RDD的操作] 2.3 RDD的转换RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。 常用的Transformation: map(func)返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成 12345678scala&gt; sc.makeRDD(1 to 10)res4: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at makeRDD at &lt;console&gt;:25scala&gt; res4.map(_ * 3)res5: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at map at &lt;console&gt;:27scala&gt; res5.collectres6: Array[Int] = Array(3, 6, 9, 12, 15, 18, 21, 24, 27, 30) filter(func)返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成，，过滤数据 12345scala&gt; sc.makeRDD(1 to 20)res7: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at makeRDD at &lt;console&gt;:25scala&gt; res7.filter(_ % 2 == 0).collectres8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20) flatMap(func)类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素），一对多 12345678scala&gt; sc.makeRDD(1 to 10)res9: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at makeRDD at &lt;console&gt;:25scala&gt; res9.flatMap(1 to _)res10: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at flatMap at &lt;console&gt;:27scala&gt; res10.collectres11: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10) mapPartitions(func)类似于map，但独立地在RDD的每一个分区上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。对于一个分区中的所有数据执行一个函数，性能比map要高 1234567891011var rdd = sc.makeRDD(List((1,1001),(2,1002),(3,1003),(4,1004),(5,1005)))def pf(is:Iterator[(Int,Int)]):Iterator[String]=&#123; var list = List[String]() while(is.hasNext)&#123; val tuble = is.next() list = (tuble._1 + "hehe"+tuble._2)::list &#125; list.iterator&#125;val res = rdd.mapPartitions(pf)res.collect mapPartitionsWithIndex(func)类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U] sample(withReplacement, fraction, seed)以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。主要用于抽样12345678scala&gt; val rdd = sc.makeRDD(1 to 100)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at makeRDD at &lt;console&gt;:24scala&gt; rdd.sample(true,0.1,10)res22: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[17] at sample at &lt;console&gt;:27scala&gt; res22.collectres23: Array[Int] = Array(2, 4, 20, 20, 53, 60, 67, 82, 92, 93) takeSample和Sample的区别是：takeSample返回的是最终的结果集合。 12scala&gt; rdd.takeSample(true,7,4)res55: Array[Int] = Array(36, 45, 40, 34, 100, 57, 67) union(otherDataset)对源RDD和参数RDD求并集后返回一个新的RDD 12scala&gt; sc.makeRDD(1 to 10).union(sc.makeRDD(20 to 25)).collectres57: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 21, 22, 23, 24, 25) intersection(otherDataset)对源RDD和参数RDD求交集后返回一个新的RDD。求交集 12345scala&gt; sc.makeRDD(1 to 10).intersection(sc.makeRDD(20 to 25)).collectres58: Array[Int] = Array() scala&gt; sc.makeRDD(1 to 10).intersection(sc.makeRDD(5 to 20)).collectres59: Array[Int] = Array(6, 9, 7, 10, 8, 5) distinct([numTasks]))对源RDD进行去重后返回一个新的RDD. 默认情况下，只有6个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。 12345scala&gt; sc.makeRDD(1 to 10).union(sc.makeRDD(5 to 15)).collectres60: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)scala&gt; sc.makeRDD(1 to 10).union(sc.makeRDD(5 to 15)).distinct.collectres61: Array[Int] = Array(6, 12, 13, 1, 7, 14, 8, 2, 15, 3, 9, 4, 10, 11, 5) partitionBy对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD123456789scala&gt; import org.apache.spark.HashPartitionerimport org.apache.spark.HashPartitionerscala&gt; rdd.map(v =&gt; (v,v)).partitionBy(new HashPartitioner(6))res64: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[85] at partitionBy at &lt;console&gt;:28scala&gt; scala&gt; res64.partitions.size reduceByKey(func, [numTasks])在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置 groupByKeygroupByKey也是对每个key进行操作，但只生成一个sequence。将key相同的value聚集在一起。 12345678910111213141516171819202122232425262728293031scala&gt; val line = "acdfakdfakdfadsajdvakldjfahdsfajdsfhasfa"line: String = acdfakdfakdfadsajdvakldjfahdsfajdsfhasfascala&gt; val rdd = sc.makeRDD(line)rdd: org.apache.spark.rdd.RDD[Char] = ParallelCollectionRDD[86] at makeRDD at &lt;console&gt;:27scala&gt; rdd.collectres66: Array[Char] = Array(a, c, d, f, a, k, d, f, a, k, d, f, a, d, s, a, j, d, v, a, k, l, d, j, f, a, h, d, s, f, a, j, d, s, f, h, a, s, f, a)scala&gt; rdd.map(w=&gt;(w,1)).groupByKey().collectres67: Array[(Char, Iterable[Int])] = Array((f,CompactBuffer(1, 1, 1, 1, 1, 1, 1)), (l,CompactBuffer(1)), (c,CompactBuffer(1)), (d,CompactBuffer(1, 1, 1, 1, 1, 1, 1, 1)), (s,CompactBuffer(1, 1, 1, 1)), (a,CompactBuffer(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)), (j,CompactBuffer(1, 1, 1)), (v,CompactBuffer(1)), (k,CompactBuffer(1, 1, 1)), (h,CompactBuffer(1, 1)))scala&gt; rdd.map(w=&gt;(w,1)).reduceByKey(_+_).collectres68: Array[(Char, Int)] = Array((f,7), (l,1), (c,1), (d,8), (s,4), (a,10), (j,3), (v,1), (k,3), (h,2))scala&gt; rdd.map(w=&gt;(w,1)).reduceByKey(_+"-&gt;"+_).collect&lt;console&gt;:30: error: type mismatch; found : String required: Int rdd.map(w=&gt;(w,1)).reduceByKey(_+"-&gt;"+_).collect ^scala&gt; rdd.map(w=&gt;(w,1)).reduceByKey((v1,v2)=&gt;v1+"-&gt;"+v2).collect&lt;console&gt;:30: error: type mismatch; found : String required: Int rdd.map(w=&gt;(w,1)).reduceByKey((v1,v2)=&gt;v1+"-&gt;"+v2).collect ^scala&gt; rdd.map(w=&gt;(w,1)).reduceByKey((v1,v2)=&gt;v1+v2+100).collectres71: Array[(Char, Int)] = Array((f,607), (l,1), (c,1), (d,708), (s,304), (a,910), (j,203), (v,1), (k,203), (h,102)) combineByKey[C]对相同K，把V合并成一个集合. 123456789101112createCombiner: V =&gt; C, 遍历分区中所有的元素，使用这个函数可以来创建一个累加器的初始值v=&gt;(v,1)mergeValue: (C, V) =&gt; C, 可以将当前这个分区中所遇到的值和当前的值累加(c:(Int,Int),v)=&gt;(c._1+v,c._2+1)mergeCombiners: (C, C) =&gt; C,将多个分区的值累加后在累加(c1:(Int,Int),c2:(Int,Int))=&gt;(c1._1+c2._1,c1._2+c2._2)求学生成绩的平均值scala&gt; val rdd1 = sc.makeRDD(List(("a",90),("a",60),("a",70),("b",96),("b",80),("b",70),("c",60),("c",80),("c",80)))rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[93] at makeRDD at &lt;console&gt;:25scala&gt; rdd1.combineByKey(v=&gt;(v,1),(c:(Int,Int),v)=&gt;(c._1+v,c._2+1),(c1:(Int,Int),c2:(Int,Int))=&gt;(c1._1+c2._1,c1._2+c2._2))res72: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[94] at combineByKey at &lt;console&gt;:28scala&gt; res72.collectres73: Array[(String, (Int, Int))] = Array((c,(220,3)), (a,(220,3)), (b,(246,3))) aggregateByKey(zeroValue)(sqpOp，combOp)在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。 123456## 举例：对相同的key，求出value的最大值。假设所有value都是大于0的scala&gt; val rdd2 = sc.makeRDD(List((1,2),(1,4),(1,3),(2,5),(2,4),(3,5),(3,3),(3,6)))rdd2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[95] at makeRDD at &lt;console&gt;:25scala&gt; rdd2.aggregateByKey(0)(math.max(_,_),_+_).collectres77: Array[(Int, Int)] = Array((3,6), (1,7), (2,5)) foldByKey(zeroValue)((V,V)=&gt;V)aggregateByKey的简化操作，seqop和combop相同 sortByKey([ascending], [numTasks])在一个(K,V)的RDD上调用，返回一个按照key进行排序的(K,V)的RDD。如果Key目前不支持排序，需要with Ordering接口，实现compare方法，告诉spark key的大小判定。 sortBy(func,[ascending], [numTasks])与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。 12345678scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[98] at makeRDD at &lt;console&gt;:25scala&gt; rdd.sortBy(_ % 3).collectres78: Array[Int] = Array(6, 9, 3, 4, 7, 10, 1, 2, 5, 8)scala&gt; rdd.sortBy(_ % 3,false).collectres79: Array[Int] = Array(2, 8, 5, 4, 1, 7, 10, 6, 3, 9) join(otherDataset, [numTasks])在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD 12345678scala&gt; val rdd = sc.makeRDD(List((1,"aa"),(2,"bb"),(3,"cc")))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[111] at makeRDD at &lt;console&gt;:25scala&gt; val rdd1 = sc.makeRDD(List((1,"11"),(2,"22"),(3,"33"),(4,"44")))rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[112] at makeRDD at &lt;console&gt;:25scala&gt; rdd.join(rdd1).collectres81: Array[(Int, (String, String))] = Array((3,(cc,33)), (1,(aa,11)), (2,(bb,22))) cogroup(otherDataset, [numTasks])在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD cartesian(otherDataset)笛卡尔积 subtract计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来 。差集 12345678910scala&gt; val rdd2 = sc.makeRDD(5 to 15)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.substract(rdd2).collect&lt;console&gt;:29: error: value substract is not a member of org.apache.spark.rdd.RDD[Int] rdd1.substract(rdd2).collect ^scala&gt; rdd1.subtract(rdd2).collectres1: Array[Int] = Array(3, 1, 4, 2) pipe(command, [envVars])对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD 1234567891011scala&gt; val rdd = sc.makeRDD(List("wangguo","yangxiu","xiaozhou","kangkang"),3)rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[9] at makeRDD at &lt;console&gt;:24scala&gt; rdd.pipe("/opt/test/spark/pipe.sh").collectres4: Array[String] = Array(wangcen, wangguohehe, wangcen, yangxiuhehe, wangcen, xiaozhouhehe, kangkanghehe)scala&gt; val rdd = sc.makeRDD(List("wangguo","yangxiu","xiaozhou","kangkang"),4)rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:24scala&gt; rdd.pipe("/opt/test/spark/pipe.sh").collectres5: Array[String] = Array(wangcen, wangguohehe, wangcen, yangxiuhehe, wangcen, xiaozhouhehe, wangcen, kangkanghehe) coalesce(numPartitions)缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。 repartition(numPartitions)根据分区数，从新通过网络随机洗牌所有数据。 repartitionAndSortWithinPartitions(partitioner)repartitionAndSortWithinPartitions函数是repartition函数的变种，与repartition函数不同的是，repartitionAndSortWithinPartitions在给定的partitioner内部进行排序，性能比repartition要高。 glom将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]] 1234567891011scala&gt; val rdd = sc.makeRDD(1 to 20,5)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at makeRDD at &lt;console&gt;:24scala&gt; rdd.glom.collectres6: Array[Array[Int]] = Array(Array(1, 2, 3, 4), Array(5, 6, 7, 8), Array(9, 10, 11, 12), Array(13, 14, 15, 16), Array(17, 18, 19, 20))scala&gt; val rdd = sc.makeRDD("gsdjghsdlkgjldfhsdfgasjdsg",5)rdd: org.apache.spark.rdd.RDD[Char] = ParallelCollectionRDD[15] at makeRDD at &lt;console&gt;:24scala&gt; rdd.glom.collectres7: Array[Array[Char]] = Array(Array(g, s, d, j, g), Array(h, s, d, l, k), Array(g, j, l, d, f), Array(h, s, d, f, g), Array(a, s, j, d, s, g)) mapValues针对于(K,V)形式的类型只对V进行操作 12345678 scala&gt; val rdd = sc.makeRDD(List((1,10001),(2,10002),(1,20001),(3,10003),(2,20002),(4,100004),(5,10005),(3,20003)))rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[17] at makeRDD at &lt;console&gt;:24 scala&gt; rdd.mapValues(a =&gt; a+1).collect res10: Array[(Int, Int)] = Array((1,10002), (2,10003), (1,20002), (3,10004), (2,20003), (4,100005), (5,10006), (3,20004)) scala&gt; rdd.mapValues(_+1).collect res11: Array[(Int, Int)] = Array((1,10002), (2,10003), (1,20002), (3,10004), (2,20003), (4,100005), (5,10006), (3,20004)) 常用的Action算子如果最后不是返回RDD，那么就是行动操作 reduce(f: (T, T) =&gt; T): T通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的. 12345678scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at makeRDD at &lt;console&gt;:24scala&gt; rdd.reduce(_+_)res12: Int = 55scala&gt; rdd.reduce((v1,v2)=&gt;v1+v2)res13: Int = 55 collect()在驱动程序中将数据集的所有元素作为数组返回 count()返回数据集中的元素数 first()返回数据集的第一个元素（类似于take（1））。 take(n)返回一个由数据集的前n个元素组成的数组 takeSample(withReplacement,num, [seed])返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子 takeOrdered(n)返回前几个的排序。先排序后拿走 saveAsTextFile(path)将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 saveAsSequenceFile(path)将数据集中的元素以Hadoop sequencefile 的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 saveAsObjectFile(path)用于将RDD中的元素序列化成对象，存储到文件中。 countByKey()针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 foreach(func)在数据集的每一个元素上，运行函数func进行更新。 1234567891011scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[25] at makeRDD at &lt;console&gt;:24scala&gt; var sum = sc.accumulator(0)warning: there were two deprecation warnings; re-run with -deprecation for detailssum: org.apache.spark.Accumulator[Int] = 0scala&gt; rdd.foreach(sum+=_)scala&gt; sum.valueres27: Int = 55 数值型RDD的统计操作Spark 对包含数值数据的 RDD 提供了一些描述性的统计操作。 Spark 的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些 统计数据都会在调用 stats() 时通过一次遍历数据计算出来，并以 StatsCounter 对象返回。 |函数名|描述| |—|—| |count()| RDD中的元素个数| |mean()| 元素的平均值| |sum()| 总和| |max()| 最大值| |min()| 最小值| |variance() |元素的方差| |sampleVariance()| 从采样中计算出方差| |stdev() |标准差| |sampleStdev()| 采样的标准差|2.4 RDD的依赖关系RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。![image_1cmihabfmvg4lnb1fd41cme18ou9.png-62.8kB][7] 窄依赖窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。因此，窄依赖我们形象的比喻为独生子女宽依赖宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle。因此，宽依赖我们形象的比喻为超生2.5 有向无环图DAG概念![image_1cmihm7oc64odrp1sm1qkuhldm.png-215.6kB][8]DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 2.6 RDD 的规划关系: 一个jar包就是一个Application 一个行动操作就是一个Job， 对应于Hadoop中的一个MapReduce任务 一个Job有很多Stage组成，划分Stage是从后往前划分，遇到宽依赖则将前面的所有转换换分为一个Stage 一个Stage有很多Task组成，一个分区被一个Task所处理，所有分区数也叫并行度。![image_1cmii9khqv9i1hphpfc1k52l0j1g.png-52.7kB][9]![image_1cmii65i5aag1oki16471cim15an13.png-81.4kB][10] 2.7 血统RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。 2.8 RDD的持久化缓存persist cache cache就是MEMORY_ONLY的persist persist可以提供存储级别 缓存后可能会由于内存不足通过类似LRU算法删除内存的备份，所以，缓存操作不会清除血统关系。 123456789101112131415def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)def cache(): this.type = persist()object StorageLevel &#123; val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(true, true, true, false, 1) 值 描述 MEMORY_ONLY 默认选项，RDD的（分区）数据直接以Java对象的形式存储于JVM的内存中，如果内存空间不足，某些分区的数据将不会被缓存，需要在使用的时候根据血统关系重新计算。 MYMORY_AND_DISK RDD的数据直接以Java对象的形式存储于JVM的内存中，如果内存空间不中，某些分区的数据会被存储至磁盘，使用的时候从磁盘读取。 MEMORY_ONLY_SER RDD的数据（Java对象）序列化之后存储于JVM的内存中（一个分区的数据为内存中的一个字节数组），相比于MEMORY_ONLY能够有效节约内存空间（特别是使用一个快速序列化工具的情况下），但读取数据时需要更多的CPU开销；如果内存空间不足，处理方式与MEMORY_ONLY相同。 MEMORY_AND_DISK_SER 相比于MEMORY_ONLY_SER，在内存空间不足的情况下，将序列化之后的数据存储于磁盘。 DISK_ONLY 仅仅使用磁盘存储RDD的数据（未经序列化）。 MEMORY_ONLY_2,MEMORY_AND_DISK_2, etc. 以MEMORY_ONLY_2为例，MEMORY_ONLY_2相比于MEMORY_ONLY存储数据的方式是相同的，不同的是会将数据备份到集群中两个不同的节点，其余情况类似。 OFF_HEAP(experimental) RDD的数据序例化之后存储至Tachyon。相比于MEMORY_ONLY_SER，OFF_HEAP能够减少垃圾回收开销、使得Spark Executor更“小”更“轻”的同时可以共享内存；而且数据存储于Tachyon中，Spark集群节点故障并不会造成数据丢失，因此这种方式在“大”内存或多并发应用的场景下是很有吸引力的。需要注意的是，Tachyon并不直接包含于Spark的体系之内，需要选择合适的版本进行部署；它的数据是以“块”为单位进行管理的，这些块可以根据一定的算法被丢弃，且不会被重建。 检查点: 检查点机制是通过将RDD保存到一个非易失存储来完成RDD的保存，一般选择HDFS 通过sc.setCheckPointDir来指定检查点的保存路径，一个应用指定一次。 通过调用RDD的checkpoint方法来主动将RDD数据保存，读取时自动的从检查点目录读取 通过检查点机制，RDD的血统关系被切断。 区别![image_1cmijhqbhvah35c1732coqil1t.png-36.8kB][11]cache 和 checkpoint 是有显著区别的， 缓存把 RDD 计算出来然后放在内存中，但是RDD 的依赖链（相当于数据库中的redo 日志）， 也不能丢掉， 当某个点某个 executor 宕了，上面cache 的RDD就会丢掉， 需要通过 依赖链重放计算出来， 不同的是， checkpoint 是把 RDD 保存在 HDFS中， 是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链， 是通过复制实现的高容错。实操: 2.8 RDD的分区Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数 注意 只有Key-Value类型的RDD才有分区的，非Key-Value类型的RDD分区的值是None 每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。 Hash分区12345678def getPartition(key: Any): Int = key match &#123; case null =&gt; 0 case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions) &#125;def nonNegativeMod(x: Int, mod: Int): Int = &#123; val rawMod = x % mod rawMod + (if (rawMod &lt; 0) mod else 0) &#125;HashPartitioner分区的原理：对于给定的key，计算其hashCode，并除于分区的个数取余，如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID。 Range分区&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RangePartitioner分区优势：尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大；但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RangePartitioner作用：将一定范围内的数映射到某一个分区内，在实现中，分界的算法尤为重要。用到了水塘抽样算法。自定义分区&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;要实现自定义的分区器，你需要继承 org.apache.spark.Partitioner 类并实现下面三个方法。numPartitions: Int:返回创建出来的分区数。getPartition(key: Any): Int:返回给定键的分区编号(0到numPartitions-1)。equals():Java 判断相等性的标准方法。这个方法的实现非常重要，Spark 需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样 Spark 才可以判断两个 RDD 的分区方式是否相同。案例：假设我们需要将相同后缀的数据写入相同的文件，我们通过将相同后缀的数据分区到相同的分区并保存输出来实现。RDD 高级3.1 RDD的累加器&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;累加器用来对信息进行聚合，通常在向 Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱 动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本， 更新这些副本的值也不会影响驱动器中的对应变量。 如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。Spark提供了一个默认的累加器，只能用于求和。没什么鸟用。 自定义累加器自定义累加器类型的功能在1.X版本中就已经提供了，但是使用起来比较麻烦，在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。AccumulatorV2抽象类中In就是输入的数据类型， Out是累加器输出的数据类型。 最好不要在转换操作中访问累加器，最好在行动操作后访问。转换或者行动操作中不能访问累加器的值，只能添加add， 3.2 广播变量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;广播变量用来高效分发较大的对象。向所有工作节点发送一个 较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发 送一个较大的只读查询表，甚至是机器学习算法中的一个很大的特征向量，广播变量用起来都很顺手。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;传统方式下，Spark 会自动把闭包中所有引用到的变量发送到工作节点上。虽然这很方便，但也很低效。原因有二:首先，默认的任务发射机制是专门为小任务进行优化的；其次，事实上你可能会在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。 如果使用本地变量中，不采用广播变量的形式，那么每一个分区中会有一个Copy如果使用了广播变量，那么每一个Executor中会有该变量的一次Copy，【一个Executor【JVM进程】中有很多分区】主要用在百兆数据的分发 案例: 4. Spark 的输入输出4.1 文本文件输入输出当我们将一个文本文件读取为 RDD 时，输入的每一行 都会成为RDD的一个元素。也可以将多个完整的文本文件一次性读取为一个pair RDD， 其中键是文件名，值是文件内容。val input = sc.textFile(“./aaa.txt”)如果传递目录，则将目录下的所有文件读取作为RDD。文件路径支持通配符 * 。通过wholeTextFiles()对于大量的小文件读取效率比较高，大文件效果没有那么高。Spark通过saveAsTextFile() 进行文本文件的输出，该方法接收一个路径，并将 RDD 中的内容都输入到路径对应的文件中。Spark 将传入的路径作为目录对待，会在那个 目录下输出多个文件。这样，Spark 就可以从多个节点上并行输出了。result.saveAsTextFile(outputFile) 4.2 JSON文件输入输出其实就是文本的输入输出，需要手动导入相关jar包进行编码和解码 4.3 CSV文件输入输出文本文件的输入和输出 4.4 SequenceFile 的输入和输出val sdata = sc.sequenceFileInt,String 对于读取还说，需要将kv的类型指定一下。 直接调用rdd.saveAsSquenceFile(path) 来进行保存 4.5 ObjectFile的输入和输出val objrdd:RDD[(Int,String)] = sc.objectFile(Int,String) 对于读取来说，需要设定kv的类型 直接调用rdd.saveAsObjectFile(path) 来进行保存。 4.6 Hadoop的输出和输出格式Spark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持.另外,由于Hadoop的API有新旧两个版本,所以Spark为了能够兼容Hadoop所有的版本,也提供了两套创建操作接口.对于外部存储创建操作而言,hadoopRDD和newHadoopRDD是最为抽象的两个函数接口. 输入: 针对旧的Hadoop API来说提供了hadoopFile以及 hadoopRDD来进行hadoop的输入 针对新的Hadoop API来说提供了newApiHadoopFile 以及 newApiHadoopRDD 来进行hadoop的输入 输出: 针对旧的Hadoop API来说提供了saveAsHadoopFile以及 saveAsHadoopDataSet来进行Hadoop的输出 针对新的Hadoop API来说提供了saveAsNewApiHadoopFile以及saveAsNewApiHadoopDataSet来进行Hadoop的输出 4.7 文件系统的输入输出Spark 支持读写很多种文件系统， 像本地文件系统、Amazon S3、HDFS等 4.8 数据库的输入输出12345678class JdbcRDD[T: ClassTag]( sc: SparkContext,//sc getConnection: () =&gt; Connection,//获取SQL连接对象con sql: String,//sql查询语句 lowerBound: Long,//主键的下确界 upperBound: Long,// numPartitions: Int,//分区数 mapRow: (ResultSet) =&gt; T = JdbcRDD.resultSetToObjectArray _)//将结果集转换成其他对象]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark第三天之SparkSQL]]></title>
    <url>%2F2019%2F04%2F25%2F1_Spark%20%E7%AC%AC%E4%B8%89%E5%A4%A9%E4%B9%8BSpark%20SQL%2F</url>
    <content type="text"><![CDATA[Spark 第三天之Spark SQL1. Spark SQL概述1.1 什么是Spark SQL![image_1cmio2ig3j22l44vdi31a1m509.png-58.8kB][1]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们已经学习了Hive，它是将Hive SQL 转换成 MapReduce 然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果说Hive是为了简化MapReduce操作，那么Spark SQL可以说是为了简化RDD的操作。让一个不懂RDD操作的人也可以轻松使用Spark。 易整合![image_1cmioarhitcofuckg21mj819dc1m.png-50.6kB][2] 统一的数据访问方式![image_1cmiobr9f16pf1s3s1bj6n3b9vl23.png-62.6kB][3] 兼容Hive![image_1cmiocej31n931sm4qrsmgi1fad2g.png-74.4kB][4] 标准的数据连接![image_1cmioct9e1c7519la14141tah1v722t.png-54.9kB][5]![image_1cmiokv241engdid15k31kt72so9.png-30.2kB][6]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。 1.2 Spark SQl的数据抽象![image_1cmioruf962jkms3t7uke10ahm.png-99.4kB][7]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。 Dataframe&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。![image_1cmip0l5k1ph1d5de5u11qo13eq13.png-67.3kB][8]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待。简单来说DataFrame=RDD+Schema DataFrame也是懒执行的。性能上比RDD要高，主要有两方面原因：定制化内存管理:数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制。优化的执行计划:查询计划通过Spark catalyst optimiser进行优化.Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错. Dataset: 是Dataframe API的一个扩展，是Spark最新的数据抽象 用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。 Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。 样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。 Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息都用Row来表示。 DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person]. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么，控制的粒度不一样。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;![image_1cmipmbtrurtraot1m5tnmkr1g.png-150.3kB][9] 三者的共性: RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利 三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过. 三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出 三者都有partition的概念 三者有许多共同的函数，如filter，排序等 在对DataFrame和Dataset进行操作许多操作都需要spark.implicits._进行支持 DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型 三者的区别: RDD RDD一般和spark mllib同时使用 RDD不支持sparksql操作 DataFrame 与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值 DataFrame与Dataset一般不与spark mllib同时使用 DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作 DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然 Dataset Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同 DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息 2. 执行SparkSQL2.1 命令式操作2.2 代码操作1234567891011121314151617package com.zhiyou100.sparkimport org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSessionobject TestSparkSql extends App &#123; val conf = new SparkConf().setAppName("testSparkSql").setMaster("local[2]") val spark = SparkSession.builder().config(conf).getOrCreate() val sc = spark.sparkContext val employee = spark.read.json("C:\\Users\\zhang\\Desktop\\employees.json") employee.show() employee.createOrReplaceTempView("employee") spark.sql("select * from employee").show() spark.stop()&#125; 2.3 数据类型转换将RDD，DataFrame，DataSet之间进行互相转换 RDD -》 DataFrame: 直接手动转换 1234567891011121314151617181920212223242526272829 scala&gt; val people = spark.read.json("/opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.json")people: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; val people1 = sc.textFile("/opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.txt")people1: org.apache.spark.rdd.RDD[String] = /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.txt MapPartitionsRDD[18] at textFile at &lt;console&gt;:24scala&gt; val peopleSplit = people1.map&#123;x =&gt; val strs = x.split(",");(strs(0),strs(1).trim.toInt)&#125;peopleSplit: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[19] at map at &lt;console&gt;:26scala&gt; peopleSplit.collectres6: Array[(String, Int)] = Array((Michael,29), (Andy,30), (Justin,19)) scala&gt; peopleSplit.totoDF toDS toDebugString toJavaRDD toLocalIterator toString topscala&gt; peopleSplit.toDFres7: org.apache.spark.sql.DataFrame = [_1: string, _2: int]scala&gt; peopleSplit.toDF("name","age")res8: org.apache.spark.sql.DataFrame = [name: string, age: int]scala&gt; res8.show+-------+---+| name|age|+-------+---+|Michael| 29|| Andy| 30|| Justin| 19|+-------+---+ 通过Scala编程实现 12345678910111213141516## 创建 schema scala&gt; val schema = StructType(StructField("name",StringType)::StructField("age",IntegerType)::Nil)schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true))## 加载RDD数据scala&gt; val rdd = sc.textFile("/opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.txt")rdd: org.apache.spark.rdd.RDD[String] = /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/people.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:30## 创建Row对象scala&gt; val data = rdd.map&#123;x =&gt; val strs = x.split(",");Row(strs(0),strs(1).trim.toInt)&#125;data: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[2] at map at &lt;console&gt;:32## 生成DFscala&gt; spark.createDataFrame(data,schema)18/09/06 09:45:00 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.018/09/06 09:45:00 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException18/09/06 09:45:02 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectExceptionres0: org.apache.spark.sql.DataFrame = [name: string, age: int] 反射 12345scala&gt; case class People(name:String,age:Int)defined class Peoplescala&gt; rdd.map&#123;x =&gt; val strs=x.split(",");People(strs(0),strs(1).trim.toInt)&#125;.toDFres2: org.apache.spark.sql.DataFrame = [name: string, age: int] DataFrame -》 RDD12scala&gt; res8.rddres10: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[26] at rdd at &lt;console&gt;:31RDD -》 DataSet1234567891011121314151617181920 scala&gt; peopleSplit.toDS res11: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int] scala&gt; case class People(name:String,age:Int) defined class People scala&gt; val peopleDSSplit = people1.map&#123;x =&gt; val strs = x.split(","); People(strs(0),strs(1).trim.toInt)&#125; peopleDSSplit: org.apache.spark.rdd.RDD[People] = MapPartitionsRDD[27] at map at &lt;console&gt;:28 scala&gt; peopleDSSplit.toDS res12: org.apache.spark.sql.Dataset[People] = [name: string, age: int] scala&gt; res12.show+-------+---+| name|age|+-------+---+|Michael| 29|| Andy| 30|| Justin| 19|+-------+---+DataSet -》 RDD: 12345scala&gt; res12.rddres14: org.apache.spark.rdd.RDD[People] = MapPartitionsRDD[32] at rdd at &lt;console&gt;:33scala&gt; res14.map(_.name).collectres15: Array[String] = Array(Michael, Andy, Justin)DataSet -》 DataFrame12scala&gt; res12.toDFres16: org.apache.spark.sql.DataFrame = [name: string, age: int]DataFrame -》 Datset12scala&gt; res16.as[People]res17: org.apache.spark.sql.Dataset[People] = [name: string, age: int]2.4 SQL的执行模式DSL风格语法1234567891011121314151617181920212223242526scala&gt; val peopleDF = rdd.map&#123;x =&gt; val strs=x.split(",");People(strs(0),strs(1).trim.toInt)&#125;.toDFpeopleDF: org.apache.spark.sql.DataFrame = [name: string, age: int]scala&gt; peopleDF.select("name").show+-------+| name|+-------+|Michael|| Andy|| Justin|+-------+scala&gt; peopleDF.filter($"age"&gt;20).show+-------+---+| name|age|+-------+---+|Michael| 29|| Andy| 30|+-------+---+scala&gt; peopleDF.groupBy("age").count.show+---+-----+ |age|count|+---+-----+| 19| 1|| 29| 1|| 30| 1|+---+-----+SQL风格语法123456789101112131415161718192021222324252627282930313233343536## 创建表## 借助于SparkSession## createOrReplaceTempView：Session内可以访问，一旦session 停止，表自动删除。不需要前缀，如果表存在，会自动替换## createGlobalOrReplaceTempView：一个应用级别的访问，多个session之间可以访问，但是一旦SparkContext关闭，也会删除。需要前缀## createTempView：Session内可以访问，一旦session 停止，表自动删除。不需要前缀，如果表存在则报错## createGlobalTempView：一个应用级别的访问，多个session之间可以访问，但是一旦SparkContext关闭，也会删除scala&gt; spark.sql("select * from people").show+-------+---+| name|age|+-------+---+|Michael| 29|| Andy| 30|| Justin| 19|+-------+---+scala&gt; spark.newSession.sql("select * from people").show## 出错scala&gt; spark.newSession.sql("select * from global_temp.people").show+-------+---+| name|age|+-------+---+|Michael| 29|| Andy| 30|| Justin| 19|+-------+---+scala&gt; spark.sql("select * from global_temp.people").show+-------+---+| name|age|+-------+---+|Michael| 29|| Andy| 30|| Justin| 19|+-------+---+3. 自定义函数3.1 UDF函数需求： 在每一行数据的name列值的前面haha1234567891011121314151617181920scala&gt; spark.sql("select * from people").show+-------+---+| name|age|+-------+---+|Michael| 29|| Andy| 30|| Justin| 19|+-------+---+scala&gt; spark.udf.register("add",(x:String)=&gt;"hehe"+x)res22: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))scala&gt; spark.sql("select add(name),age from people").show+-------------+---+|UDF:add(name)|age|+-------------+---+| heheMichael| 29|| heheAndy| 30|| heheJustin| 19|+-------------+---+3.2 UDAF函数需要通过继承 UserDefinedAggregateFunction 来实现自定义聚合函数。案例：计算一下员工的平均工资 弱类型聚合函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.zhiyou100.sparkimport org.apache.spark.SparkConfimport org.apache.spark.sql.&#123;Row, SparkSession&#125;import org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types._/** * 弱类型的 * 计算员工的平均薪资 */class AverageSalaryRuo extends UserDefinedAggregateFunction&#123; //输入的数据的格式 override def inputSchema: StructType = StructType(StructField("salary",IntegerType) :: Nil) //每个分区中共享的数据变量结构 override def bufferSchema: StructType = StructType(StructField("sum",LongType) :: StructField("count",IntegerType):: Nil) //输出的数据的类型 override def dataType: DataType = DoubleType //表示如果有相同的输入是否会存在相同的输出，是：true override def deterministic: Boolean = true //初始化的每个分区共享变量 override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0L buffer(1) = 0 &#125; //每一个分区的每一条数据聚合的时候进行buffer的更新 override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; //将buffer中的薪资总和的数据进行更新，原数据加上新输入的数据，buffer就类似于resultSet buffer(0) = buffer.getLong(0) + input.getInt(0) //每添加一个薪资，就将员工的个数加1 buffer(1) = buffer.getInt(1)+1 &#125; //将每个分区的输出合并 override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getInt(1)+buffer2.getInt(1) &#125; //获取最终的结果 override def evaluate(buffer: Row): Any = &#123; //计算平均薪资并返回 buffer.getLong(0).toDouble/buffer.getInt(1) &#125;&#125;object AverageSalaryRuo extends App&#123; val conf = new SparkConf().setAppName("udaf").setMaster("local[3]") val spark = SparkSession.builder().config(conf).getOrCreate() val data = spark.read.json("C:\\Users\\zhang\\Desktop\\employees.json") data.createOrReplaceTempView("employee") //注册自定义聚合函数 spark.udf.register("avgSalary",new AverageSalaryRuo) spark.sql("select avgSalary(salary) from employee").show() spark.stop()&#125; 强类型聚合函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.zhiyou100.sparkimport org.apache.spark.SparkConfimport org.apache.spark.sql.expressions.Aggregatorimport org.apache.spark.sql.&#123;Encoder, Encoders, SparkSession&#125;/** * 弱类型的 * 计算员工的平均薪资 *///对于强类型来说，无非就是借助于样例类case class Employee(name:String,salary:Long)case class Average(var sum:Long,var count:Int)class AverageSalaryQiang extends Aggregator[Employee,Average,Double]&#123; //初始化方法 override def zero: Average = Average(0L,0) //一个分区内的聚合调用，类似于update方法 override def reduce(b: Average, a: Employee): Average = &#123; b.sum = b.sum + a.salary b.count = b.count + 1 b &#125; override def merge(b1: Average, b2: Average): Average = &#123; b1.sum = b1.sum + b2.sum b1.count = b1.count + b2.count b1 &#125; //最终的计算结果 override def finish(reduction: Average): Double = &#123; reduction.sum.toDouble /reduction.count &#125; //对buffer编码 override def bufferEncoder: Encoder[Average] = Encoders.product //对out编码 override def outputEncoder: Encoder[Double] = Encoders.scalaDouble&#125;object AverageSalaryQiang extends App&#123; val conf = new SparkConf().setAppName("udaf").setMaster("local[3]") val spark = SparkSession.builder().config(conf).getOrCreate() import spark.implicits._ val employee = spark.read.json("C:\\Users\\zhang\\Desktop\\employees.json").as[Employee] employee.show() employee.createOrReplaceTempView("employee") //注册自定义函数 val aaa = new AverageSalaryQiang().toColumn.name("aaaa") spark.sql("select * from employee").show() //spark.sql("select aaaa(salary) from employee").show() employee.select(aaa).show() spark.stop()&#125;3.3 开窗函数over（）开窗函数: 在使用聚合函数后，会将多行变成一行，而开窗函数是将一行变成多行； 并且在使用聚合函数后，如果要显示其他的列必须将列加入到group by中，而使用开窗函数后，可以不使用group by，直接将所有信息显示出来。 开窗函数适用于在每一行的最后一列添加聚合函数的结果。 开窗函数作用: 为每条数据显示聚合信息.(聚合函数() over()) 为每条数据提供分组的聚合函数结果(聚合函数() over(partition by 字段) as 别名) --按照字段分组，分组后进行计算 与排名函数一起使用(row number() over(order by 字段) as 别名) 常用分析函数：（最常用的应该是1.2.3 的排序）: row_number() over(partition by … order by …) rank() over(partition by … order by …) dense_rank() over(partition by … order by …) count() over(partition by … order by …) max() over(partition by … order by …) min() over(partition by … order by …) sum() over(partition by … order by …) avg() over(partition by … order by …) first_value() over(partition by … order by …) last_value() over(partition by … order by …) lag() over(partition by … order by …) lead() over(partition by … order by …)lag 和lead 可以获取结果集中，按一定排序所排列的当前行的上下相邻若干offset 的某个行的某个列(不用结果集的自关联）；lag ，lead 分别是向前，向后；lag 和lead 有三个参数，第一个参数是列名，第二个参数是偏移的offset，第三个参数是超出记录窗口时的默认值 案例实战![image_1cmmpgfn07vudg5177i1sa51uq09.png-60.9kB][10]12数据格式姓名 班级 分数 4. Spark SQL的数据源4.1 与Hive集成4.1.1 使用内置的Hive: 123456[root@master spark-2.2.2-bin-hadoop2.7]# /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/bin/spark-shell --master spark://master:7077 --total-executor-cores 3 --executor-memory 512m --conf spark.sql.warehouse.dir=hdfs://master:9000/spark/warehousescala&gt; spark.sql("create table student(name String,age Int)")scala&gt; spark.sql("load data local inpath '/opt/test/spark/student' into table student")scala&gt; spark.sql("select * from student").show## 只有在第一次操作的时候需要指定数据文件存放的目录，第二次不需要指定。 4.1.2 使用外置的Hive: 1234## 将Hive的配置文件放到spark 的conf目录[root@master conf]# ln -s /opt/apps/Hive/hive-2.3.3/conf/hive-site.xml ./hive-site.xml## 将Hive中的mysql的驱动jar包上传到spark中[root@master lib]# cp mysql-connector-java-5.1.46-bin.jar /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/jars 启动spark-shell可以直接通过Hive的元数据信息进行Hive的管理 Spark SQL其实就是整合了很多数据库的操作，在于Hive整合的时候，只要有了Hive的元数据信息，以及元数据存储的数据的驱动就可以直接进行管理 4.2 输入输出格式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称定json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。[数据源][11] 1scala&gt; employee.write.format("jdbc").option("url","jdbc:mysql://master:3306/mysql_bigdata").option("user","root").option("password","123456").option("dbtable","employee").mode("overwrite").save() [保存数据的方式][12] 4.3 与数据库交互将数据导入MySQL123456789101112131415scala&gt; val employee = spark.read.json("/opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/src/main/resources/employees.json")employee: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]scala&gt; employee.show+-------+------+| name|salary|+-------+------+|Michael| 3000|| Andy| 4500|| Justin| 3500|| Berta| 4000|+-------+------+scala&gt; employee.write.format("jdbc").option("url","jdbc:mysql://master:3306/mysql_bigdata").option("user","root").option("password","123456").option("dbtable","employee").save()读取MySQL中的数据1scala&gt; spark.read.format("jdbc").option("url","jdbc:mysql://master:3306/mysql_bigdata").option("user","root").option("password","123456").option("dbtable","student").load().show5. Spark SQL架构原理5.1 Spark SQL 的模块Spark SQL模块划分为Core、caralyst、hive和hive- ThriftServer四大模块。![image_1cml86ltg16l2eio14m963b1q2rm.png-14.6kB][13] 5.2 Spark SQL的运行![image_1cml8bq7t3go1moefiirs01uh41g.png-93.6kB][14] 使用SessionCatalog保存元数据在解析SQL语句之前，会创建SparkSession，或者如果是2.0之前的版本初始化SQLContext，SparkSession只是封装了SparkContext和SQLContext的创建而已。会把元数据保存在SessionCatalog中，涉及到表名，字段名称和字段类型。创建临时表或者视图，其实就会往SessionCatalog注册 解析SQL,使用ANTLR生成未绑定的逻辑计划当调用SparkSession的sql或者SQLContext的sql方法，我们以2.0为准，就会使用SparkSqlParser进行解析SQL. 使用的ANTLR进行词法解析和语法解析。它分为2个步骤来生成Unresolved LogicalPlan：词法分析：Lexical Analysis，负责将token分组成符号类，构建一个分析树或者语法树AST 使用分析器Analyzer绑定逻辑计划在该阶段，Analyzer会使用Analyzer Rules，并结合SessionCatalog，对未绑定的逻辑计划进行解析，生成已绑定的逻辑计划。 使用优化器Optimizer优化逻辑计划优化器也是会定义一套Rules，利用这些Rule对逻辑计划和Exepression进行迭代处理，从而使得树的节点进行和并和优化 使用SparkPlanner生成物理计划SparkSpanner使用Planning Strategies，对优化后的逻辑计划进行转换，生成可以执行的物理计划SparkPlan. 使用QueryExecution执行物理计划此时调用SparkPlan的execute方法，底层其实已经再触发JOB了，然后返回DataFrame 6. 案例实战6.1 数据说明数据集是货品交易数据集。![image_1cml6v59u8p31c501bj97h31d859.png-64.1kB][15]每个订单可能包含多个货品，每个订单可以产生多次交易，不同的货品有不同的单价。 6.2 需求 统计所有订单中每年的销售单数、销售总额 统计每年最大金额订单的销售额 统计每年最畅销货品（哪个货品销售额amount在当年最高，哪个就是最畅销货品） ![11]( http://spark.apache.org/docs/2.2.2/sql-programming-guide.html#data-sources ![12]( http://spark.apache.org/docs/2.2.2/sql-programming-guide.html#save-modes]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark第一天]]></title>
    <url>%2F2019%2F04%2F25%2F1_Spark%20%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[Spark第一天1. Spark概述1.1 什么是Spark&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。项目是用Scala进行编写。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLLib、SparkR等子项目，Spark是基于内存计算的大数据并行计算框架。除了扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark 使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析 过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分别管理的负担。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大一统的软件栈，各个组件关系密切并且可以相互调用，这种设计有几个好处：1、软件栈中所有的程序库和高级组件 都可以从下层的改进中获益。2、运行整个软件栈的代价变小了。不需要运 行 5 到 10 套独立的软件系统了，一个机构只需要运行一套软件系统即可。系统的部署、维护、测试、支持等大大缩减。3、能够构建出无缝整合不同处理模型的应用。 1.2 Hadoop与Spark的区别![image_1cmdes1api2akv1194gsr0hnj9.png-110kB][1] 1.3 Spark 主要角色![image_1cmdevnb21s9kbj0rss1ae9js0m.png-59.9kB][2] Spark Core：实现了 Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core 中还包含了对弹性分布式数据集(resilient distributed dataset，简称RDD)的 API 定义。Spark SQL：是 Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比 如 Hive 表、Parquet 以及 JSON 等。Spark Streaming：是 Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。Spark MLlib：提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。集群管理器： Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark 支持在各种集群管理器(cluster manager)上运行，包括 Hadoop YARN、Apache Mesos，以及 Spark 自带的一个简易调度 器，叫作独立调度器。 1.4 Spark特点快 : 与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。 易用Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。通用Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。兼容性Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。2. 集群搭建2.1 集群角色两个Master（类似于Hadoop中的yarn ，ResourceManager），多个worker 2.2 安装步骤2.2.1 解压1[root@master Spark]# tar -zxvf spark-2.2.2-bin-hadoop2.7.tgz2.2.2 修改文件名12[root@master conf]# mv spark-env.sh.template spark-env.sh[root@master conf]# mv slaves.template slaves2.2.3 添加配置信息1234567[root@master conf]# vim spark-env.shSPARK_MASTER_HOST=masterSPARK_MASTER_PORT=7077[root@master conf]# vim slavesslave1slave2slave32.2.4 HA配置12[root@master conf]# vim spark-env.shexport SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2,slave3 -Dspark.deploy.zookeeper.dir=/spark"2.2.5 启动集群: 1234## 在master节点上启动start-all.sh## 在slave1节点上使用start-master.sh 3. 执行Spark程序123[root@master bin]# ./spark-submit \--master spark://master:7077,slave1:7077 \ --class org.apache.spark.examples.SparkPi \ /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.2.jar 10000 4. Spark Shell1234[root@master ~]# /opt/apps/Spark/spark-2.2.2-bin-hadoop2.7/bin/spark-shell## 在不指定集群的时候（master），可以正常启动shell，但是这种模式只是启动了本地模式。没有与集群建立连接scala&gt; sc.textFile("hdfs://master:9000/wc.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://master:9000/out2") 在Shell中可以直接编写Spark代码。SparkContext类会默认初始化为sc对象。 5. 本地模式执行SPark程序]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 之性能优化]]></title>
    <url>%2F2019%2F04%2F25%2F1_Spark%20%E4%B9%8B%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Spark 之性能优化1. Spark的通信架构Spark作为分布式计算框架，多个节点的设计与相互通信模式是其重要的组成部分。 Spark一开始使用 Akka 作为内部通信部件。在Spark 1.3的时候，为了解决大块数据（如Shuffle）的传输问题，Spark引入了Netty通信框架。到了 Spark 1.6, Spark可以配置使用 Akka 或者 Netty 了，这意味着 Netty 可以完全替代 Akka了。再到 Spark 2, Spark 已经完全抛弃 Akka了，全部使用Netty了。为什么呢？官方的解释是： 很多Spark用户也使用Akka，但是由于Akka不同版本之间无法互相通信，这就要求用户必须使用跟Spark完全一样的Akka版本，导致用户无法升级Akka。 Spark的Akka配置是针对Spark自身来调优的，可能跟用户自己代码中的Akka配置冲突。 Spark用的Akka特性很少，这部分特性很容易自己实现。同时，这部分代码量相比Akka来说少很多，debug比较容易。如果遇到什么bug，也可以自己马上fix，不需要等Akka上游发布新版本。而且，Spark升级Akka本身又因为第一点会强制要求用户升级他们使用的Akka，对于某些用户来说是不现实的。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark之MLlib]]></title>
    <url>%2F2019%2F04%2F25%2F1_Spark%20%E4%B9%8B%20MLlib%2F</url>
    <content type="text"><![CDATA[Spark 之 MLlib1. 机器学习1.1 什么是机器学习机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。简单来说就是通过算法使计算机能够模拟人类的判别能力。 它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。 1.2 机器学习的应用![image_1cn1s012qel8e8i1i2cbm9hrp9.png-176.1kB][1] 1.3 机器学习中的几个分类监督学习(supervised learning)监督学习，即在机械学习过程中提供对错指示。一般实在是数据组中包含最终结果（0，1）。通过算法让机器自我减少误差。这一类学习主要应用于分类和预测 (regression &amp; classify)。监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。非监督学习(unsupervised learning)非监督学习又称归纳性学习（clustering）利用K方式(Kmeans)，建立中心（centriole），通过循环和递减运算(iteration&amp;descent)来减小误差，达到分类的目的。强化学习Alphago用的就是强化学习，强化学习是一种学习模型，它并不会直接给你解决方案——你要通过试错去找到解决方案。强化学习不需要标签，你选择的行动（move）越好，得到的反馈越多，所以你能通过执行这些行动看是输是赢来学习下围棋，不需要有人告诉你什么是好的行动什么是坏的行动。深度学习深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。1.4 机器学习怎么用![image_1cn1t695fk4715t1jat1stgl4d16.png-109.3kB][2] 1.5 常见的几个概念1.5.1 数据集与测试集一组数据的集合被称作数据集，用于模型训练的数据集叫训练集，用于测试的数据集叫测试集。一个数据集包含多条数据，一条数据包含多个属性。 ![image_1cn1tej521deo18qrh901pou1n2l1j.png-42.9kB][3] 1.5.2 维度、特征对于西瓜数据集，色泽、根蒂、敲声就是维度，也叫特征值。 ![image_1cn1tej521deo18qrh901pou1n2l1j.png-42.9kB][4] 1.5.3 泛化能力是指机器学习通过训练集进行模型的训练之后对未知的输入的准确判断能力 1.5.4 过拟合和欠拟合过拟合是指在利用训练数据进行模型训练的时候，模型过多的依赖训练数据中过多的特征属性。欠拟合是指没有通过训练集达到识别的能力。![image_1cn1u7lun153163k10cb70p33220.png-120.6kB][5] 1.5.5 模型模型就是复杂的数学相关函数，只是该函数具有很多的未知的参数，通过训练集训练来确定模型中的参数，生成的已知参数的函数就是模型。学习就是根据业务构建模型的过程。 机器学习分为有监督学习和无监督学习，有监督学习是指训练集中有明确的标记，如下数据集：各种特征的西瓜是不是好瓜，有明确的标记。分类就是典型的有监督学习。无监督学习是指训练集中没有明确的标记，聚类就是典型的无监督学习。 1.5.6 损失函数在统计学，统计决策理论和经济学中，损失函数是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数。更通俗地说，在统计学中损失函数是一种衡量损失和错误（这种损失与“错误地”估计有关，如费用或者设备的损失）程度的函数。简单来说，通过模型预测的预测值与真实值的差产生的函数就是损失函数。常见的损失函数如下：$$ J(\theta)=\frac{1}{2}\sum_{i-1}^m{h_\theta(x^i-y^i)^2}$$ 常用损失函数: 分类算法，其损失函数一般有和两种: 对数损失函数，与Adaboost类似 指数损失函数，分为二元分类和多元分类两种 回归算法，常用损失函数有如下4种: 均方差 绝对损失 Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。 1.5.7 查全率与查准率查准率（正确率）和查全率（召回率）是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；查全率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。举个例子： 某池塘有1400条鲤鱼，300只虾，300只鳖。现在以捕鲤鱼为目的。撒一大网，逮着了700条鲤鱼，200只虾，100只鳖。那么，这些指标分别如下：查准率 = 700 / (700 + 200 + 100) = 70%查全率 = 700 / 1400 = 50% 1.5.8 评估参数均方误差（MSE）均方误差（mean-square error, MSE）是反映估计量与被估计量之间差异程度的一种度量。是指参数估计值与参数真实值之差平方的期望。MSE 可以评价数据的变化程度，MSE 的值越小，说明预测模型描述实验数据具有更好的精确度。$$ MSE=\frac{1}{N}\sum_{t=1}^N{(observed_t-predicted_t)^2}$$均方根误差（RMSE）均方根误差（Root Mean Square Error）亦称标准误差,是均方误差的算术平方根。$$ RMSE=\sqrt{\frac{1}{N}\sum_{t=1}^N{(observed_t-predicted_t)^2}}$$平均绝对误差（MAE）平均绝对误差（Mean Absolute Deviation），又叫平均绝对离差，是所有单个观测值与算术平均值的偏差的绝对值的平均。平均绝对误差可以避免误差相互抵消的问题，因而可以准确反映实际预测误差的大小。$$ MAE=\frac{1}{N}\sum_{i=1}^N{\left|(f_i-y_i)\right|}$$标准差（SD）标准差（Standard Deviation） ，中文环境中又常称均方差，是离均差平方的算术平均数的平方根，用$\sigma$表示。反应的是一个数据集的离散程度，平均数相容的两组数据，标准差未必相同。$$ RMSE=\sqrt{\frac{1}{N}\sum_{i=1}^N{(x_i-u)^2}}$$其中$u$表示平均值（$u=\frac{1}{N}(x_1+…+x_N)$）1.7 机器学习的常见算法逻辑回归: 优点：计算代价不高，易于理解和实现。 缺点：容易欠拟合，分类精度可能不高。 关键词：Sigmoid函数、Softmax解决多分类 适用数据类型：数值型和标称型数据。 其它：逻辑回归函数虽然是一个非线性的函数，但其实其去除Sigmoid映射函数之后，其他步骤都和线性回归一致。 支持向量机: 优点：适合小数量样本数据，可以解决高维问题，理论基础比较完善，对于学数学的来说它的理论很美;可以提高泛化能力； 缺点：数据量大时，内存资源消耗大（存储训练样本和核矩阵），时间复杂度高，这时候LR等算法就比SVM要好；对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数,对于核函数的运用对SVM来说确实是一个亮点，但是核函数不是SVM专属的，其他算法一旦涉及到内积运算，就可以使用核函数。它的优化方向的话就是各种不同的场景了，比如扩展到多分类，类别标签不平衡等都可以对SVM做些改变来适应场景 关键词：最优超平面 最大间隔 拉格朗日乘子法 对偶问题 SMO求解 核函数 hinge损失 松弛变量 惩罚因子 多分类 适用数据类型：数值型和标称型数据。 参数：选择核函数，如径向基函数（低维到高维）、线性核函数，以及核函数的参数; 惩罚因子 其它：SVM也并不是在任何场景都比其他算法好，SVM在邮件分类上不如逻辑回归、KNN、bayes的效果好，是基于距离的模型，需要归一化 决策树: 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。 缺点：过拟合，可限制树深度及叶子节点个数 关键词：ID3(信息增益) C4.5(信息增益比) CART(基尼系数) 数据要求：标称型数据，因此数值型数据必须离散化 随机森林: 优点： 对很多数据集表现很好，精确度比较高 不容易过拟合 可以得到变量的重要性排序 既能处理离散型数据，也能处理连续型数据，且不需要进行归一化处理 能够很好的处理缺失数据 容易并行化 AdaboostAdaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。是一种典型的boosting算法。下面是总结下它的优缺点。 优点 adaboost是一种有很高精度的分类器。 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。 简单，不用做特征筛选。 不容易发生overfitting。 缺点：对outlier比较敏感 GBDT: 优点： 精度高 适用于线性和非线性数据 能处理多特征类型，共线性特征 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。 能处理缺失值。使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 缺点： 难并行 多类别分类，时空复杂度高 正则化防止过拟合： 第一种是和Adaboost类似的正则化项，即步长(learning rate) 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。 第三种是对于弱学习器即CART回归树进行正则化剪枝。 xgboost这是一个近年来出现在各大比赛的大杀器，夺冠选手很大部分都使用了它。高准确率高效率高并发，支持自定义损失函数，既可以用来分类又可以用来回归可以像随机森林一样输出特征重要性，因为速度快，适合作为高维特征选择的一大利器在目标函数中加入正则项，控制了模型的复杂程度，可以避免过拟合支持列抽样，也就是随机选择特征，增强了模型的稳定性对缺失值不敏感，可以学习到包含缺失值的特征的分裂方向另外一个广受欢迎的原因是支持并行，速度杠杠的用的好，你会发现他的全部都是优点朴素贝叶斯优点：对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练; 对缺失数据不太敏感，算法也比较简单，常用于文本分类。缺点：朴素贝叶斯模型假设属性之间相互独立，实际应用中属性之间相关性较大时，分类效果不好;需要知道先验概率，且先验概率很多时候取决于假设数据类型：标称型数据。朴素：特征之间相互独立；每个特征同等重要。高偏差低方差模型注意事项：Laplace校准 K-近邻算法(KNN)优点：精度高、对异常值不敏感、无数据输入假定。缺点：计算复杂度高，空间复杂度，数据不平衡问题。KD-Tree数据类型：数值型和标称型。其它：K值如何选择；数据不平衡时分类倾向更多样本的类，解决方法是距离加权重k值的选择：当k值较小时，预测结果对近邻的实例点非常敏感，容易发生过拟合；如果k值过大模型会倾向大类，容易欠拟合；通常k是不大于20的整数（参考《机器学习实战》） K-Means(K 均值算法)优点：容易实现。缺点：K值不容易确定，对初始值敏感，可能收敛到局部最小值。KD-Tree数据类型：数值型数据。K值的确定：簇类指标（半径、直径）出现拐点克服K-均值算法收敛于局部最小值，需确定初始聚类中心：K-Means++算法：初始的聚类中心之间的相互距离要尽可能的远。二分K-均值算法：首先将所有点作为一个簇，然后将簇一分为二。之后选择其中一个簇继续划分，选择哪个一簇进行划分取决于对其划分是否可以最大程度降低SSE(Sum of Squared Error，两个簇的总误差平方和)的值。Apriori算法优点：容易实现。缺点：在大型数据集上速度较慢。空间复杂度高，主要是C2候选项;时间复杂度高，需多次扫描数据库数据类型：数值型或标称型数据原理：如果某个项集时频繁的，那么他的所有子集也是频繁的。简述：Apriori算法是发现频繁项集的一种方法。Apriori算法的两个输入参数分别是最小支持度和数据集。该算法首先会生成所有单个item的项集列表。然后扫描列表计算每个item的项集支持度，将低于最小支持度的item排除掉，然后将每个item两两组合，然后重新计算整合后的item列表的支持度并且和最小支持度比较。重复这一过程，直至所有项集都被去掉。FPGrowth算法优点：时间复杂度和空间复杂度都要优于Apriori。缺点：实现比较困难，在某些数据集上性能会下降原理：标称型数据关键词：过滤不频繁集合，项头表支持度排序 FP树 条件模式基 条件树简述：FP-growth也是用于发现频繁项集的算法，基本数据结构包含一个一棵FP树和一个项头表。构建FP树时只对数据集扫描两次，第二次从FP树中挖掘频繁项集。继续改进方法：包括数据库划分，数据采样人工神经网络优点：分类的准确度高；并行分布处理能力强,分布存储及学习能力强，对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系；具备联想记忆的功能。缺点：神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值；不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度；学习时间过长,甚至可能达不到学习的目的。1.8 什么是Spark MLlib![image_1cn219m8o16aau1q1tc5h57rad47.png-70.8kB][6] MLlib是Spark的机器学习（Machine Learning）库，旨在简化机器学习的工程实践工作，并方便扩展到更大规模。MLlib由一些通用的学习算法和工具组成，包括分类、回归、聚类、协同过滤、降维等，同时还包括底层的优化原语和高层的管道API。 ###]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>MLlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 之 Graphx]]></title>
    <url>%2F2019%2F04%2F25%2F1_Spark%20%E4%B9%8B%20Graphx%2F</url>
    <content type="text"><![CDATA[Spark 之 Graphx1. Graphx 概述1.1 什么是 Graphx![image_1cmv2t6351udeheo17feif818sgm.png-77.5kB][1] Spark GraphX是一个并行图计算框架，它是基于Spark平台提供对图计算和图挖掘简洁易用的而丰富的接口，极大的方便了对饼形图处理的需求。那么什么是图，都计算些什么？众所周知社交网络中人与人之间有很多关系链，例如Twitter、Facebook、微博和微信等，数据中出现网状结构关系都需要图计算。 GraphX是一个新的Spark API，它用于图和并行图(graph-parallel)的计算。GraphX通过引入弹性分布式属性图（Resilient Distributed Property Graph）： 顶点和边均有属性的有向多重图，来扩展Spark RDD。为了支持图计算，GraphX开发了一组基本的功能操作以及一个优化过的Pregel API。另外，GraphX也包含了一个快速增长的图算法和图builders的集合，用以简化图分析任务。 从社交网络到语言建模，不断增长的数据规模以及图形数据的重要性已经推动了许多新的分布式图系统的发展。 通过限制计算类型以及引入新的技术来切分和分配图，这些系统可以高效地执行复杂的图形算法，比一般的分布式数据计算（data-parallel，如spark、MapReduce）快很多。 ![image_1cmv2i1fs13e9bi1sftrs919bg9.png-149kB][2] 1.2 Graphx 的特点灵活性高![image_1cmv30vj68jrssb1c9m27term1j.png-95.1kB][3]图可以和集合进行无缝对接GraphX统一了单个系统中的ETL、探索性分析和迭代图计算。您可以查看与图形和集合相同的数据，有效地使用RDD 转换和图的连接操作，以及使用 Pregel API 编写自定义迭代图算法速度快![image_1cmv35mns1gdo1sei1jvk16uceac20.png-86kB][4]可以与最快的专业图形处理系统相媲美。GraphX与最快的图形系统竞争性能，同时保留Spark的灵活性，容错性和易用性。算法多![image_1cmv39mjn186e1nu61fdv1vt1luf2d.png-66.5kB][5]从不断增加的图算法库中进行选择。除了高度灵活的API之外，GraphX还提供了各种图形算法，其中许多都是由我们的用户提供的。网页排名，连接组件，标签传播，SVD ++，强大的连接组件以及三角计数等1.3 学什么本章主要学习图如何构建，如何计算图，如何应用 1.4 关键抽象![image_1cmv3h2369kt12m51jtilon1ro52q.png-36.5kB][6] RDPG： Resilient Distributed Property Graph（弹性分布式属性图） GraphX的核心抽象是弹性分布式属性图，它是一个有向多重图，带有连接到每个顶点和边的用户定义的对象。 有向多重图中多个并行的边共享相同的源和目的顶点。支持并行边的能力简化了建模场景，相同的顶点可能存在多种关系(例如co-worker和friend)。 每个顶点用一个唯一的64位长的标识符（VertexID）作为key。GraphX并没有对顶点标识强加任何排序。同样，边拥有相应的源和目的顶点标识符。![image_1cmv3u1as1pbqv9jgrqv6trfj3k.png-91kB][7] 相关概念: 顶点RDD[(VertexId, VD)] 表示顶点。 VertexId 就是Long类型，表示顶点的ID【主键】。 VD表示类型参数，可以是任意类型, 表示的是该顶点的属性。 VertexRDD[VD] 继承了RDD[(VertexId, VD)]， 他是顶点的另外一种表示方式，在内部的计算上提供了很多的优化还有一些更高级的API。 边 RDD[Edge[VD]] 表示边， Edge中有三个东西： srcId表示 源顶点的ID， dstId表示的是目标顶点的ID， attr表示表的属性，属性的类型是VD类型，VD是一个类型参数，可以是任意类型。 EdgeRDD[ED] 继承了 RDD[Edge[ED]] ,他是边的另外一种表示方式，在内部的计算上提供您改了很多的优化还有一些更高级的API。![image_1cmv4s5hm1co815181ijg2qg1t0h41.png-216.7kB][8] 三元组EdgeTriplet[VD, ED] extends Edge[ED] 他表示一个三元组， 比边多了两个顶点的属性。包含以下内容srcId、srcAttr、 attr、 dstid、dstAttr![image_1cmv52otn11c3g1v1uecte21qgk4e.png-30.5kB][9] 图 Graph[VD: ClassTag, ED: ClassTag] VD 是顶点的属性、 ED是边的属性 2. 简单案例2.1 创建一个图目标![image_1cmv5rcf6g27obk1rvkd0ko8a4r.png-216.7kB][10]将上述图中的顶点和边输入到一个图中添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-graphx_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt;代码详情1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.zhiyou100.sparkimport org.apache.spark.graphx.&#123;Edge, Graph, VertexId&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext, graphx&#125;object GraphxHelloWorld extends App&#123; //配置对象 val conf = new SparkConf().setAppName("graphx").setMaster("local[3]") //创建sc对象 val sc = new SparkContext(conf) //创建顶点数据集 val vertexRDD:RDD[(VertexId,(String,String))] = sc.makeRDD(Array( (3L,("zhangsan","student")), (7L,("wangchen","博士后")), (5L,("zhangyu","教授")), (2L,("wangguo","教授")) )) //创建边的数据 val edgesRDD:RDD[Edge[String]] = sc.makeRDD(Array( Edge(3L,7L,"合作者"), Edge(5L,3L,"指导"), Edge(2L,5L,"同事"), Edge(5L,7L,"同事") )) //构建一个图 val graphx = Graph(vertexRDD,edgesRDD) //RDD展示 val result = graphx.triplets.collect() result.foreach( triplet =&gt; println(s"srcId=$&#123;triplet.srcId&#125; srcAttr=$&#123;triplet.srcAttr&#125;--edge=$&#123;triplet.attr&#125;--dstId=$&#123;triplet.dstId&#125; dstAttr=$&#123;triplet.dstAttr&#125; ") ) sc.stop()&#125;3. 创建3.1 def apply[VD: ClassTag, ED: ClassTag]3.2 根据边构建图根据边直接创建， 所有顶点的属性都一样为 defaultValue def fromEdges[VD: ClassTag, ED: ClassTag] 12 3.3 根据边的两个顶点数据构建根据裸边来进行创建，顶点的属性是 defaultValue ，边的属性为1 def fromEdgeTuples[VD: ClassTag] 12 4. 计算模式4.1 基本信息获取边的数量val numEdges: Long获取顶点的数量val numVertices: Long获取所有顶点的入度val inDegrees: VertexRDD[Int]获取所有顶点的出度val outDegrees: VertexRDD[Int]获取所有顶点入度与出度之和val degrees: VertexRDD[Int]1234567891011121314151617181920212223242526scala&gt; graphx.numEdgesres0: Long = 4 scala&gt; graphx.numVerticesres1: Long = 4 scala&gt; graphx.inDegreesres2: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[19] at RDD at VertexRDD.scala:57scala&gt; graphx.inDegrees.collectres3: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((3,1), (7,2), (5,1))scala&gt; graphx.outDegrees.collectres4: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((3,1), (5,2), (2,1))scala&gt; graphx.degrees.collectres5: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((3,2), (7,2), (5,3), (2,1))scala&gt; graphx.edges.collectres6: Array[org.apache.spark.graphx.Edge[String]] = Array(Edge(3,7,合作者), Edge(5,3,指导), Edge(2,5,同事), Edge(5,7,同事))scala&gt; graphx.vertices.collectres7: Array[(org.apache.spark.graphx.VertexId, (String, String))] = Array((3,(zhangsan,student)), (7,(wangchen,博士后)), (5,(zhangyu,教授)), (2,(wangguo,教授)))scala&gt; graphx.triplets.collectres8: Array[org.apache.spark.graphx.EdgeTriplet[(String, String),String]] = Array(((3,(zhangsan,student)),(7,(wangchen,博士后)),合作者), ((5,(zhangyu,教授)),(3,(zhangsan,student)),指导), ((2,(wangguo,教授)),(5,(zhangyu,教授)),同事), ((5,(zhangyu,教授)),(7,(wangchen,博士后)),同事)) 4.2 转换操作GraphX中的转换操作主要有mapVertices,mapEdges和mapTriplets三个，它们在Graph文件中定义，在GraphImpl文件中实现。下面分别介绍这三个方法。 4.2.1 mapVerticesmapVertices用来更新顶点属性。从图的构建那章我们知道，顶点属性保存在边分区中，所以我们需要改变的是边分区中的属性。对图中的每一个顶点进行map操作，顶点的ID不能改变，可以将顶点的属性改变成另外一种类型。 4.2.2 mapEdgesdef mapEdges[ED2: ClassTag](map: Edge[ED] =&gt; ED2): Graph[VD, ED2] 对图中的每一个边进行map操作， 边的方向不能改变，可以将边的属性改变为另外一种类型 4.2.3 mapTriplets函数定义 函数名 [传入的泛型] （参数：函数（被调用者每一个元素为EdgeTriplet，也就是说这里面的函数的参数就是这种的类型的，拿到这个参数后做什么事都无所谓，但是要保证这个函数返回为ED2））def mapTriplets[ED2: ClassTag](map: EdgeTriplet[VD, ED] =&gt; ED2): Graph[VD, ED2] 对于图中的每一个三元组进行map操作， 只能修改边的属性。 4.3 结构操作4.3.1 reversereverse操作返回一个新的图，这个图的边的方向都是反转的。例如，这个操作可以用来计算反转的PageRank。因为反转操作没有修改顶点或者边的属性或者改变边的数量，所以我们可以 在不移动或者复制数据的情况下有效地实现它。def reverse: Graph[VD, ED] 1234567891011scala&gt; graphx.mapVertices((id,attr)=&gt;attr._1+":"+attr._2).triplets.collect.foreach(triplet =&gt; println(s"srcId=$&#123;triplet.srcId&#125; srcAttr=$&#123;triplet.srcAttr&#125;--edge=$&#123;triplet.attr&#125;--dstId=$&#123;triplet.dstId&#125; dstAttr=$&#123;triplet.dstAttr&#125; "))srcId=3 srcAttr=zhangsan:student--edge=合作者--dstId=7 dstAttr=wangchen:博士后 srcId=5 srcAttr=zhangyu:教授--edge=指导--dstId=3 dstAttr=zhangsan:student srcId=2 srcAttr=wangguo:教授--edge=同事--dstId=5 dstAttr=zhangyu:教授 srcId=5 srcAttr=zhangyu:教授--edge=同事--dstId=7 dstAttr=wangchen:博士后 scala&gt; graphx.mapVertices((id,attr)=&gt;attr._1+":"+attr._2).reverse.triplets.collect.foreach(triplet =&gt; println(s"srcId=$&#123;triplet.srcId&#125; srcAttr=$&#123;triplet.srcAttr&#125;--edge=$&#123;triplet.attr&#125;--dstId=$&#123;triplet.dstId&#125; dstAttr=$&#123;triplet.dstAttr&#125; "))srcId=7 srcAttr=wangchen:博士后--edge=合作者--dstId=3 dstAttr=zhangsan:student srcId=3 srcAttr=zhangsan:student--edge=指导--dstId=5 dstAttr=zhangyu:教授 srcId=5 srcAttr=zhangyu:教授--edge=同事--dstId=2 dstAttr=wangguo:教授 srcId=7 srcAttr=wangchen:博士后--edge=同事--dstId=5 dstAttr=zhangyu:教授 4.3.2 subgraphsubgraph操作利用顶点和边的判断式（predicates），返回的图仅仅包含满足顶点判断式的顶点、满足边判断式的边以及满足顶点判断式的triple。subgraph操作可以用于很多场景，如获取 感兴趣的顶点和边组成的图或者获取清除断开连接后的图。def subgraph( epred: EdgeTriplet[VD, ED] =&gt; Boolean = (x =&gt; true), vpred: (VertexId, VD) =&gt; Boolean = ((v, d) =&gt; true)) : Graph[VD, ED] 123456789scala&gt; graphx.subgraph(x=&gt;if(x.attr=="同事") true else false,(a,b)=&gt;true).triplets.collect.foreach(tripl =&gt; println(s"srcId=$&#123;triplet.srcId&#125; srcAttr=$&#123;triplet.srcAttr&#125;--edge=$&#123;triplet.attr&#125;--dstId=$&#123;triplet.dstId&#125; dstAttr=$&#123;triplet.dstAttr&#125; "))srcId=2 srcAttr=(wangguo,教授)--edge=同事--dstId=5 dstAttr=(zhangyu,教授) srcId=5 srcAttr=(zhangyu,教授)--edge=同事--dstId=7 dstAttr=(wangchen,博士后) scala&gt; graphx.vertices.collectres19: Array[(org.apache.spark.graphx.VertexId, (String, String))] = Array((3,(zhangsan,student)), (7,(wangchen,博士后)), (5,(zhangyu,教授)), (2,(wangguo,教授)))scala&gt; graphx.subgraph(x=&gt;if(x.attr=="同事") true else false,(a,b)=&gt;true).vertices.collectres20: Array[(org.apache.spark.graphx.VertexId, (String, String))] = Array((3,(zhangsan,student)), (7,(wangchen,博士后)), (5,(zhangyu,教授)), (2,(wangguo,教授))) 4.3.3 maskmask操作构造一个子图，类似于交集，这个子图包含输入图中包含的顶点和边。它的实现很简单，顶点和边均做inner join操作即可。这个操作可以和subgraph操作相结合，基于另外一个相关图的特征去约束一个图。只使用ID进行对比，不对比属性。def mask[VD2: ClassTag, ED2: ClassTag](other:Graph[VD2, ED2]): Graph[VD, ED] 12345scala&gt; graphx.mask(graphx1).triplets.collect.foreach(triplet =&gt; println(s"srcId=$&#123;triplet.srcId&#125; srcAttr=$&#123;triplet.srcAttr&#125;--edge=$&#123;triplet.attr&#125;--dstId=$&#123;triplet.dstId&#125; dstAttr=$&#123;triplet.dstAttr&#125; "))srcId=3 srcAttr=(zhangsan,student)--edge=合作者--dstId=7 dstAttr=(wangchen,博士后) srcId=5 srcAttr=(zhangyu,教授)--edge=指导--dstId=3 dstAttr=(zhangsan,student) srcId=5 srcAttr=(zhangyu,教授)--edge=同事--dstId=7 dstAttr=(wangchen,博士后) 4.3.4 groupEdgesgroupEdges操作合并多重图中的并行边(如顶点对之间重复的边)，并传入一个函数来合并两个边的属性。在大量的应用程序中，并行的边可以合并（它们的权重合并）为一条边从而降低图的大小。（两个边需要在一个分区内部才行）。合并两条边，通过函数合并边的属性。 【注意】两条边要在一个分区中def groupEdges(merge: (ED, ED) =&gt; ED): Graph[VD, ED] 123456789101112scala&gt; graphx2.groupEdges((e1,e2)=&gt;e1+e2).triplets.collect.foreach(triplet =&gt; println(s"srcId=$&#123;triplet.srcId&#125; srcAttr=$&#123;triplet.srcAttr&#125;--edge=$&#123;triplet.attr&#125;--dstId=$&#123;triplet.dstId&#125; dstAttr=$&#123;triplet.dstAttr&#125; "))srcId=3 srcAttr=(zhangsan,student)--edge=合作者--dstId=7 dstAttr=(wangchen,博士后) srcId=2 srcAttr=(wangguo,教授)--edge=同事--dstId=5 dstAttr=(zhangyu,教授) srcId=5 srcAttr=(zhangyu,教授)--edge=指导--dstId=3 dstAttr=(zhangsan,student) srcId=5 srcAttr=(zhangyu,教授)--edge=同事表兄弟--dstId=7 dstAttr=(wangchen,博士后) scala&gt; graphx2.triplets.collect.foreach(triplet =&gt; println(s"srcId=$&#123;triplet.srcId&#125; srcAttr=$&#123;triplet.srcAttr&#125;--edge=$&#123;triplet.attr&#125;--dstId=$&#123;triplet.dstId&#125; dstAttr=$&#123;triplet.dstAttr&#125; "))srcId=3 srcAttr=(zhangsan,student)--edge=合作者--dstId=7 dstAttr=(wangchen,博士后) srcId=2 srcAttr=(wangguo,教授)--edge=同事--dstId=5 dstAttr=(zhangyu,教授) srcId=5 srcAttr=(zhangyu,教授)--edge=指导--dstId=3 dstAttr=(zhangsan,student) srcId=5 srcAttr=(zhangyu,教授)--edge=同事--dstId=7 dstAttr=(wangchen,博士后) srcId=5 srcAttr=(zhangyu,教授)--edge=表兄弟--dstId=7 dstAttr=(wangchen,博士后) 4.4 聚合4.4.1 collectNeighbors该方法的作用是收集每个顶点的邻居顶点的顶点id和顶点属性。需要指定方向EdgeDirection.out：出的方向EdgeDirection.in:入的方向EdgeDirection.Either：入边或出边EdgeDirection.Both：入边且出边 def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[Array[(VertexId, VD)]] 收集邻居节点的数据，根据指定的方向。返回的数据为RDD[(VertexId, Array[(VertexId, VD)])] 12scala&gt; graphx.collectNeighbors(EdgeDirection.Either).collectres29: Array[(org.apache.spark.graphx.VertexId, Array[(org.apache.spark.graphx.VertexId, (String, String))])] = Array((3,Array((5,(zhangyu,教授)), (7,(wangchen,博士后)))), (7,Array((5,(zhangyu,教授)), (3,(zhangsan,student)))), (5,Array((2,(wangguo,教授)), (7,(wangchen,博士后)), (3,(zhangsan,student)))), (2,Array((5,(zhangyu,教授))))) 4.4.2 collectNeighborIds该方法的作用是收集每个顶点的邻居顶点的顶点id。它的实现和collectNeighbors非常相同。需要指定方向 def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]] 4.4.3 aggregateMessagesdef aggregateMessages[A: ClassTag]( sendMsg: EdgeContext[VD, ED, A] =&gt; Unit, mergeMsg: (A, A) =&gt; A, tripletFields: TripletFields = TripletFields.All) : VertexRDD[A] 每一个边都会通过sendMsg 发送一个消息， 每一个顶点都会通过mergeMsg 来处理所有他收到的消息。 TripletFields存在主要用于定制 EdgeContext 对象中的属性的值是否存在，为了减少数据通信量。 1234567891011121314151617181920212223242526272829303132333435363738val vertexRDD3 = sc.makeRDD(Array( (1L,("zhang1",25)), (2L,("zhang2",18)), (3L,("zhang3",45)), (4L,("zhang4",20)), (5L,("zhang5",70)), (6L,("zhang6",79)), (7L,("zhang7",101)) )) val edgesRDD3=sc.makeRDD(Array( Edge(1L,2L,"1"), Edge(3L,2L,"1"), Edge(1L,4L,"1"), Edge(7L,2L,"1"), Edge(3L,5L,"1"), Edge(7L,6L,"1"), Edge(6L,4L,"1") )) val graph3 = Graph(vertexRDD3,edgesRDD3) /* aggregateMessages[A: ClassTag]( sendMsg: EdgeContext[VD, ED, A] =&gt; Unit, mergeMsg: (A, A) =&gt; A, ) */ graph3.aggregateMessages[Int]( //发送消息:Map阶段 triplet=&gt;&#123; //将追求者的年龄发送给被追求者 triplet.sendToDst(triplet.srcAttr._2) //triplet.sendToSrc(triplet.dstAttr._2) &#125; ,//Reduce (a,b)=&gt;math.max(a,b) ) 4.5 关联操作4.5.1 joinVerticesdef joinVertices[U: ClassTag](table: RDD[(VertexId, U)])(mapFunc: (VertexId, VD, U) =&gt; VD) : Graph[VD, ED] 将相同顶点ID的数据进行加权,将U这种类型的数据加入到 VD这种类型的数据上，但是不能修改VD的类型。 4.5.2 outerJoinVerticesdef outerJoinVertices[U: ClassTag, VD2: ClassTag](other: RDD[(VertexId, U)]) (mapFunc: (VertexId, VD, Option[U]) =&gt; VD2)(implicit eq: VD =:= VD2 = null) : Graph[VD2, ED] 和joinVertices类似。，只不过是如果没有相对应的节点，那么join的值默认为None。 4.6 Pregel4.6.1 计算模型12345678def pregel[A: ClassTag] ( initialMsg: A,//图初始化的时候，开始模型计算的时候，所有节点都会收到的一个默认的消息 maxIterations: Int = Int.MaxValue,//最大的迭代次数 activeDirection: EdgeDirection = EdgeDirection.Either)//发送消息的方向 (vprog: (VertexId, VD, A) =&gt; VD,//节点调用该消息将聚合后的数据和本节点进行属性的合并 sendMsg: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexId, A)],//激活态的节点调用这个方法发送消息 mergeMsg: (A, A) =&gt; A)//如果一个节点收到多条消息，那么就会使用mergeMsg将消息合并为一条消息，如果只接收到一条消息，则不合并 4.6.2 最短路计算123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package com.zhiyou100.sparkimport org.apache.spark.graphx.&#123;Edge, EdgeDirection, Graph, VertexId&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext, graphx&#125;object GraphxHelloWorld extends App&#123; //配置对象 val conf = new SparkConf().setAppName("graphx").setMaster("local[3]") //创建sc对象 val sc = new SparkContext(conf) //最短路径算法:计算的是从V1到所有顶点的最短路径 val vertexRDD4:RDD[(VertexId,Int)] = sc.makeRDD(Array( (1L,0), (2L,Int.MaxValue), (3L,Int.MaxValue), (4L,Int.MaxValue), (5L,Int.MaxValue), (6L,Int.MaxValue), (7L,Int.MaxValue), (8L,Int.MaxValue), (9L,Int.MaxValue) )) val edgesRDD4 = sc.makeRDD(Array( Edge(1L,2L,6), Edge(1L,3L,3), Edge(1L,4L,1), Edge(3L,2L,2), Edge(3L,4L,2), Edge(2L,5L,1), Edge(5L,4L,6), Edge(5L,6L,4), Edge(6L,5L,10), Edge(5L,7L,3), Edge(5L,8L,6), Edge(4L,6L,10), Edge(6L,7L,2), Edge(7L,8L,4), Edge(9L,5L,2), Edge(9L,8L,3) )) val graph4 = Graph(vertexRDD4,edgesRDD4) val r = graph4.pregel[Int]( //初始化消息的数据 /*graph4.mapVertices((id,v)=&gt;if(id == 1L) 0 else Int.MaxValue),*/ //最大的迭代次数 Int.MaxValue //激活态的消息发送的方向 //EdgeDirection.Out )( //节点调用该消息将聚合后的数据和本节点进行属性的合并:求最小值 (id,v,a) =&gt; math.min(v,a), //发送消息的方法 triplet=&gt;&#123; //如果三元组的边的权重+入口的值小于目的的值，那么就发送消息，反之不发送 if(triplet.srcAttr!= Int.MaxValue &amp;&amp; triplet.srcAttr+triplet.attr&lt;triplet.dstAttr)&#123; println(s"$&#123;triplet.srcAttr&#125;--$&#123;triplet.attr&#125;--$&#123;triplet.dstAttr&#125;") Iterator((triplet.dstId,triplet.srcAttr+triplet.attr)) &#125;else&#123; //不发送消息 Iterator.empty &#125; &#125;, //最后合并，取所有消息中的最小值 (a,b) =&gt;&#123; println(s"merge:$a---$b") math.min(a,b) &#125; ) val r1 = r.triplets.collect() r1.foreach( triplet =&gt; println(s"srcId=$&#123;triplet.srcId&#125; srcAttr=$&#123;triplet.srcAttr&#125;--edge=$&#123;triplet.attr&#125;--dstId=$&#123;triplet.dstId&#125; dstAttr=$&#123;triplet.dstAttr&#125; ") ) println(r.vertices.collect.mkString("\t")) sc.stop()&#125; 5. 应用PageRank算法(网页排名，Google左侧排名，佩奇排名)]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Graphx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet的作用域和内置对象]]></title>
    <url>%2F2019%2F04%2F25%2F1_Servlet%E7%9A%84%E4%BD%9C%E7%94%A8%E5%9F%9F%E5%92%8C%E5%86%85%E7%BD%AE%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[Servlet的作用域和内置对象1. 三大作用域对象作用域(scope):作用是实现在多个web组件(Servlet/JSP)之间的数据共享Servlet三大作用域: 名称 类型 描述 request HttpServletReuqest 将数据放在请求作用域中,在一次请求中实现数据的共享,比如请求转发 session HttpSession 将数据放在当前的会话作用域中,只要浏览器不关闭,都能共享 application ServletContext 将无数据放在当前应用作用域中,应用在服务器启动的时候创建,关闭的时候销毁 操作作用域中的共享数据 1.设置共享数据 void setAttribute(String name, Object o) name:参数的名称 o:参数值2.获取共享数据 Object getAttribute(String name) :根据指定的名称获取共享数据3.修改共享数据 重新设置一个同名的共享数据4.删除共享数据 void removeAttribute(String name) ;根据指定的名称删除共享数据共享数据放在哪一个作用域中就只能在哪一个作用域中获取 2. JSP三大指令标准指令:设定JSP网页的整体配置信息特点: 并不向客户端产生任何输出， 指令在JSP整个文件范围内有效 为翻译阶段提供了全局信息指令的语法格式: &lt;%@ 指令名称 属性名=属性值 属性名=属性值%&gt;1.page 作用：定义JSP页面的各种属性 属性： language:指示JSP页面中使用脚本语言。默认值java，目前只支持java。 extends：指示JSP对应的Servlet类的父类。不要修改。 import：导入JSP中的Java脚本使用到的类或包。（如同Java中的import语句） JSP引擎自动导入以下包中的类： javax.servlet.* javax.servlet.http.* javax.servlet.jsp.* 注意：一个import属性可以导入多个包，用逗号分隔。 sessioin:指示JSP页面是否创建HttpSession对象。默认值是true，创建 buffer：指示JSP用的输出流的缓存大小.默认值是8Kb。 autoFlush：自动刷新输出流的缓存。 isThreadSafe：指示页面是否是线程安全的（过时的）。默认是true。 true：不安全的。 false：安全的。指示JSP对应的Servlet实现SingleThreadModel接口。 errorPage:指示当前页面出错后转向（转发）的页面。目标页面如果以”/“（当前应用）就是绝对路径。 配置全局错误提示页面：web.xml 12345678&lt;error-page&gt;&lt;exception-type&gt;java.lang.Exception&lt;/exception-type&gt;&lt;location&gt;/error.jsp&lt;/location&gt; &lt;/error-page&gt; &lt;error-page&gt;&lt;error-code&gt;404&lt;/error-code&gt;&lt;location&gt;/404.jsp&lt;/location&gt; &lt;/error-page&gt; isErrorPage:指示当前页面是否产生Exception对象。 contentType：指定当前页面的MIME类型。作用与Servlet中的response.setContentType()作用完全一致 pageEncoding：通知引擎读取JSP时采用的编码（因为要翻译）,还有contentType属性的作用。 isELIgnored:是否忽略EL表达式。${1+1}。默认值是false。 page指令最简单的使用方式：&lt;%@ page pageEncoding=&quot;UTF-8&quot;%&gt;2.include（静态包含,开发中能用静的不用动的） 作用：包含其他的组件。 语法：&lt;%@include file=&quot;&quot;%&gt;file指定要包含的目标组件。路径如果以&quot;/&quot;（当前应用）就是绝对路径 原理：把目标组件的内容加到源组件中，输出结果。 动态包含： 采用动作元素：&lt;jsp:include page=&quot;&quot;/&gt;路径如果以&quot;/&quot;（当前应用）就是绝对路径。3.taglib 作用：引入外部的标签 语法：&lt;%@taglib uri=&quot;标签名称空间&quot; prefix=&quot;前缀&quot;%&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot;%&gt;3. 九大内置对象 名称 类型 描述 pageContext PageContext 表示当前的JSP对象 request HttpServletRequest 表示一次请求对象 session HttpSession 表示一次会话对象,session=”true” application ServletContext 表示当前应用对象 response HttpServletResponse 表示一次响应对象 exception Throwable 表示异常对象,isErrorPage=”true” config ServletConfig 表示当前JSP的配置对象 out JspWriter 表示一个输出流对象 page Object 表示当前页面 ## 4. JSP的四大作用域 名称 类型 描述 ——– —– —- pageContext PageContext 当前的JSp对象 request HttpServletReuqest 一次请求对象 session HttpSession 一次会话对象 application ServletContext 当前应用对象 ## 5. 静态包含与动态包含 静态包含: &lt;%@include file=&quot;被包含的页面的路径&quot;%&gt; 包含的时机:在JSP文件被翻译的时候合并在一起 最终翻译得到一个java文件动态包含: &lt;jsp:include page=&quot;被包含页面的路径&quot;&gt;&lt;/jsp:include&gt; 包含的时机:在运行阶段合并代码 最终得到两个java文件动态包含和静态包含的选择: 如果被包含的页面如果是静态页面,那么使用静态包含 如果被包含的如果是动态页面,那么使用动态包含在实际开始中通常将被包含的jsp页面的后缀名设置为jspf]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet总结]]></title>
    <url>%2F2019%2F04%2F25%2F1_Servlet%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Servlet总结一. 创建Java Web项目 : 1. 开发模式切换到Java EE 2. 配置服务器 ![image_1cg0g07rjos6teo54vtlu1vqf9.png-163.3kB][1] 3. 创建Java Web项目 4. web.xml要注意勾选 二. 创建Servlet: 1. 创建一个实现Servlet接口的类或者创建一个直接继承或者间接继承HttpServlet的类 * 该类必须存在无参构造方法 * 重写doGet，doPost或者service方法 * **生命周期：** - 创建 服务器来创建，可以手动更改创建时机。可以修改`&lt;load-on-startup&gt;&lt;/load-on-startup&gt;`配置。 - 初始化 创建完对象，不需要第一次请求即可初始化。 - 执行 每一次请求都会执行service方法。 - 销毁 服务器正常关闭的时候执行destroy方法。 2. 修改配置文件web.xml * 创建servlet 12345&lt;servlet&gt; &lt;servlet-name&gt;login&lt;/servlet-name&gt; &lt;servlet-class&gt;com.zhiyou100.servlet.DemoServlet1&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; * 创建servlet-mapping（里面的servlet-name必须存在） 12345&lt;servlet-mapping&gt; &lt;servlet-name&gt;login&lt;/servlet-name&gt; &lt;!--url-pattern可以有多个，但是一个只能对应于一个Servlet-name--&gt; &lt;url-pattern&gt;/login&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 3. servlt请求 请求的地址必须在url-pattern中有一个与之对应（匹配）三. 数据共享问题1. Servlet三大作用域|作用域对象名称|类型| |--|--| |request|HttpRequest| |Session|HttpSession| |application|ServletContext| 2. Jsp的四大作用域 |作用域对象名称|类型| |--|--| |request|HttpServletRequest| |Session|HttpSession| |application|ServletContext| |page|PageContext| 3. Jsp的九大内置对象 |对象名称|类型| |--|--| |request|HttpServletRequest| |Session|HttpSession| |application|ServletContext| |page|Object| |pageContext|PageContext| |out|JSPWriter| |exception|Throwable| |config|ServletConfig| |response|HttpResponse| cookie 如何设置cookie Cookie c = new Cookie(); response.addCookie(c); 啊啊]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet]]></title>
    <url>%2F2019%2F04%2F25%2F1_Servlet%2F</url>
    <content type="text"><![CDATA[ServletServlet是什么 Servlet其实就是一个规范，也可以说就是一个Java程序，这个程序运行在服务器上，用于接收和响应客户端的请求。 创建一个Hello Servlet 创建WEB项目 ![image_1ceks61uo15vk9jql57mm6f2u9.png-110.7kB][1] 创建一个HelloServlet 实现Servlet接口 配置web.xml中的servlet及映射如下 12345678910 &lt;!-- 告诉tomcat，我这个项目有一个Servlet，名字是hello，对应的类是com.zhiyou100.servlet.HelloServlet --&gt;&lt;servlet&gt; &lt;servlet-name&gt;hello&lt;/servlet-name&gt; &lt;servlet-class&gt;com.zhiyou100.servlet.HelloServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;!-- 注册一个servlet映射，servlet-name 具体要指向哪一个servlet ，url-pattern是客户端请求资源地址--&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;hello&lt;/servlet-name&gt; &lt;url-pattern&gt;/s&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 启动 执行过程 ![image_1cel04jpj12qe19ap124g10ebr6c39.png-119.1kB][2] Servlet生命周期生命周期创建——&gt;初始化——&gt;执行程序——&gt;销毁 创建： 是由服务器进行创建，因此Servlet必须有无参构造方法，并且不能是private，默认的以及protected修饰权限 初始化： 是第一次请求时调用。但是不绝对，可以通过web.xml文件的配置提前启动。 配置如下： ![image_1celb64amfb7178u1hu71cp8q4j66.png-23.6kB][3] 执行 每次请求执行一次 销毁 服务器正常关闭时执行，执行destroy()方法，销毁Servlet对象。该方法是在Tomcat正常关闭的时候才会执行。一般情况下，不会往里面写内容。]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala基础]]></title>
    <url>%2F2019%2F04%2F25%2F1_Scala%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Scala基础1. Scala 介绍1.1 什么是 ScalaScala 是一种多范式的编程语言，其设计的初衷是要集成面向对象编程和函数式编程的各种特性 。 Scala 运行于 Java 平台 （ Java 虚拟机 ）， 并兼容现有的 Java 程序 。![image_1cm0mnjs3fvp1l2e1pca1nh11ve19.png-694.4kB][1] 1.2 为什么要学 Scala 优雅：这是框架设计师第一个要考虑的问题，框架的用户是应用开发程序员，API 是否优雅直接影响用户体验。 速度快：Scala 语言表达能力强，一行代码抵得上 Java 多行，开发速度快；Scala 是静态编译的，所以和 JRuby,Groovy 比起来速度会快很多。 能融合到 Hadoop 生态圈：Hadoop 现在是大数据事实标准，Spark 并不是要取代 Hadoop，而是要完善 Hadoop 生态。JVM 语言大部分可能会想到 Java，但 Java 做出来的 API 太丑，或者想实现一个优雅的 API 太费劲。![image_1cm0msml54s817neot313e3lpjm.png-340.4kB][2] 2. 开发环境准备2.1 安装Java2.2 安装Scala3. 基本语法3.1 cmd命令初体验3.2 声明值和变量Scala声明变量有两种方式，一个用val，一个用var。val / var 变量名 : 变量类型 = 变量值。val定义的值是不可变的，它不是一个常量，是不可变量，或称之为只读变量。 scala默认为匿名变量分配val val定义的变量虽然不能改变其引用的内存地址，但是可以改变其引用的对象的内部的其他属性值。 为了减少可变性引起的bug，应该尽可能地使用不可变变量。变量类型可以省略，解析器会根据值进行推断。val和var声明变量时都必须初始化。 3.3 数据类型Scala 和 Java 一样，有 7 种数值类型 Byte、Char、Short、Int、Long、Float 和 Double（无包装类型）和 Boolean、Unit 类型.注意: Unit 表示无值，和其他语言中 void 等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。|类型|描述||—|—||Boolean| true 或者 false||Byte |8位, 有符号||Short |16位, 有符号||Int |32位, 有符号||Long |64位, 有符号||Char |16位, 无符号||Float |32位, 单精度浮点数||Double |64位, 双精度浮点数||String| 其实就是由Char数组组成|与Java中的数据类型不同，Scala并不区分基本类型和引用类型，所以这些类型都是对象，可以调用相对应的方法。String直接使用的是java.lang.String. 不过，由于String实际是一系列Char的不可变的集合，Scala中大部分针对集合的操作，都可以用于String，具体来说，String的这些方法存在于类scala.collection.immutable.StringOps中。由于String在需要时能隐式转换为StringOps，因此不需要任何额外的转换，String就可以使用这些方法。每一种数据类型都有对应的Rich* 类型，如RichInt、RichChar等，为数值类型提供了更多的有用操作。 3.4 常用类型结构图Scala中，所有的值都是类对象，而所有的类，包括值类型，都最终继承自一个统一的根类型Any。统一类型，是Scala的又一大特点。更特别的是，Scala中还定义了几个底层类（Bottom Class），比如Null和Nothing。 Null是所有引用类型的子类型，而Nothing是所有类型的子类型。Null类只有一个实例对象，null，类似于Java中的null引用。null可以赋值给任意引用类型，但是不能赋值给值类型。 Nothing，可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于Nothing是其他任意类型的子类，他还能跟要求返回值的方法兼容。 Unit类型用来标识过程，也就是没有明确返回值的函数。 由此可见，Unit类似于Java里的void。Unit只有一个实例，()，这个实例也没有实质的意义。 ![image_1cm0p1biefv43em11s7s1tkvd1g.png-79.9kB][3] 3.5 运算符的重载+-*/%可以完成和Java中相同的工作，但是有一点区别，他们都是方法。你几乎可以用任何符号来为方法命名。注意： Scala中没有++、–操作符，需要通过+=、-=来实现同样的效果。 3.6 函数与方法在scala中，一般情况下我们不会刻意的去区分函数与方法的区别，但是他们确实是不同的。 案例1234567891011121314151617181920212223import scala.math._//_和java中*类似object TestScala01 &#123; def main(args: Array[String]): Unit = &#123; /** * 定义变量：var，val * var/val 变量名[:数据类型] = 变量值 */ val name = "张三" //name = "李四" 不能直接重新赋值 var age = 17 //定义变量的时候，也可以直接指定数据类型 var banji:String = "大数据2期" println(age) //调用一个函数，求一下平方根 var c = 5 println(sqrt(c)) &#125;&#125;3.7 字符串的格式化输出123456789101112131415//Java中的输出：正常输出println("name = "+name,"age = "+age)/* f:插值器，允许创建一个格式化的字符串，C语言里面printf %s,%d，%f等等：都是对于这个值的类型的说明 $name%s表示的是以String的变量来打印name的值 */println(f"name = $name%s age = $age%2.6f")/* s:插值器，允许在处理字符串的时候直接使用变量的值，类似于shell */println(s"name = $name age = $age")println(s"2 + 3 = $&#123;2+3&#125;")println("2 + 3 = "+(2+3)) 4. 控制结构4.1 if elsescala中没有三目运算符，因为根本不需要。scala中if else表达式是有返回值的，如果if或者else返回的类型不一样，就返回Any类型（所有类型的公共超类型）。 12345678910111213var age = 19var res = if(age &gt; 40)&#123; "年龄大于40" &#125;else&#123; "年龄小于40" &#125;println(res)var res1 = if(age&gt;40) "年龄大于40" else "年龄小于40"println(res1)var res2 = if(age&gt;40) "年龄大于40" else ageprintln(res2) 4.2 while在scala中和Java中是一样的。while的返回值为Unit类型(()) 1234567891011121314151617181920import scala.util.control.Breaksobject TestScala04_while &#123; def main(args: Array[String]): Unit = &#123; var k = 1 while(k&lt;=10)&#123; k+=1 &#125; val break = new Breaks break.breakable&#123; var i = 1 while(i&lt;10)&#123; println(i) i+=1; if(i == 5) break.break() &#125; &#125; &#125;&#125; 注意： 中断循环的方法：1. 用Boolean类型的变量来控制。2. 使用嵌套函数，使用函数中return。3. 使用Breaks对象的break方法 总结: 变量的声明 val：初始化之后不能再次被赋值 var：初始化之后可以多次被赋值 类型 Any（老祖宗），Anyval，Anyref，Null，Nothing if else 有返回值，可以返回不同类型的值 while 无返回值，如果说有返回值（Unit）也可以 break的用法 for 4.3 for{}和()对于for表达式来说都可以。for 推导式有一个不成文的约定：当for 推导式仅包含单一表达式时使用原括号，当其包含多个表达式时使用大括号。值得注意的是，使用原括号时，早前版本的Scala 要求表达式之间必须使用分号。 5. 函数基本格式def 函数名(参数名1:参数类型1，参数名2:参数类型2):返回值类型 = {函数体} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768object TestScala07_function &#123; //def 函数名(参数名1:参数类型1，参数名2:参数类型2):返回值类型 = &#123;函数体&#125; def main(args: Array[String]): Unit = &#123; f1("孙康") println(sum(1,2,3,4,5,6,7,8,9,10)) println( TestScala07_function.sum(1,2,3,4,5,6,7,8,9,10)) f6("哈哈哈哈") f6("lalal",10) println("----------------------") println(f(4)) &#125; //定义一个没有返回值的函数 def f1(str:String):Unit=&#123; println(s"哈哈哈哈，我是第一个函数$str") &#125; //定义一个隐式的没有返回值的函数 def f2(str:String)=&#123; println(s"啦啦啦啦啦，我是$str") &#125; //定义一个多返回值的函数，但是可以不写返回类型，默认为Any def f3(n:Int)=&#123; if(n&gt;10) "大于10" else n &#125; //定义一个返回特定类型的函数 def f4(score:Int):String=&#123; if(score&gt;60) "及格" else "不及格" &#125; //定义一个不含参数的函数 def f5() = &#123; println("这是一个无参的函数") &#125; //定义一个不限量参数 def sum(args:Int*) =&#123; var result = 0 for(arg &lt;- args)&#123; result+=arg &#125; result &#125; //带默认值参数的函数 def f6(str:String,length:Int=5)=&#123; println(s"$str----$length") &#125; //定义一个递归函数 1 1 2 3 5 8 13 21 34 55 斐波那契序列 // f(n) = f(n-1) + f(n -2) //递归函数：一般情况下，必须指定返回值类型。 def f(n:Int):Int=&#123; if(n&lt;1) &#123; println("输入格式有误") 0 &#125; else&#123; if(n == 1 || n ==2) 1 else f(n-1)+f(n-2) &#125; &#125;&#125;6. 过程就是返回类型为Unit的函数就是过程 7. 数据结构7.1 数组7.2 元组7.3 List7.4 Queue]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 之高级操作]]></title>
    <url>%2F2019%2F04%2F25%2F1_Scala%20%E4%B9%8B%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Scala 之高级操作面向对象类class 单例对象object 接口trait 继承与实现接口extends 抽象类abstract 2. Scala 的并发编程——Actor 注：我们现在学的Scala Actor是scala 2.10.x版本及以前版本的Actor。Scala在2.11.x版本中将Akka加入其中，作为其默认的Actor，老版本的Actor已经废弃 2.1 什么是Scala Actor概念Scala中的Actor能够实现并行编程的强大功能，它是基于事件模型的并发机制，Scala是运用消息（message）的发送、接收来实现多线程的。使用Scala能够更容易地实现多线程应用的开发。传统java并发编程与Scala Actor编程的区别![image_1cm5qspl5ag1ast1auirrifi19.png-52.4kB][1]对于Java，我们都知道它的多线程实现需要对共享资源（变量、对象等）使用synchronized 关键字进行代码块同步、对象锁互斥等等。而且，常常一大块的try…catch语句块中加上wait方法、notify方法、notifyAll方法是让人很头疼的。原因就在于Java中多数使用的是可变状态的对象资源，对这些资源进行共享来实现多线程编程的话，控制好资源竞争与防止对象状态被意外修改是非常重要的，而对象状态的不变性也是较难以保证的。 而在Scala中，我们可以通过复制不可变状态的资源（即对象，Scala中一切都是对象，连函数、方法也是）的一个副本，再基于Actor的消息发送、接收机制进行并行编程Actor方法执行顺序: 首先调用start()方法启动Actor 调用start()方法后其act()方法会被执行 向Actor发送消息 发送消息的方式|格式|描述| |—|—| |!| 发送异步消息，没有返回值。| |!? |发送同步消息，等待返回值。| |!! |发送异步消息，返回值是 Future[Any]。|2.2 案例简单案例: Scala 高阶函数样例类]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2019%2F04%2F25%2F1_Redis%2F</url>
    <content type="text"><![CDATA[Redis1. Redis 简介Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。 1.1 Redis 与 MemcachedRedis 是一个高性能的key-value数据库。 Redis的出现，很大程度补偿了memcached这类key-value数据库存储的不足，在部分场合可以对其他数据库起到很好的补充作用。它提供了Java，C/C++，C#，PHP，JavaScript，Perl，Object-C，Python，Ruby，Erlang等客户端，使用很方便 这里所说的数据库和传统意义上的数据库不太一样。传统意义上的数据库：SQL Server、MySQL、Oracle， 我们称之为关系型数据库（RDBMS）, 而Redis我们称之为NoSQL数据库。 Memcached和Redis都是主流的缓存数据库，在技术选型时，经常会拿出来做比较: Memcached 很早出现的NoSql数据库 数据都在内存中，一般不持久化 支持简单的key-value模式 多线程+锁（memcached） 一般是作为缓存数据库辅助持久化的数据库 Redis 几乎覆盖了Memcached的绝大部分功能 数据都在内存中，支持持久化，主要用作备份恢复 除了支持简单的key-value模式，还支持多种数据结构的存储，比如 list、set、hash、zset等。 单线程+多路IO复用 一般是作为缓存数据库辅助持久化的数据库 Redis是一个开源的key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，Redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是Redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 1.2 Redis 的安装下载[下载地址][1]解压1[root@master Redis]# tar -zxvf redis-4.0.10.tar.gz安装123456789101112## 先编译，后安装。编译前请确认是否安装c，c++编译器## 安装C编译器[root@master redis-4.0.10]# yum -y install gcc## 安装C++编译器[root@master redis-4.0.10]# yum -y install gcc-c++## 开始编译[root@master redis-4.0.10]# make MALLOC=libc## 开始安装[root@master redis-4.0.10]# make install## 进入到相应的bin目录[root@master redis-4.0.10]# cd /usr/local/bin/命令说明|文件|描述| |—|—| |redis-benchmark| 性能测试工具，可以在自己本子运行，看看自己本子性能如何(服务启动起来后执行)| |redis-check-aof| 修复有问题的AOF文件| |redis-check-dump| 修复有问题的dump.rdb文件| |redis-sentinel| redis集群使用| |redis-server| redis服务器启动命令| |redis-cli |客户端，操作入口|Redis启动1[root@master /]# redis-server![image_1cnp7in987t21rl41ccopr4f22p.png-143kB][2] 设置后台启动: 1234567891011121314## 备份配置文件[root@master redis-4.0.10]# mkdir /myredis[root@master redis-4.0.10]# cp redis.conf /myredis/## 修改配置文件[root@master redis-4.0.10]# cd /myredis/[root@master myredis]# vim redis.conf ## 修改daemonize no为daemonize yes[root@master myredis]# redis-server /myredis/redis.conf## 启动客户端[root@master myredis]# redis-cli## 在客户端中关闭服务端127.0.0.1:6379&gt; shutdown## 直接关闭服务端[root@master myredis]# redis-cli shutdown MySQL默认有4个数据库information_schema, performance_schema, test, mysql，用于存储自身相关的内容，如果我们想要使用，可以使用test库或根据需要创建自己的数据库。 Redis默认提供了16个库，类似数组下标从0开始，初始默认使用0号库，可以在客户端中采用命令select 来切换数据库，如：select 8默认情况下，Redis没有密码就可以访问。如果考虑安全性，也可以增加密码访问。如果设定密码了，那么所有的库的密码相同，要么都OK要么一个也连接不上。 2. 数据类型Redis的五大数据类型 string: String是Redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value String类型是二进制安全的。意味着Redis的string可以包含任何数据。比如jpg图片或者序列化的对象。 String类型是Redis最基本的数据类型，一个Redis中字符串value最多可以是512M。 list: 单键多值 Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边） 它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差 set: Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。 Redis的Set是string类型的无序集合。它底层其实是一个value为null的hash表,所以添加，删除，查找的复杂度都是O(1)。![1]( http://www.redis.cn/download.html]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL详解]]></title>
    <url>%2F2019%2F04%2F25%2F1_MySQL%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[MySQL详解1. 数据库概述1.1 数据库(DataBase:DB)：数据库是按照数据结构来组织、存储和管理数据的仓库。 存储和管理数据的仓库,其实数据库就是磁盘中的文件+一个管理系统.1.2 数据库管理系统（Database Management System:DBMS）：是专门用于管理数据库的计算机系统软件。数据库管理系统能够为数据库提供数据的定义、建立、维护、查询和统计等操作功能，并完成对数据完整性、安全性进行控制的功能。 MySQL,Oracle,SQL Server等这些都是数据管理系统/数据库服务器. 我们一般说的数据库,就是指的DBMS: 数据库服务器 数据管理员（DBA）：负责创建、监控和维护整个数据库，使数据能被任何有权使用的人有效使用。数据库管理员一般是由业务水平较高，资历较深的人员担任.1.3 数据库的发展历程1.3.1 层次数据库和网状数据库技术阶段；使用指针来表示数据之间的联系。1.3.2 关系数据库技术阶段(表格)；经典的里程碑阶段。代表DBMS:Oracle、DB2、MySQL、SQL Server、PostgreSQL等。 Oracle：大型企业数据库，支持数据量，速度较快，安全性非常好，提供完善的存储过程支持；新的版本提供了众多新功能； DB2（IBM）：大型企业数据库，支持数据量，速度较快，安全性较好； SQL Server（MS）：大型企业数据库，支持数据量，速度较快，安全性较好； MySQL（Oracle）：性能不错，使用方便，体积小，易扩展；是目前使用最广的关系型数据库； PostgreSQL（postgres）：免费,实现更完整，功能更强大,更稳定； 1.3.3 后关系数据库技术阶段；关系型数据库存在数据模型，性能，拓展伸缩性的缺点，出现了： ORDBMS：面向对象数据库技术。 NoSQL ：非关系型的数据库。随着大数据的不断发展，非关系型的数据库现在成了一个极其热门的新领域，非关系数据库产品的发展非常迅速，出色的NoSQL数据库：1.3.4 常见的NoSQL数据库分为四大类1): 键值存储数据库：Oracle BDB,Redis,BeansDB2): 列式储数数据库：HBase,Cassandra,Riak3): 文档型数据库：MongoDB,CouchDB4): 图形数据库：Neo4J,InfoGrid,Infinite Graph1.3.5 常见的关系数据库|数据库系统|所属公司||—|–| |Oracle|Oracle| |DB2|IBM |SQL Server|MS |MySQL|AB–&gt;SUN–&gt;Oracle|Oracle：运行稳定，可移植性高，功能齐全，性能超群！适用于大型企业领域。DB2：速度快、可靠性好，适于海量数据，恢复性极强。适用于大中型企业领域。SQL Server：全面，效率高，界面友好，操作容易，但是不跨平台。适用于于中小型企业领域。MySQL：开源，体积小，速度快。适用于于中小型企业领域。 2. SQL概述2.1 SQL是什么SQL：结构化查询语言(Structured Query Language)。是关系型数据库标准语言。特点：简单，灵活，功能强大。 2.2 SQL的分类2.2.1 数据查询语言（DQL）：其语句，也称为“数据检索语句”，用以从表中获得数据，确定数据怎样在应用程序给出。保留字SELECT是DQL（也是所有SQL）用得最多的动词，其他DQL常用的保留字有WHERE，ORDER BY，GROUP BY和HAVING。这些DQL保留字常与其他类型的SQL语句一起使用。2.2.2 数据操作语言（DML）：其语句包括动词INSERT，UPDATE和DELETE。它们分别用于添加，修改和删除表中的行。也称为动作查询语言。2.2.3 事务处理语言（TPL）：它的语句能确保被DML语句影响的表的所有行及时得以更新。TPL语句包括BEGIN TRANSACTION，COMMIT和ROLLBACK。2.2.4 数据控制语言（DCL）：它的语句通过GRANT或REVOKE获得许可，确定单个用户和用户组对数据库对象的访问。某些RDBMS可用GRANT或REVOKE控制对表单个列的访问。2.2.5 数据定义语言（DDL）：其语句包括动词CREATE和DROP。在数据库中创建新表或删除表（CREAT TABLE 或 DROP TABLE）；为表加入索引等。DDL包括许多与人数据库目录中获得数据有关的保留字。它也是动作查询的一部分。2.2.6 指针控制语言（CCL）：它的语句，像DECLARE CURSOR，FETCH INTO和UPDATE WHERE CURRENT用于对一个或多个表单独行的操作。2.3 SQL语法规则 在数据库中，SQL语句大小写不敏感, SELECT/select/SeLeCt SQL语句可单行或多行书写 在SQL语句中，关键字不能跨多行或缩写 为了提高可读性，一般关键字大写，其他小写. 空格和缩进使程序易读 3. 表的概念 我们说MySQL是一种关系型数据库。关系数据库最重要的概念就是表。 表具有固定的列数和任意的行数，在数学上称为“关系”。 二维表是同类实体的各种属性的集合，每个实体对应于表中的一行，在关系中称为元组，相当于通常的一条记录； 表中的列表示属性，称为Field，相当于通常记录中的一个数据项，也叫列、字段。 比如使用一张表存储学生的信息 student表|学号|姓名|年龄|班级|专业|户籍地 |—|—|—|—|—|—| |1001|张三|20|101|数学与应用数学|河南-郑州 |1002|李四|20|101|数学与应用数学|河南-郑州 |1003|王五|20|101|数学与应用数学|河南-郑州 |1004|马六|20|101|数学与应用数学|河南-郑州4. 数据库操作数据库对象存储，管理和使用数据的不同结构形式，如：表、视图、存储过程、函数、触发器、事件、索引等。数据库存储数据库对象的容器。数据库分两种 系统数据库（系统自带的数据库）：不能修改 information_schema:存储数据库对象信息，如：用户表信息，列信息，权限，字符，分区等信息。 performance_schema:存储数据库服务器性能参数信息。 mysql:存储数据库用户权限信息。 test:任何用户都可以使用的测试数据库。 用户数据库（用户自定义的数据库）：一般的，一个项目一个用户数据库。 常用的操作命令 查看数据库服务器存在哪些数据库： SHOW DATABASE;使用指定的数据库： USE database_name;查看指定的数据库中有哪些数据表: SHOW TABLES;创建指定名称的数据库： CREATE DATABASE database_name;删除数据库： DROP DATABASE database_name;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyEclipse2017破解教程]]></title>
    <url>%2F2019%2F04%2F25%2F1_MyEclipse2017%E7%A0%B4%E8%A7%A3%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[MyEclipse2017破解教程 下载MyEclipse 2017 CI 10（破解文件仅支持这个版本）官网：https://www.genuitec.com/products/myeclipse/download/官方中文网：http://www.myeclipsecn.com/download/![image_1c5at9h2i10l71sq7n711ccs4kl8m.png-143.3kB][1] 破解MyEclipse 2017 CI 10下载myeclise-2017-CI-10破解文件：链接：https://pan.baidu.com/s/1dpblFo 密码：qbu5破解步骤： 1 解压破解文件![image_1c5asfqgu8hoie239k1u52qtb1t.png-13.3kB][2] 2 打开文件夹patch![image_1c5asl6111d0g1loj5111pqp2hl44.png-8.6kB][3] 3 将里面的文件全部复制![image_1c5asi3i3r3vo53df2l101t4i2n.png-42.5kB][4] 4 找到MyEclipse安装目录下的plugins文件夹，打开![image_1c5asmjicjh116h9128r1f3g1264h.png-46.8kB][5] 5 粘贴，有同名就替换![image_1c5asohvk10dd12kb1sukad31e8u4u.png-98.9kB][6] 6 双击myeclipse2017_keygen里的crack.bat![image_1c5asr0i51tbp1o2n1ean1sc7ard5h.png-18.3kB][7] 7 a、输入Usercode: 任意字母或者数字b、选择Bluec、点击SystemId（点两次才会生成）d、点击Activee、点击菜单栏-&gt;Tools-&gt;saveProperties ![20180201112425512523.png-45.4kB][8]2.8 启动MyEclipse，help–&gt;subscription information…就能看到产品已激活。![QQ图片20180202172253.png-39.6kB][9]]]></content>
      <categories>
        <category>Java开发工具</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyEclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis]]></title>
    <url>%2F2019%2F04%2F25%2F1_Mybatis%2F</url>
    <content type="text"><![CDATA[Mybatis简介 MyBatis 是支持普通 SQL 查询,存储过程和高级映射的优秀持久层框架。MyBatis 消除 了几乎所有的 JDBC 代码和参数的手工设置以及结果集的检索。使用简单的 XML 或注解用于配置和原始映射,将接口和 Java 的 POJO(Plain Ordinary Java Object,普通的 Java 对象)映射成数据库中的记录。MyBatis的前身是iBatis，MyBatis在iBatis的基础上面，对代码结构进行了大量的重构和简化。 第一个程序 导入相关的包![image_1ch13bp7i1mgl1ie11uaqfkq139jp.png-28.6kB][1] 添加主配置文件，设置数据库连接信息 123456789101112131415161718192021222324252627 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt; &lt;configuration&gt; &lt;!-- 环境配置，可以配置多个 --&gt; &lt;environments default="development"&gt; &lt;!-- 表示一个数据库环境 --&gt; &lt;environment id="development"&gt; &lt;!-- 事务管理器：权限定名=包名+类名 type表示一个类，就是一个权限定名的别名。使用的是JDBC的一个事务管理 --&gt; &lt;transactionManager type="JDBC"/&gt; &lt;!-- 数据源配置，与Spring继承就需要被替换掉 POOLED：使用mybatis中的内置连接池 --&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="url" value="jdbc:mysql:///spring"/&gt; &lt;property name="username" value="root"/&gt; &lt;property name="password" value="123456"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;/configuration&gt; 创建一个实体类文件User.java 添加映射文件UserMapper.xml文件 1234567891011121314151617 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;!-- namespace=包名+Mapper文件的文件名 --&gt;&lt;mapper namespace="com.zhiyou100.mybatis.pojo.UserMapper"&gt; &lt;!-- insert:表示要插入的语句 com.zhiyou100.mybatis.pojo.User：调用insert方法传入的参数 keyColumn:数据库中自增字段的名字 keyProperty:自增长的ID值应该注入到哪个实体的字段中 useGeneratedKeys：标记这个标签需要使用数据库中的自增长 --&gt; &lt;insert id="save" parameterType="com.zhiyou100.mybatis.pojo.User" keyColumn="id" keyProperty="id" useGeneratedKeys="true"&gt; insert into user(name,password,phone,money) values (#&#123;name&#125;,#&#123;password&#125;,#&#123;phone&#125;,#&#123;money&#125;) &lt;/insert&gt;&lt;/mapper&gt; 在主配置文件中配置映射文件 123 &lt;mappers&gt; &lt;mapper resource="com/zhiyou100/mybatis/pojo/UserMapper.xml"/&gt;&lt;/mappers&gt; 创建一个测试类进行测试 1234567891011121314151617181920package com.zhiyou100.mybatis.test;import java.io.IOException;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;import com.zhiyou100.mybatis.pojo.User;public class TestMybatis &#123; @Test public void save() throws IOException &#123; SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(Resources.getResourceAsReader("mybatis-config.xml")); SqlSession ss = ssf.openSession(); User user = new User("zhangsan2","123456","123",12.2); ss.insert("com.zhiyou100.mybatis.pojo.UserMapper.save", user); ss.commit(); ss.close(); System.out.println(user); &#125;&#125; 细节： POOLED：对应org.apache.ibatis.datasource.pooled.PooledDataSourceFactory JDBC：对应org.apache.ibatis.transaction.jdbc.JdbcTransaction 配置日志打印，监控SQL语句创建log4j.properties文件，添加一下内容: 12345678# Global logging configurationlog4j.rootLogger=ERROR, stdout# MyBatis logging configuration...下面为namespace的包或者父包log4j.logger.com.zhiyou100.mybatis.pojo.UserMapper=TRACE# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n 更新操作 添加配置信息，在UserMapper.xml中 123 &lt;update id="update" parameterType="com.zhiyou100.mybatis.pojo.User"&gt; update user set name=#&#123;name&#125;,password=#&#123;password&#125;,phone=#&#123;phone&#125;,money=#&#123;money&#125; where id=#&#123;id&#125;&lt;/update&gt; 测试代码 1234567891011 @Testpublic void update() throws IOException &#123; SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(Resources.getResourceAsReader("mybatis-config.xml")); SqlSession ss = ssf.openSession(); User user = new User("zhangsan2","123456","123",12.2); user.setId(4); ss.update("com.zhiyou100.mybatis.pojo.UserMapper.update", user); ss.commit(); ss.close(); System.out.println(user);&#125; 查询操作 添加配置信息，在UserMapper.xml文件中 123456 &lt;!-- resultType:查询出的每一条结果封装成什么样的对象 --&gt;&lt;select id="select" parameterType="java.lang.Integer" resultType="com.zhiyou100.mybatis.pojo.User"&gt; select id,name,password,phone,money from user where id=#&#123;id&#125;&lt;/select&gt; 测试代码 12345678910 @Testpublic void select() throws IOException &#123; SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(Resources.getResourceAsReader("mybatis-config.xml")); SqlSession ss = ssf.openSession(); User user = ss.selectOne("com.zhiyou100.mybatis.pojo.UserMapper.select", 4); ss.commit(); ss.close(); System.out.println(user);&#125; 查询所有 添加配置信息，在UserMapper.xml文件中 123456 &lt;!-- resultType:查询出的每一条结果封装成什么样的对象 --&gt;&lt;select id="list" resultType="com.zhiyou100.mybatis.pojo.User"&gt; select id,name,password,phone,money from user&lt;/select&gt; 测试代码 123456789101112 @Testpublic void list() throws IOException &#123; SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(Resources.getResourceAsReader("mybatis-config.xml")); SqlSession ss = ssf.openSession(); List&lt;User&gt; list = ss.selectList("com.zhiyou100.mybatis.pojo.UserMapper.list"); ss.commit(); ss.close(); for(User u:list) &#123; System.out.println(u); &#125;&#125; 删除操作 添加配置信息，在UserMapper.xml文件中 123 &lt;delete id="delete" parameterType="java.lang.Integer"&gt; delete from user where id=#&#123;id&#125;&lt;/delete&gt; 测试代码 12345678 @Testpublic void delete() throws IOException &#123; SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(Resources.getResourceAsReader("mybatis-config.xml")); SqlSession ss = ssf.openSession(); ss.delete("com.zhiyou100.mybatis.pojo.UserMapper.delete",6); ss.commit(); ss.close();&#125; 别名细节参考官方文档 XML配置——&gt;别名|别名| 映射的类型|–|–||_byte| byte|_long| long|_short| short|_int |int|_integer| int|_double| double|_float| float|_boolean| boolean|string |String|byte |Byte|long |Long|short |Short|int |Integer|integer| Integer|double |Double|float |Float|boolean| Boolean|date |Date|decimal| BigDecimal|bigdecimal| BigDecimal|object |Object|map |Map|hashmap| HashMap|list |List|arraylist |ArrayList|collection |Collection|iterator |Iterator 注意：底层其实都是调用update方法，但是尽可能还是规范使用 配置自己的别名 在主配置文件中 1234567 &lt;typeAliases&gt; &lt;!-- type:需要定义别名的全限定名类 alias：起的别名 --&gt; &lt;typeAlias type="com.zhiyou100.mybatis.pojo.User" alias="user" /&gt;&lt;/typeAliases&gt; 使用别名 123 &lt;insert id="save" parameterType="user" keyColumn="id" keyProperty="id" useGeneratedKeys="true"&gt; insert into user(name,password,phone,money) values (#&#123;name&#125;,#&#123;password&#125;,#&#123;phone&#125;,#&#123;money&#125;)&lt;/insert&gt; 数据库信息自定义配置 数据库配置文件db.properties 主配置文件就使用占位符（类似于Spring） 测试类代码 1234567891011121314 @Testpublic void update() throws IOException &#123; Properties p = new Properties(); p.load(Resources.getResourceAsReader("db.properties")); SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(Resources.getResourceAsReader("mybatis-config.xml"),p); SqlSession ss = ssf.openSession(); User user = new User("zhangsan2","123456","123",12.2); user.setId(4); ss.update("com.zhiyou100.mybatis.pojo.UserMapper.update", user); ss.commit(); ss.close(); System.out.println(user);&#125; 方式2，直接配置主配置文件如下1&lt;properties resource="db.properties"&gt;&lt;/properties&gt; 使用Mapper接口的方式 出现的问题 使用statementID的方式存在如下问题 1.传入的statement字符串有可能会写错,写错的时候必须等到运行的时候才发现 2.传入的参数无法限定类型. 使用Mapper接口的方式 新建UserMapper接口 接口的全限定名==UserMapper.xml中的namespace 接口中方法==UserMapper.xml中标签ID 接口方法上的参数==UserMapper.xml中的parameterType 接口方法上的返回值类型==UserMapper.xml中的resultType 对应的API ：UserMapper mapper = session.getMapper(UserMapper.class); 测试代码 123456789101112131415161718192021222324 //接口 package com.zhiyou100.mybatis.pojo;//com.zhiyou100.mybatis.pojo.UserMapperimport java.util.List;public interface UserMapper &#123; void save(User user); void update(User user); void delete(int id); User select(int id); List&lt;User&gt; list();&#125;//测试方法@Test public void list() throws IOException &#123; SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(Resources.getResourceAsReader("mybatis-config.xml")); SqlSession ss = ssf.openSession(); UserMapper mapper = ss.getMapper(UserMapper.class); List&lt;User&gt; list = mapper.list(); ss.close(); for(User u:list) &#123; System.out.println(u); &#125; &#125; ResultMap映射当查询出来的字段名和对象中的属性名不一致的情况,就没办法使用resultType来默认映射(同名规则) 解决方案:使用resultMap来映射数据库中的字段到底注入到对象中什么属性中. 在Mapper文件中定义一下内容 12345678910111213 &lt;resultMap id="base_map" type="user"&gt; &lt;!-- column:查询出来的结果的列名 property：对象的属性名 jdbcType：数据库中的字段类型 javaType：实体类中的字段类型 --&gt; &lt;id column="_id" property="id" jdbcType="int" javaType="java.lang.Integer"/&gt; &lt;result column="_name" property="name"/&gt; &lt;result column="_password" property="password"/&gt; &lt;result column="_phone" property="phone"/&gt; &lt;result column="_money" property="money"/&gt;&lt;/resultMap&gt; 在查询结果中使用 123 &lt;select id="list" resultMap="base_map" &gt; select id as _id,name as _name,password as _password,phone as _phone,money as _money from user;&lt;/select&gt; 注意：resultMap与resultType只能写一个 数据库连接池的配置https://blog.csdn.net/tianxiezuomaikong/article/details/68942638]]></content>
      <categories>
        <category>Java框架</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux第二天]]></title>
    <url>%2F2019%2F04%2F25%2F1_Linux%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[Linux第二天常用命令gzipGNU zip 作用: 压缩（解压）文件，压缩文件的后缀名为.gz 语法: gzip [-d] 文件 特点: 只能压缩文件，不能压缩目录，不保留原文件 案例:1234567891011121314151617 [root@localhost test]# lsservices[root@localhost test]# gzip services [root@localhost test]# lsservices.gz[root@localhost test]# mkdir aaa[root@localhost test]# lsaaa services.gz[root@localhost test]# gzip aaagzip: aaa is a directory -- ignored[root@localhost test]# gzip -d services.gz [root@localhost test]# lsaaa services[root@localhost test]# gzip services [root@localhost test]# gunzip services.gz [root@localhost test]# lsaaa services bzip2: 作用: 压缩（解压）文件，压缩后的文件后缀为.bz2 语法: bzp2 [-kd] [文件]- -k: 产生压缩文件后保留原文件 - -d: 解压缩 案例:1234567891011121314 [root@localhost test]# lsaaa services[root@localhost test]# bzip2 services [root@localhost test]# lsaaa services.bz2[root@localhost test]# bzip2 -d services.bz2 [root@localhost test]# lsaaa services[root@localhost test]# bzip2 -k services [root@localhost test]# lsaaa services services.bz2[root@localhost test]# bzip2 aaabzip2: Input file aaa is a directory.[root@localhost test]# bunzip2 services.bz2 zip: 作用: 压缩（解压）文件，压缩文件的后缀为.zip 语法: zip [-r] [压缩后的文件名] [文件或目录]- -r: 压缩目录的时候需要使用 案例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@localhost test]# zip services zip error: Nothing to do! (services.zip)[root@localhost test]# lsaaa services[root@localhost test]# zip services.zip services adding: services (deflated 80%)[root@localhost test]# lsaaa services services.zip[root@localhost test]# zip -r services.zip zip error: Nothing to do! (services.zip)[root@localhost test]# zip -r services.zip se zip warning: name not matched: sezip error: Nothing to do! (try: zip -r services.zip . -i se)[root@localhost test]# unzip services.zip Archive: services.zipreplace services? [y]es, [n]o, [A]ll, [N]one, [r]ename: rnew name: hahaha inflating: hahaha [root@localhost test]# lsaaa hahaha services services.zip[root@localhost test]# unzip services.zip seArchive: services.zipcaution: filename not matched: se[root@localhost test]# cd ..[root@localhost opt]# lsrh test vmware-tools-distrib[root@localhost opt]# zip test.zip test adding: test/ (stored 0%)[root@localhost opt]# ll总用量 4drwxr-xr-x. 2 root root 6 3月 26 2015 rhdrwxr-xr-x. 3 root root 67 7月 24 10:44 test-rw-r--r--. 1 root root 160 7月 24 10:46 test.zipdrwxr-xr-x. 9 root root 145 3月 22 17:10 vmware-tools-distrib[root@localhost opt]# zip -r test.zip testupdating: test/ (stored 0%) adding: test/aaa/ (stored 0%) adding: test/services (deflated 80%) adding: test/services.zip (stored 0%) adding: test/hahaha (deflated 80%)[root@localhost opt]# ll总用量 400drwxr-xr-x. 2 root root 6 3月 26 2015 rhdrwxr-xr-x. 3 root root 67 7月 24 10:44 test-rw-r--r--. 1 root root 409121 7月 24 10:46 test.zipdrwxr-xr-x. 9 root root 145 3月 22 17:10 vmware-tools-distrib tar: 作用: 文件、目录打包，解包 语法: tar [-zxvfcj] [压缩后的文件名] [文件或目录]- -z：使用gzip命令进行压缩或解压 - -c：将文件或目录进行打包，后缀是.tar - -x：解包（extract） - -j：使用bzip2命令压缩或解压 - -v：显示解压或压缩的过程 - -f：指定文件名，必须一个选项 案例:1234567891011121314151617181920212223242526272829303132333435363738394041424344 [root@localhost test]# cd aaa[root@localhost aaa]# lshahaha services services.zip[root@localhost aaa]# cd ..[root@localhost test]# tar -cf aa.tar aaa[root@localhost test]# lsaaa aa.tar hahaha services services.zip[root@localhost test]# tar -xf aa.tar [root@localhost test]# lsaaa aa.tar hahaha services services.zip[root@localhost test]# rm -rf aaa[root@localhost test]# lsaa.tar hahaha services services.zip[root@localhost test]# tar -xf aa.tar [root@localhost test]# lsaaa aa.tar hahaha services services.zip[root@localhost test]# gzip aa.tar[root@localhost test]# lsaaa aa.tar.gz hahaha services services.zip[root@localhost test]# tar -zcvf aaa.tar.gz aaaaaa/aaa/hahahaaaa/servicesaaa/services.zip[root@localhost test]# lsaaa aaa.tar.gz aa.tar.gz hahaha services services.zip[root@localhost test]# rm -rf aaa[root@localhost test]# lsaaa.tar.gz aa.tar.gz hahaha services services.zip[root@localhost test]# tar -zxvf aaa.tar.gz aaa/aaa/hahahaaaa/servicesaaa/services.zip[root@localhost test]# lsaaa aaa.tar.gz aa.tar.gz hahaha services services.zip[root@localhost test]# tar -zxvf aaa.tar.gz -C /usr/aaa/aaa/hahahaaaa/servicesaaa/services.zip[root@localhost test]# cd /usr/[root@localhost usr]# lsaaa bin etc games include lib lib64 libexec local sbin share src tmp shutdown: 作用: 系统关机的命令 语法: shutdown [-chr] 时间- -c: 表示取消操作 - -h：关机 - -r：重启 案例:12345678910## 现在立刻马上关机[root@localhost ~]# shutdown -h now## 在指定的时间关机[root@localhost ~]# shutdown -h 10:40[root@localhost ~]# halt[root@localhost ~]# poweroff[root@localhost ~]# init 0##重启[root@localhost ~]# reboot[root@localhost ~]# init 6 常用快捷键操作 : ctrl + c 结束当前进程 Ctrl + z 挂起当前进程，放后台 Ctrl + r 查看命令历史（history） 方向键 上，下：查看执行过的命令 Ctrl + l 清屏（clear） vim编辑器vim工作模式![image_1cj5605pd1pk8122bkd617mi1p09.png-41.6kB][1] 命令模式：一般模式 编辑模式：底行模式，命令行模式 插入模式：文件编辑模式插入命令|命令名|作用| |—|—| |i|在光标之前插入文本| |a|在光标之后插入文本| |I（shift+i）|在文本的开始插入文本，行首| |A（shift+a）|在文本的结尾插入文本，行末| |o|在光标的下方插入新行| |O（shift+o）|在光标所处行的上方插入新行|保存和退出|命令|作用| |—|—| |:w|保存修改，但是不退出| |:w newFileName|另存为指定文件| |:w &gt;&gt; 文件名|将本文件中的内容追加到其他文件中去，，其他文件必须存在| |:wq|保存并退出| |:q!|不保存并退出| |:q|直接退出，但是如果修改了会有提示| |:wq!|保存修改并退出，可以忽略文件只读属性|定位命令|命令名|作用| |—|—| |:set nu|设置并显示行号| |:set nonu|取消显示行号| |gg|直接回到第一行| |G（shift+g）|到最后一行| |nG|到第n行| |:n|定位到第n行|删除命令|命令名|作用| |—|—| |x|删除光标所在位置的字符| |nx|删除从光标位置开始计算的后面n个字符| |dd|删除光标所在行| |ndd|删除光标所在行以及光标后面的n-1行| |:n1,n2d|删除指定范围的行，:5,9d 表示删除5,6,7,8,9这几行| |dG|删除光标所在行到最后一行| |D|删除从光标位置到行尾| 复制和剪切的命令 : |命令名|作用| |—|—| |yy,y,Y|复制当前行| |p|粘贴，粘贴到光标所在行的下方| |P|粘贴，粘贴到光标所在行的上方| |nyy|复制当前行以及以下n-1行| |dd|剪切当前行| |ndd|剪切当前行及以下行共n行| 替换和取消的命令 : |命令名|作用| |—|—| |r|替换光标位置的字符| |R|从光标位置开始替换，直到esc结束| |u|取消上一步操作| |Ctrl+r|返回到新的状态，直到最新| 替换和搜索的命令%表示全文，g表示的全局替换，s表示的开始，c表示替换要询问 |命令名|作用| |—|—| |/字符串|向后搜索指定的字符串| |?字符串|向前搜索指定的字符串| |n|搜索字符串的下一个出现的位置，与搜索顺序相同| |N|搜索字符串的上一个出现的位置，与搜索顺序相反| |:%s/老字符串/新字符串/g|| |:n1,n2s/老字符串/新字符串/g|在指定范围内替换指定字符串| 案例 12:11155,11175s/^/#aaaaaa/g:r !pwd可视化字符模式|命令名|作用| |—|—| |v|字符视图模式| |V|行视图视图模式|Linux高级设置IP地址的修改: 可视化修改 setup启动虚拟界面进行配置（7以下，不包含7） 修改配置文件1234567#编辑文件[root@localhost test]# vim/etc/sysconfig/network-scripts/ifcfg-ens33#重启网卡服务，但是是centos6的模式，7中已经取消[root@localhost test]# service network restart#重启网卡服务，这个在7中可以用，6中不识别[root@localhost test]# systemctl restart network ping: 作用：测试网络的连通性 语法: ping [-c] IP地址 ifconfig: 作用: 显示网卡的配置信息 语法: ifconfig [-a] [网卡设备的名称] 案例:123[root@localhost ~]# ifconfig[root@localhost ~]# ifconfig -a[root@localhost ~]# ifconfig -a ens33 netstat: 作用: 用于检测主机的网络配置状况 语法: netstat [-atunl]- -a: 显示所有连接的端口信息 - -t：仅显示tcp通讯相关信息 - -u：仅显示ucp通讯相关信息 - -n: 使用数字方式显示地址和端口 - -l: 显示监听中的服务的socket 案例：1234[root@localhost ~]# netstat[root@localhost ~]# netstat -tuln[root@localhost ~]# netstat -ua[root@localhost ~]# netstat -ta 进程管理命令看PPT 用户管理useradd: 语法: useradd [选项] 用户名 passwd: 语法：passwd [用户名] userdel: 语法: userdel [-r] 用户名- -r：删除账号的时候，删除宿主目录 磁盘空间命令df -h:易读方式显示 -M:以MB方式显示 -k: du 查看文件或目录的大小du [-ahsb] [文件名，目录] free 权限相关![image_1cj5qjudmvu81qfuq5p1let1dhjm.png-60.2kB][2] d: 这个位置表示文件的类型：d，l，- 接下来的3位 rwx：这三位表示当前用户对该文件（目录的权限） r： 表示读-4，w表示写-2，x表示执行-1 接下来的3位 r-x：这三位表示当前用户所属组对该文件的权限 最后3位 r-x：这个表示其他用户的对该文件的权限 修改文件的权限chmod 777 文件名或目录名chmod 666 文件名或目录名]]></content>
      <categories>
        <category>Linux学习</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux第三天-Shell编程]]></title>
    <url>%2F2019%2F04%2F25%2F1_Linux%E7%AC%AC%E4%B8%89%E5%A4%A9-Shell%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Linux第三天-Shell编程什么是ShellShell是命令解释器(command interpreter)，是Unix操作系统的用户接口，程序从用户接口得到输入信息，shell将用户程序及其输入翻译成操作系统内核（kernel）能够识别的指令，并且操作系统内核执行完将返回的输出通过shell再呈现给用户，下图所示用户、shell和操作系统的关系：![image_1cj6aap5313rs62n1ba91e3u1qefm.png-33.6kB][1]Shell也是一门编程语言，即shell脚本，shell是解释执行的脚本语言，可直接调用linux命令。类似于JS一个系统可以存在多个shell，可以通过cat /etc/shells命令查看系统中安装的shell，不同的shell可能支持的命令语法是不相同的 123456789101112131415[root@localhost ~]# cat /etc/shells/bin/sh#常用/bin/bash#伪用户使用/sbin/nologin/usr/bin/sh/usr/bin/bash/usr/sbin/nologin/bin/tcsh/bin/csh#在ll的结果冲查找sh[root@localhost ~]# ll /bin/ | grep sh#通过下面的操作可以看出sh也是bashlrwxrwxrwx. 1 root root 4 7月 23 19:29 sh -&gt; bash Shell的分类操作系统内核（kernel）与shell是独立的套件，而且都可被替换：不同的操作系统使用不同的shell；同一个kernel之上可以使用不同的shell。常见的shell分为两大主流： sh（Linux常用）Bourne shell（sh）Solaris,hpux默认shellBourne again shell（bash） ,Linux系统默认shellcsh （Unix常用）C shell(csh)tc shell(tcsh)查看当前系统使用的shell12[root@localhost ~]# echo $SHELL/bin/bashShell中的环境定义临时环境变量所谓临时变量是指在用户在当前登陆环境生效的变量，用户登陆系统后，直接在命令行上定义的环境变量便只能在当前的登陆环境中使用。当退出系统后，环境变量将不能下次登陆时继续使用。 12345[root@localhost ~]# echo $name[root@localhost ~]# name=zhangsan[root@localhost ~]# echo $namezhangsan设置永久环境变量通过将环境变量定义写入到配置文件中，用户每次登陆时系统自动定义，则无需再到命令行重新定义。定义环境变量的常见配置文件如下： /etc/profile针对系统所有用户生效，此文件应用于所有用户每次登陆系统时的环境变量定义 用户目录/.bash_profile针对特定用户生效，当用户登陆系统后，首先继承/etc/profile文件中的定义，再应用用户目录/.bash_profile文件中的定义。 系统预定义的环境变量系统环境变量对所有用户有效，如：$PATH、$HOME、$SHELL、$PWD等等，如下用echo命令打印上述的系统环境变量： 12345678[root@localhost ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@localhost ~]# echo $HOME/root[root@localhost ~]# echo $SHELL/bin/bash[root@localhost ~]# echo $PWD/root创建Shell脚本一个shell脚本通常包含如下部分 首行 第一行内容在脚本的首行左侧，表示脚本将要调用的shell解释器，内容如下 1#!/bin/bash #！符号能够被内核识别成是一个脚本的开始，这一行必须位于脚本的首行，/bin/bash是bash程序的绝对路径，在这里表示后续的内容将通过bash程序解释执行。 注释注释符号# 放在需注释内容的前面，如下 1#这是我的第一个shell程序 内容可执行内容和shell结构 第一个Shell程序 12345#!/bin/bash#这是我的第一个shell程序echo "你好，世界" echo "你好，小张" 执行这个脚本 123456[root@localhost test]# ./HelloWord.sh-bash: ./HelloWord.sh: 权限不够[root@localhost test]# chmod 777 HelloWord.sh [root@localhost test]# ./HelloWord.sh你好，世界你好，小张 执行脚本的三种方式 输入脚本的绝对路径或相对路径 1./HelloWord.sh 这种情况下会新开一个shell环境 bash或sh +脚本 1bash HelloWord.sh 注：当脚本没有x权限时，root和文件所有者通过该方式可以正常执行。这种情况下不会新开一个shell环境，以下代码可以证明 在脚本的路径前再加”. “ 或source 12source HelloWord.sh. ./HelloWord.sh 123456789101112131415[root@localhost test]# echo $namezhangsan[root@localhost test]# echo "echo \$name" &gt;&gt; HelloWord.sh [root@localhost test]# ./HelloWord.sh 你好，世界你好，小张[root@localhost test]# bash HelloWord.sh 你好，世界你好，小张[root@localhost test]# . HelloWord.sh 你好，世界你好，小张zhangsan Shell变量变量：是shell传递数据的一种方式，用来代表每个取值的符号名。当shell脚本需要保存一些信息时，如一个文件名或是一个数字，就把它存放在一个变量中。 命名规范: 变量名称可以由字母，数字和下划线组成，但是不能以数字开头，环境变量名建议大写，便于区分。 在bash中，变量的默认类型都是字符串型，如果要进行数值运算，则必须指定变量类型为数值型。 变量用等号连接值，等号左右两侧不能有空格。 变量的值如果有空格，需要使用单引号或者双引号包括。123456789 [root@localhost test]# n1=1 [root@localhost test]# n2=2 [root@localhost test]# n=$n1+$n2 [root@localhost test]# echo $n1+2[root@localhost test]# expr 1 + 23[root@localhost test]# n3 = $n1+$n2 bash: n3: 未找到命令... 变量分类Linux Shell中的变量分为用户自定义变量,环境变量，位置参数变量和预定义变量。可以通过set命令查看系统中存在的所有变量 系统变量：保存和系统操作环境相关的数据。$HOME、$PWD、$SHELL、$USER等等 位置参数变量：主要用来向脚本中传递参数或数据，变量名不能自定义，变量作用固定。 预定义变量：是Bash中已经定义好的变量，变量名不能自定义，变量作用也是固定的。 自定义变量 用户自定义变量用户自定义的变量由字母或下划线开头，由字母，数字或下划线序列组成，并且大小写字母意义不同，变量名长度没有限制。设置变量习惯上用大写字母来命名变量。变量名以字母表示的字符开头，不能用数字。变量调用在使用变量时，要在变量名前加上前缀$. 使用echo 命令查看变量值。echo $A变量赋值: 定义时赋值变量＝值等号两侧不能有空格 1[root@localhost test]# AGE=18 将一个命令的执行结果赋给变量 1234567891011 [root@localhost test]# lsHelloWord.sh[root@localhost test]# lsvalue=`ls`[root@localhost test]# echo $lsvalue HelloWord.sh[root@localhost test]# lsvalue=`ll`[root@localhost test]# echo $lsvalue 总用量 4 -rwxrwxrwx. 1 root root 104 7月 24 23:04 HelloWord.sh[root@localhost test]# lsvalue=$(ll)[root@localhost test]# echo $lsvalue 总用量 4 -rwxrwxrwx. 1 root root 104 7月 24 23:04 HelloWord.sh 反引号，运行里面的命令，并把结果返回给变量lsvalue 将一个变量赋给另一个变量 123456789101112131415 [root@localhost test]# test=hahaha [root@localhost test]# echo $testhahaha[root@localhost test]# newtest=lalalatest[root@localhost test]# echo $newtest lalalatest [root@localhost test]# newtest=lalala$test[root@localhost test]# echo $newtest lalalahahaha [root@localhost test]# newtest=lalala$&#123;test&#125;[root@localhost test]# echo $newtest lalalahahaha [root@localhost test]# newtest=lalala"$test"[root@localhost test]# echo $newtest lalalahahaha 单引号和双引号的区别： 现象：单引号里的内容会全部输出，而双引号里的内容会有变化 原因：单引号会将所有特殊字符脱意 123456 [root@localhost test]# newtest=lalala'$test'[root@localhost test]# echo $newtestlalala$test[root@localhost test]# newtest=lalala"$test"[root@localhost test]# echo $newtestlalalahahaha 删除变量12 [root@localhost test]# unset newtest[root@localhost test]# echo $newtest注意: readonly修饰的变量不能删除用户自定义的变量，作用域为当前的shell环境。 1234[root@localhost test]# readonlytest=12[root@localhost test]# unset test-bash: unset: test: 无法反设定: 只读 variable 环境变量用户自定义变量只在当前的shell中生效，而环境变量会在当前shell和其所有子shell中生效。如果把环境变量写入相应的配置文件，那么这个环境变量就会在所有的shell中生效。export 变量名=变量值 申明变量作用域：当前shell以及所有的子shell1234567891011[root@localhost test]# nn=1[root@localhost test]# export nnn=2[root@localhost test]# echo $nn1[root@localhost test]# echo $nnn2[root@localhost test]# bash[root@localhost test]# echo $nn[root@localhost test]# echo $nnn2 位置参数变量|变量|描述||—|—||$n|n为数字，$0代表命令本身，$1-$9代表第一到第9个参数,十以上的参数需要用大括号包含，如${10}。||$*|代表命令行中所有的参数，把所有的参数看成一个整体。以$1 $2 … $n的形式输出所有参数|$@| 代表命令行中的所有参数，把每个参数区分对待。以&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot; 的形式输出所有参数||$#|代表命令行中所有参数的个数。添加到shell的参数个数|123456789101112131415#!/bin/bashecho '$0='$0echo "$1="$1echo "\$2="$2echo "\$*="$@echo "\$@="$@echo "\$#="$#[root@localhost test]# ./haha.sh zhang 12 11 13 15$0=./loc.shzhang=zhang$2=12$*=zhang 12 11 13 15$@=zhang 12 11 13 15$#=5 **`$*` 和 `$@`的区别** `$*` 和 `$@` 都表示传递给函数或脚本的所有参数，不被双引号&quot; &quot;包含时，都以`&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot;` 的形式输出所有参数 当它们被双引号&quot; &quot;包含时，`&quot;$*&quot;` 会将所有的参数作为一个整体，以`&quot;$1 $2 … $n&quot;`的形式输出所有参数；`&quot;$@&quot;` 会将各个参数分开，以`&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot;` 的形式输出所有参数 代码如下 12345678910111213141516171819202122232425262728293031#!/bin/bashecho '$0='$0echo "$1="$1echo "\$2="$2echo "\$*="$@echo "\$@="$@echo "\$#="$#echo "这是\$*的结果——不加引号"for i in $*doecho $idoneecho "这是\$@的结果"for i in $@doecho $idoneecho "这是\$*的结果——加双引号"for i in "$*"doecho $idoneecho "这是\$@的结果——加双引号"for i in "$@"doecho $idone![image_1cj6ghfa98pi1ka1a1uet4b69.png-90.4kB][2] ![image_1cj6giaut1e6g15sv1kp9oe1hrjm.png-4kB][3] shift指令：参数左移，每执行一次，参数序列顺次左移一个位置，$# 的值减1，用于分别处理每个参数，移出去的参数不再可用 123456789101112#!/bin/bashecho '$0='$0echo "$1="$1echo "\$2="$2echo "\$*="$@echo "\$@="$@echo "\$#="$#shiftecho "\$1="$1echo "\$#="$# ![image_1cj6h0kl81hrfr691qpr16dk1ijs13.png-36.7kB][4] 预定义变量|命令|描述| |—|—| |$?| 执行上一个命令的返回值 执行成功，返回0，执行失败，返回非0（具体数字由命令决定）| |$$ |当前进程的进程号（PID），即当前脚本执行时生成的进程号| |$! |后台运行的最后一个进程的进程号（PID），最近一个被放入后台执行的进程 &amp;| 12345#!/bin/bashpwdecho "\$$="$$ls /etc &gt; out.log &amp;echo "\$!="$!![image_1cj6hjpkptlv1bopueml4eik91g.png-19.8kB][5] 12345678 [root@localhost test]# pwd/usr/test[root@localhost test]# echo $?0[root@localhost test]# cdcbash: cdc: 未找到命令...[root@localhost test]# echo $?127read命令read [选项] 值read -p(提示语句) -n(字符个数) -t(等待时间，单位为秒) –s(隐藏输入) 123#!/bin/bashread -t 10 -p "请输入你的名字" nameecho $name![image_1cj6i78t31ir31knt3laoo1q2o1t.png-17.8kB][6] 123#!/bin/bashread -t 10 -sp "请输入你的名字" nameecho $name ![image_1cj6i8toqv571deqa35are1i302q.png-16.9kB][7] 123 #!/bin/bash read -t 10 -n 1 -p "请输入你的性别[m,w]" sexecho $sex运算符格式 :expr m + n 或$((m+n)) 注意expr运算符间要有空格expr命令：对整数型变量进行算术运算 12345678910111213141516 [root@localhost test]# vi read.sh [root@localhost test]# vi read.sh [root@localhost test]# n1=1 [root@localhost test]# n2=2 [root@localhost test]# num=n1+n2 [root@localhost test]# echo $numn1+n2[root@localhost test]# num=$n1+$n2[root@localhost test]# echo $num 1+2[root@localhost test]# num=`expr $n1 + $n2` [root@localhost test]# echo $num3[root@localhost test]# num=$(($n1 + $n2)) [root@localhost test]# echo $num 3测试加减乘除$()与${}的区别$( )的用途和反引号 ``一样，用来表示优先执行的命令${ } 就是取变量了$((运算内容)) 适用于数值运算条件测试: 内置test命令内置test命令常用操作符号[]表示，将表达式写在[]中，如下：[ expression ]或者：test expression注意： expression首尾都有个空格测试范围：整数、字符串、文件表达式的结果为真，则test的返回值为0，否则为非0。当表达式的结果为真时，则变量$?的值就为0，否则为非0 字符串测试test str1 == str2 测试字符串是否相等 =test str1 != str2 测试字符串是否不相等test str1 测试字符串是否不为空,不为空为true反之为falsetest -n str1 测试字符串是否不为空test -z str1 测试字符串是否为空![image_1cj6jrggo15d21tg510tu1kqi18v837.png-78.5kB][8]； 命令连接符号&amp;&amp; 逻辑与 条件满足，才执行后面的语句[ -z “$name” ] &amp;&amp; echo invalid || echo ok|| 逻辑或，条件不满足，才执行后面的语句test “$name” == ”yangmi” &amp;&amp; echo ok || echo invalid 整数测试test int1 -eq int2 测试整数是否相等 equalstest int1 -ge int2 测试int1是否&gt;=int2test int1 -gt int2 测试int1是否&gt;int2test int1 -le int2 测试int1是否&lt;=int2test int1 -lt int2 测试int1是否&lt;int2test int1 -ne int2 测试整数是否不相等 文件测试test -d file 指定文件是否目录test –e file 文件是否存在 existstest -f file 指定文件是否常规文件test –L File 文件存在并且是一个符号链接test -r file 指定文件是否可读test -w file 指定文件是否可写test -x file 指定文件是否可执行 多重条件测试条件1 –a 条件2 逻辑与 两个都成立，则为真条件1 –o 条件2 逻辑或 只要有一个为真，则为真！ 条件 逻辑非 取反1234[root@localhost test]# [ "11" == "11" -a "11" == "13" ] &amp;&amp; echo "成功" || echo "失败"失败[root@localhost test]# [ "11" == "11" -a "11" == "11" ] &amp;&amp; echo "成.." || echo "失败"成功 流程控制语句if123456789101112#!/bin/bashread -p "请输出一个名字" nameecho $nameif [ "$name" == root ]then echo "你好，管理员"elif [ "$name" == hadoop ];then echo "hadoop是我的最爱"else echo "我的名字是"$namefi![image_1cj6krps21ut7d0n1s8t1n9n183444.png-83.3kB][9]casecase命令是一个多分支的if/else命令，case变量的值用来匹配value1,value2,value3等等。匹配到后则执行跟在后面的命令直到遇到双分号为止(;;)case命令以esac作为终止符。 123456789101112131415#!/bin/bashcase $1 instart) echo "启动一个程序" ;;restart) echo "重启一个程序" ;;stop) echo "停止一个程序" ;;*) echo "命令有误"esac![image_1cj6lc8n51e6n1bne1dscli81tbe4h.png-56.3kB][10]for循环1234 [root@localhost test]# for i in 1 2 3 4 5;do echo $i;done[root@localhost test]# for i in &#123;1,2,3,4,5&#125;;do echo $i;done [root@localhost test]# for i in &#123;1..5&#125;;do echo $i;done[root@localhost test]# for i in &#123;2..5&#125;;do echo $i;done123456789101112 #i/bin/bash for((i=1;i&lt;10;i+=2)) do echo $i done sum=0 for((i=1;i&lt;=100;i++)) do sum=$(($sum + $i))doneecho $sum ![image_1cj6lrkaoaabip04os1slig1b4u.png-15.8kB][11]while循环12345678910#!/bin/bashsum=0i=0while((i&lt;=100))do sum=$(($sum + $i)) i=$(($i + 1))doneecho $sum![image_1cj6m3vvv17q814ev6ghhuu174f5b.png-10.9kB][12] 123456789sum=0i=0while [ $i -le 100 ]do sum=$(($sum + $i)) i=$(($i + 1))doneecho $sum自定义函数函数代表着一个或一组命令的集合，表示一个功能模块，常用于模块化编程。以下是关于函数的一些重要说明： 在shell中，函数必须先定义，再调用 使用return value来获取函数的返回值 函数在当前shell中执行，可以使用脚本中的变量。格式:123456789#!/bin/bashfunction start()&#123;echo "启动一个程序"&#125;stop()&#123;echo "停止一个程序"&#125;start 传参 12345678910#!/bin/bashfunction start()&#123;echo "启动一个程序"echo $#&#125;stop()&#123;echo "停止一个程序"&#125;start $* 带返回值 123456789101112#!/bin/bashfunction start()&#123;echo "启动一个程序"echo $#return $?&#125;stop()&#123;echo "停止一个程序"&#125;a=`start $*`echo "\$a"$a 注意：如果函数名后没有（），在函数名和{ 之间，必须要有空格以示区分。函数返回值，只能通过$? 系统变量获得，可以显示加：return 返回值，如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值n(0-255) 脚本调试: bash -x script这将执行该脚本并显示所有变量的值。 bash -n script不执行脚本只是检查语法的模式，将返回所有语法错误。 bash –v script执行并显示脚本内容 在shell脚本里添加set -x 对部分脚本调试 awk和sedcut: cut [选项] 文件名 默认分割符是制表符 选项 -f 列号： 提取第几列 -d 分隔符： 按照指定分隔符分割列 cut -f 2 aa.txt 提取第二列cut -d “:” -f 1,3 /etc/passwd 以:分割，提取第1和第3列cat /etc/passwd | grep /bin/bash | grep -v root | cut -d “:” -f 1 获取所有可登陆的普通用户用户名cut的局限性 不能分割空格 df -h 不能使用cut分割df -h | grep sda1 | cut -f 5]]></content>
      <categories>
        <category>Linux学习</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA经典算法设计题]]></title>
    <url>%2F2019%2F04%2F25%2F1_JAVA%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E9%A2%98%2F</url>
    <content type="text"><![CDATA[JAVA经典算法设计题【程序1】题目：古典问题：有一对兔子（刚出生的），从出生后第3个月起每个月都生一对兔子，小兔子长到第三个月后每个月又生一对兔子，假如兔子都不死，问每个月的兔子总数为多少？ 程序分析：兔子的规律为数列1,1,2,3,5,8,13,21….其实就是斐波那契序列，使用递归法 1234567891011121314151617181920212223242526package com.zhiyou100;import java.util.Scanner;/** * 先寻找规律【得出为斐波那契序列的结论】 * 1：T1 * 2：T1 * 3：T1 T2(T1所生) * 4：T1 T2 T3(T1所生) * 5：T1 T2 T3 T4(T1所生) T5(T2所生) * @author zhang */public class Demo01 &#123; public static void main(String[] args) &#123; System.out.println("请输入要查询第几个月的兔子数量"); Scanner sc = new Scanner(System.in); int n = sc.nextInt(); System.out.println("第" + n + "个月的兔子数量为" + f(n) + "对"); sc.close(); &#125; public static int f(int n) &#123; if (n == 1 || n == 2) return 1; else return f(n - 1) + f(n - 2); &#125;&#125; 【程序2】题目：找出[100,300)之间的所有素数，并统计数量。 程序分析：判断素数的方法：用这个数分别去除以2到其一半，如果存在一个数能被整除，则表明这个数不是素数，反之是素数。 123456789101112131415161718192021package com.zhiyou100;public class Demo02 &#123; public static void main(String[] args) &#123; int sum = 0; for (int i = 100; i &lt; 300; i++) &#123; boolean isPrime = true;// 是素数则为true，否则为false for (int j = 2; j &lt; i / 2; j++) &#123; if (i % j == 0) &#123; isPrime = false; break; &#125; &#125; if (isPrime) &#123; System.out.print(i + "\t"); sum++; &#125; &#125; System.out.println(); System.out.println(sum); &#125;&#125; 【程序3】题目：打印出所有的水仙花数，所谓水仙花数是指一个三位数，其各位数字立方和等于该数本身。例如：153是一个水仙花数，因为$153=1^3+5^3+3^3$。 程序分析：使用for循环遍历100-999个数，计算每个数的个位，十位，百位。 123456789101112package com.zhiyou100;public class Demo03 &#123; public static void main(String[] args) &#123; for (int i = 100; i &lt; 1000; i++) &#123; int bw = i / 100; int sw = i % 100 / 10; int gw = i % 10; if (i == (bw * bw * bw + sw * sw * sw + gw * gw * gw)) System.out.println(i); &#125; &#125;&#125; 【程序4】题目：将一个正整数分解质因数。例如：在控制台输入10,就打印出10=2*5。 程序分析：对n进行分解质因数，应先找到一个最小的质数i，然后按下述步骤完成：使用递归法 (1) 如果这个质数恰等于n，则说明分解质因数的过程已经结束，打印出即可。 (2) 如果n &gt; i，但n能被i整除，则应打印出i的值，并用n除以i的商,作为新的正整数,重复执行第一步。 (3) 如果n不能被i整除，则用i+1作为i的值,重复执行第一步。 123456789101112131415161718192021222324package com.zhiyou100;import java.util.Scanner;public class Demo04 &#123; public static void main(String[] args) &#123; System.out.println("请输入一个整数："); Scanner sc = new Scanner(System.in); int n = sc.nextInt(); System.out.print(n+"="); resolve(n); sc.close(); &#125; public static void resolve(int n) &#123; for(int i = 2;i&lt;n+1;i++) &#123; if(n%i == 0) &#123; System.out.print(i); if(i!=n) &#123; System.out.print("*"); resolve(n/i); &#125; return; &#125; &#125; &#125;&#125; 【程序5】题目：使用三元运算符的嵌套来完成此题：学习成绩&gt;=90分的同学则输出优秀，[75,90)分之间的输出良好，[60,75)分之间的输出及格，低于60分的输出不及格。 程序分析：其实使用的就是(a&gt;b)?a:b三元运算符 1234567891011package com.zhiyou100;import java.util.Scanner;public class Demo05 &#123; public static void main(String[] args) &#123; System.out.println("请输入成绩值："); Scanner sc = new Scanner(System.in); int n = sc.nextInt(); System.out.println(n &gt;= 90 ?"优秀": (n &gt;= 75 ?"良好": (n &gt;= 60 ? "及格":"不及格"))); sc.close(); &#125;&#125; 【程序6】题目：通过控制台输入两个正整数m和n，求其最大公约数和最小公倍数。 程序分析：多种方法，第一种辗除法，第二种倒序检验法 方法一：辗除法 123456789101112131415161718192021package com.zhiyou100;import java.util.Scanner;public class Demo06_1 &#123; public static void main(String[] args) &#123; Scanner sc=new Scanner(System.in); System.out.println("请输入第一个正整数："); int m = sc.nextInt(); System.out.println("请输入第二个正整数："); int n = sc.nextInt(); System.out.println(m+"和"+n+"的最大公约数为："+gcd(m, n)); System.out.println(m+"和"+n+"的最小公倍数为："+(m*n/gcd(m, n))); sc.close(); &#125; public static int gcd(int m,int n) &#123; if(m%n==0) &#123; return n; &#125;else &#123; return gcd(n,m%n); &#125; &#125;&#125; 方法二：倒序检验法 1234567891011121314151617181920212223package com.zhiyou100;import java.util.Scanner;public class Demo06_2 &#123; public static void main(String[] args) &#123; Scanner sc=new Scanner(System.in); System.out.println("请输入第一个正整数："); int m = sc.nextInt(); System.out.println("请输入第二个正整数："); int n = sc.nextInt(); System.out.println(m+"和"+n+"的最大公约数为："+gcd(m, n)); System.out.println(m+"和"+n+"的最小公倍数为："+(m*n/gcd(m, n))); sc.close(); &#125; public static int gcd(int m,int n) &#123; int min = m&gt;n?n:m; int k = min; do &#123; if(m%k==0&amp;&amp;n%k==0) return k; k--; &#125;while(k&lt;min); return k;//这个无所谓，返回什么都行，因为前面最终必然会有返回值 &#125;&#125; 【程序7】题目：通过控制台输入一行字符串，分别统计出英文字母、空格、数字和其它字符的个数。 程序分析：只需要将该字符串转变为字符，根据字符的整数形式进行比较即可 12345678910111213141516171819202122232425262728293031package com.zhiyou100;import java.util.Scanner;public class Demo07 &#123; public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); System.out.println("请输入一个字符串："); String str = sc.nextLine(); count(str); sc.close(); &#125; public static void count(String str) &#123; int character = 0, digital = 0, blank = 0, other = 0; char[] cs = str.toCharArray(); for (int i = 0; i &lt; str.length(); i++) &#123; if (cs[i] &gt;= 'a' &amp;&amp; cs[i] &lt;= 'z' || cs[i] &gt;= 'A' &amp;&amp; cs[i] &lt;= 'Z') character++; else if (cs[i] &gt;= '0' &amp;&amp; cs[i] &lt;= '9') digital++; else if (cs[i] == ' ') blank++; else other++; &#125; System.out.println("英文字母的个数：" + character); System.out.println(" 数字的个数：" + digital); System.out.println(" 空格的个数：" + blank); System.out.println("其他字符的个数：" + other); &#125;&#125; 【程序8】题目：求s = a + aa + aaa + aaaa + aa…a的值，其中a是一个数字。例如2 + 22 + 222 + 2222 + 22222(此时共有5个数相加)，几个数相加有键盘控制。 程序分析：关键是计算出每一项的值。 1234567891011121314151617import java.util.Scanner;public class Demo08 &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); System.out.println(请输入a的值); int a = in.nextInt(); System.out.println(请输入n个数); int n = in.nextInt(); int s = 0,t=0; for (int i = 1; i &lt;= n; i++) &#123; t += a; a = a*10; s += t; &#125; System.out.println(s); &#125;&#125; 【程序9】题目：一个数如果恰好等于它的因子之和，这个数就称为”完数”。例如6=1＋2＋3。编程找出1000以内的所有完数。 1234567891011121314package com.zhiyou100;public class Demo09 &#123; public static void main(String[] args) &#123; int sum; for (int i = 1; i &lt; 1001; i++) &#123; sum = 0; for (int j = 1; j &lt; i; j++) if (i % j == 0) sum = sum + j; if (sum == i) System.out.print(i + " " ); &#125; &#125;&#125; 【程序10】 题目：一球从100米高度自由落下，每次落地后反跳回原高度的一半；再落下，求它在第10次落地时，共经过多少路程？第10次反弹多高？ 程序分析：注意这中间会有小数，故要使用double类型 1234567891011121314package com.zhiyou100;public class Demo10 &#123; public static void main(String[] args) &#123; double s = 0; double h = 100; for (int i = 1; i &lt; 11; i++) &#123; s += h;//正常落下 h = h/2; s += h;//反弹上 &#125; System.out.println("经过路程："+s); System.out.println("反弹高度："+h); &#125;&#125; 【程序11】题目：计算字符串中子串出现的次数。 1234567891011121314151617181920212223242526import java.util.Scanner;public class Demo46 &#123; public static void main(String[] args) &#123; Scanner in=new Scanner(System.in); System.out.println("请输入主串："); String str1 = in.nextLine(); System.out.println("请输入子串："); String str2 = in.nextLine(); // 生成子串长度的N个字符串数组 String[] sa = new String[str1.length() - str2.length() + 1]; for (int i = 0; i &lt; sa.length; i++) &#123; sa[i] = str1.substring(i, i + str2.length()); &#125; int sum = 0; // 子串与N个拆开的子串比对 for (int i = 0; i &lt; sa.length; i++) &#123; if (sa[i].equals(str2)) &#123; // 成功配对，计数器+1； sum++; // 因为不计算重叠的子串，所以跳过配对之后的部分拆分子串 i = i + str2.length(); &#125; &#125; System.out.println("主串中共包含" + sum + "个字串"); &#125;&#125; 【程序12】题目：企业发放的奖金根据利润提成。利润(I)低于或等于10万元时，奖金可提10%；利润高于10万元，低于20万元时，低于10万元的部分按10%提成，高于10万元的部分，可提成7.5%；20万到40万之间时，高于20万元的部分，可提成5%；40万到60万之间时高于40万元的部分，可提成3%；60万到100万之间时，高于60万元的部分，可提成1.5%，高于100万元时，超过100万元的部分按1%提成，从键盘输入当月利润lirun，求应发放奖金总数sum？ 程序分析：请利用数轴来分界，定位。注意定义时需把奖金定义成长整型。 1234567891011121314151617181920212223import java.util.Scanner;public class Demo12 &#123; public static void main(String[] args) &#123; double sum; System.out.println("输入当月利润：(万元)"); Scanner in = new Scanner(System.in); double lirun = in.nextDouble(); if (lirun &lt;= 10) &#123; sum = lirun * 0.1; &#125; else if (lirun &lt;= 20) &#123; sum = 10*0.1 + (lirun - 10) * 0.075; &#125; else if (lirun &lt;= 40) &#123; sum = 10*0.1 + 10*0.075 + (lirun - 20) * 0.05; &#125; else if (lirun &lt;= 60) &#123; sum = 10*0.1 + 10*0.075 + 10*0.05 + (lirun - 40) * 0.03; &#125; else if (lirun &lt;= 100) &#123; sum = 10*0.1 + 10*0.075 + 10*0.05 + 10*0.03 + (lirun - 60) * 0.015; &#125; else &#123; sum = 10*0.1 + 10*0.075 + 10*0.05 + 10*0.03 + 10*0.015 + (lirun - 100) * 0.01; &#125; System.out.println("应发的奖金是："+sum+"(万元)"); &#125;&#125; 【程序13】题目：一个整数，它加上100后是一个完全平方数，加上168又是一个完全平方数，请问该数是多少？ 程序分析：在10万以内判断，先将该数加上100后再开方，再将该数加上168后再开方，如果开方后的结果满足如下条件，即是结果。请看具体分析： 123456789public class Demo13 &#123; public static void main(String[] args) &#123; for(int x=1;x&lt;100000;x++)&#123; if(Math.sqrt(x+100)%1==0) if(Math.sqrt(x+100+168)%1==0) System.out.println(x+"加上100后是一个完全平方数，加上168又是一个完全平方数"); &#125; &#125;&#125; 【程序14】题目：输入某年某月某日，判断这一天是这一年的第几天？ 程序分析：以3月5日为例，应该先把前两个月的加起来，然后再加上5天即本月的第几天，特殊情况，闰年且输入月份大于3时需考虑多加一天。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129import java.util.Calendar;import java.util.Scanner;public class Demo14 &#123; public static void main(String[] args) &#123; System.out.println("请输入年,月,日："); Scanner in = new Scanner(System.in); int year = in.nextInt(); int month = in.nextInt(); int day = in.nextInt(); Calendar cal = Calendar.getInstance(); cal.set(year, month - 1, day); int sum = cal.get(Calendar.DAY_OF_YEAR); System.out.println("这一天是这一年的第" + sum +"天"); &#125;&#125;或import java.util.*;public class Demo14 &#123; public static void main(String[] args)&#123; int year,month,day,sum=0; Scanner in=new Scanner(System.in); System.out.println("输入年："); year=in.nextInt(); System.out.println("输入月："); month=in.nextInt(); System.out.println("输入日："); day=in.nextInt(); switch(month)&#123; case 1: sum=0; break; case 2: sum=31; break; case 3: sum=59; break; case 4: sum=90; break; case 5: sum=120; break; case 6: sum=151; break; case 7: sum=181; break; case 8: sum=212; break; case 9: sum=243; break; case 10: sum=273; break; case 11: sum=304; break; case 12: sum=334; break; default: System.out.println("wrong input!"); return; &#125; sum=sum+day; boolean leap; if(year%400==0||(year%4==0&amp;&amp;year%100!=0))&#123; leap=true; &#125;else &#123; leap=false; &#125; if(leap&amp;&amp;month&gt;2)&#123; sum++; &#125; System.out.println("It is the "+sum+"th day."); &#125;&#125;或import java.util.Scanner;public class Demo14 &#123; public static void main(String[] args)&#123; System.out.println("请输入年 月 日："); Scanner in=new Scanner(System.in); int year=in.nextInt(); int month=in.nextInt(); int day=in.nextInt(); System.out.println("是该年的第"+count(year,month,day)+"天"); &#125; public static int count(int year,int month,int day)&#123; int sum=0; int days=0; for(int i=1;i&lt;month;i++)&#123; switch(i)&#123; case 1: case 3: case 5: case 7: case 8: case 10: case 12: days=31; break; case 4: case 6: case 9: case 11: days=30; break; case 2: if(year%400==0||year%4==0&amp;&amp;year%100!=0)&#123; days=29; &#125;else&#123; days=28; &#125; break; &#125; sum+=days; &#125; sum+=day; return sum; &#125;&#125; 【程序15】题目：输入三个整数x,y,z，请把这三个数由小到大输出。 程序分析：我们想办法把最小的数放到x上，先将x与y进行比较，如果x&gt;y则将x与y的值进行交换，然后再用x与z进行比较，如果x&gt;z则将x与z的值进行交换，这样能使x最小。 123456789101112131415161718import java.util.Arrays;import java.util.Scanner;public class Demo15 &#123; public static void main(String[] args) &#123; System.out.print("请输入三个数:"); Scanner in = new Scanner(System.in); int[] arr = new int[3]; for (int i = 0; i &lt; 3; i++) &#123; arr[i] = in.nextInt(); &#125; Arrays.sort(arr); for (int i=0;i&lt;arr.length;i++) &#123; System.out.print(arr[i] + " "); &#125; &#125;&#125;或if(x &gt; y) &#123; int t = x; x = y; y = t; &#125; if(x &gt; z) &#123; int t = x; x = z; z = t; &#125; if(y &gt; z) &#123; int t = y; y = z; z = t; &#125; 【程序16】题目：输出9*9口诀乘法表。 程序分析：分行与列考虑，共9行9列，i控制行，j控制列。 1234567891011121314151617181920//出现重复的乘积（全矩形）public class Demo16 &#123; public static void main(String[] args) &#123; for (int i = 1; i &lt;= 9; i++) &#123; for (int j = 1; j &lt;= 9; j++) System.out.print(i + "*" + j + "=" + (i*j) + "\t"); System.out.println(); &#125; &#125;&#125;//不现重复的乘积(下三角)public class Demo16 &#123; public static void main(String[] args) &#123; for (int i = 1; i &lt;= 9; i++) &#123; for (int j = 1; j &lt;= i; j++) System.out.print(i + "*" + j + "=" + (i*j) + "\t"); System.out.println(); &#125; &#125;&#125; 【程序17】题目：猴子吃桃问题：猴子第一天摘下若干个桃子，当即吃了一半，还不瘾，又多吃了一个第二天早上又将剩下的桃子吃掉一半，又多吃了一个。以后每天早上都吃了前一天剩下的一半零一个。到第10天早上想再吃时，见只剩下一个桃子了。求第一天共摘了多少。 程序分析：采取逆向思维的方法，从后往前推断。 123456789public class Demo17 &#123; public static void main(String[] args) &#123; int sum = 1; for (int i = 0; i &lt; 9; i++) &#123; sum = (sum + 1) * 2; &#125; System.out.println("第一天共摘"+sum); &#125;&#125; 【程序18】题目：两个乒乓球队进行比赛，各出三人。甲队为a,b,c三人，乙队为x,y,z三人。已抽签决定比赛名单。有人向队员打听比赛的名单。a说他不和x比，c说他不和x,z比，请编程序找出三队赛手的名单。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Demo18 &#123; static char[] m = &#123; 'a', 'b', 'c' &#125;; static char[] n = &#123; 'x', 'y', 'z' &#125;; public static void main(String[] args) &#123; for (int i = 0; i &lt; m.length; i++) &#123; for (int j = 0; j &lt; n.length; j++) &#123; if (m[i] == 'a' &amp;&amp; n[j] == 'x') &#123; continue; &#125; else if (m[i] == 'a' &amp;&amp; n[j] == 'y') &#123; continue; &#125; else if ((m[i] == 'c' &amp;&amp; n[j] == 'x') || (m[i] == 'c' &amp;&amp; n[j] == 'z')) &#123; continue; &#125; else if ((m[i] == 'b' &amp;&amp; n[j] == 'z') || (m[i] == 'b' &amp;&amp; n[j] == 'y')) &#123; continue; &#125; else System.out.println(m[i] + " vs " + n[j]); &#125; &#125; &#125;&#125;或public class Demo18 &#123; public String a, b, c; public Demo18(String a, String b, String c) &#123; this.a = a; this.b = b; this.c = c; &#125; public static void main(String[] args) &#123; Demo18 arr_a = new Demo18("a", "b", "c"); String[] b = &#123; "x", "y", "z" &#125;; for (int i = 0; i &lt; 3; i++) &#123; for (int j = 0; j &lt; 3; j++) &#123; for (int k = 0; k &lt; 3; k++) &#123; Demo18 arr_b = new Demo18(b[i], b[j], b[k]); if (!arr_b.a.equals(arr_b.b) &amp; !arr_b.b.equals(arr_b.c) &amp; !arr_b.c.equals(arr_b.a) &amp; !arr_b.a.equals("x") &amp; !arr_b.c.equals("x") &amp; !arr_b.c.equals("z")) &#123; System.out.println(arr_a.a + "--" + arr_b.a); System.out.println(arr_a.b + "--" + arr_b.b); System.out.println(arr_a.c + "--" + arr_b.c); &#125; &#125; &#125; &#125; &#125;&#125; 【程序19】题目：打印出如下图案（菱形） 程序分析：先把图形分成两部分来看待，前四行一个规律，后三行一个规律，利用双重for循环，第一层控制行，第二层控制列。 123456789101112131415161718192021222324三角形：****************************public class Demo19 &#123; public static void main(String[] args) &#123; int i=0; int j=0; for ( i = 1; i &lt;= 4; i++) &#123; for ( j = 1; j &lt;= 2 * i - 1; j++) System.out.print("*"); System.out.println(); &#125; for ( i = 3; i &gt;= 1; i--) &#123; for ( j = 1; j &lt;= 2 * i - 1; j++) System.out.print("*"); System.out.println(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728菱形： * *** ************ ***** *** *public class Demo19 &#123; public static void main(String[] args) &#123; int i = 0; int j = 0; for (i = 1; i &lt;= 4; i++) &#123; for (int k = 1; k &lt;= 4 - i; k++) System.out.print( " " ); for (j = 1; j &lt;= 2 * i - 1; j++) System.out.print("*"); System.out.println(); &#125; for (i = 3; i &gt;= 1; i--) &#123; for (int k = 1; k &lt;= 4 - i; k++) System.out.print( " " ); for (j = 1; j &lt;= 2 * i - 1; j++) System.out.print("*"); System.out.println(); &#125; &#125;&#125; 【程序20】 题目：有一分数序列：2/1，3/2，5/3，8/5，13/8，21/13…求出这个数列的前20项之和。 程序分析：请抓住分子与分母的变化规律。 12345678910111213141516public class Demo20 &#123; public static void main(String[] args) &#123; float fm = 1.0f; float fz = 1.0f; float temp; float sum = 0f; for (int i = 0; i &lt; 20; i++) &#123; temp = fm; fm = fz; fz = fz + temp; System.out.println((int) fz + "/" + (int) fm); sum += fz / fm; &#125; System.out.println(sum); &#125;&#125; 【程序21】题目：求1+2!+3!+…+20!的和。 程序分析：此程序只是把累加变成了累乘。 1234567891011public class Demo21 &#123; public static void main(String[] args) &#123; long sum = 0; long fac = 1; for (int i = 1; i &lt;= 20; i++) &#123; fac = fac * i; sum += fac; &#125; System.out.println(sum); &#125;&#125; 【程序22】题目：利用递归方法求5!。 程序分析：递归公式：f(n)=f(n-1)*4! 123456789101112131415161718import java.util.Scanner;public class Demo22 &#123; public static long fac(int n) &#123; long value = 0; if (n == 1 || n == 0) &#123; value = 1; &#125; else if (n &gt; 1) &#123; value = n * fac(n - 1); &#125; return value; &#125; public static void main(String[] args) &#123; System.out.println("请输入一个数："); Scanner in = new Scanner(System.in); int n = in.nextInt(); System.out.println(n + "的阶乘为：" + fac(n)); &#125;&#125; 【程序23】题目：有5个人坐在一起，问第五个人多少岁？他说比第4个人大2岁。问第4个人岁数，他说比第3个人大2岁。问第三个人，又说比第2人大两岁。问第2个人，说比第一个人大两岁。最后问第一个人，他说是10岁。请问第五个人多大？ 程序分析：利用递归的方法，递归分为回推和递推两个阶段。要想知道第五个人岁数，需知道第四人的岁数，依次类推，推到第一人（10岁），再往回推。 12345678910111213141516171819202122//直接求解：public class Demo23 &#123; public static void main(String[] args) &#123; int n = 10; for (int i = 0; i &lt; 4; i++) &#123; n = n + 2; &#125; System.out.println("第五个人" + n + "岁"); &#125;&#125;//递归求解：public class Demo23 &#123; public static int getAge(int n) &#123; if (n == 1) &#123; return 10; &#125; return 2 + getAge(n - 1); &#125; public static void main(String[] args) &#123; System.out.println("第五个的年龄为" + getAge(5)); &#125;&#125; 【程序24】题目：给一个不多于5位的正整数，要求：一、求它是几位数，二、逆序打印出各位数字。 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.Scanner;public class Demo24 &#123; public static void main(String[] args) &#123; Demo24 use = new Demo24(); System.out.println("请输入："); Scanner in = new Scanner(System.in); long a = in.nextLong(); if (a &lt; 0 || a &gt;= 100000) &#123; System.out.println("Error Input, please run this program Again!"); System.exit(0); &#125; if (a &gt;= 0 &amp;&amp; a &lt;= 9) &#123; System.out.println(a + "是一位数"); System.out.println("按逆序输出是:" + a); &#125; else if (a &gt;= 10 &amp;&amp; a &lt;= 99) &#123; System.out.println(a + "是二位数"); System.out.println("按逆序输出是:"); use.converse(a); &#125; else if (a &gt;= 100 &amp;&amp; a &lt;= 999) &#123; System.out.println(a + "是三位数"); System.out.println("按逆序输出是:"); use.converse(a); &#125; else if (a &gt;= 1000 &amp;&amp; a &lt;= 9999) &#123; System.out.println(a + "是四位数"); System.out.println("按逆序输出是:"); use.converse(a); &#125; else if (a &gt;= 10000 &amp;&amp; a &lt;= 99999) &#123; System.out.println(a + "是五位数"); System.out.println("按逆序输出是:"); use.converse(a); &#125; &#125; public void converse(long l) &#123; String s = Long.toString(l); char[] ch = s.toCharArray(); for (int i = ch.length - 1; i &gt;= 0; i--) &#123; System.out.print(ch[i]); &#125; &#125;&#125; 【程序25】题目：一个5位数，判断它是不是回文数。即12321是回文数，个位与万位相同，十位与千位相同。 123456789101112131415161718192021222324252627import java.util.Scanner;public class Demo25 &#123; public static void main(String[] args) &#123; System.out.println("请输入："); Scanner in = new Scanner(System.in); String str = in.next(); int l = Integer.parseInt(str);//转换成整数 if (l &lt; 10000 || l &gt; 99999) &#123; System.out.println("输入错误！"); System.exit(0); &#125; boolean is=false; char[] ch = str.toCharArray(); for(int i=0;i&lt;ch.length/2;i++)&#123; if(ch[i]!=ch[ch.length-i-1])&#123; is=false; &#125;else&#123; is=true; &#125; &#125; if(is)&#123; System.out.println("这是一个回文!"); &#125;else&#123; System.out.println("不是一个回文!"); &#125; &#125;&#125; 【程序26】 题目：请输入星期几的第一个字母来判断一下是星期几，如果第一个字母一样，则继续判断第二个字母。 程序分析：用情况语句比较好，如果第一个字母一样，则判断用情况语句或if语句判断第二个字母。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import java.util.Scanner;public class Demo26 &#123; public static void main(String[] args) &#123; char weekSecond;//保存第二字母 Scanner in = new Scanner(System.in);//接收用户输入 System.out.println("请输入星期的第一个字母："); String letter = in.next(); if (letter.length() == 1) &#123;//判断用户控制台输入字符串长度是否是一个字母 char weekFirst = letter.charAt(0);//取第一个字符 switch (weekFirst) &#123; case 'm': case 'M': System.out.println("星期一(Monday)"); break; case 't': case 'T': System.out.print("由于星期二(Tuesday)与星期四(Thursday)均以字母T开头，故需输入第二个字母才能正确判断："); letter = in.next(); if (letter.length() == 1) &#123; weekSecond = letter.charAt(0); if (weekSecond == 'U' || weekSecond == 'u') &#123; System.out.println("星期二(Tuesday)"); break; &#125; else if (weekSecond == 'H' || weekSecond == 'h') &#123; System.out.println("星期四(Thursday)"); break; &#125; else &#123; System.out.println("Error!"); break; &#125; &#125; else &#123; System.out.println("输入错误，只能输入一个字母，程序结束！"); break; &#125; case 'w': case 'W': System.out.println("星期三(Wednesday)"); break; case 'f': case 'F': System.out.println("星期五(Friday)"); break; case 's': case 'S': System.out.print("由于星期六(Saturday)与星期日(Sunday)均以字母S开头，故需输入第二个字母才能正确判断："); letter = in.next(); if (letter.length() == 1) &#123; weekSecond = letter.charAt(0); if (weekSecond == 'A' || weekSecond == 'a') &#123; System.out.println("星期六(Saturday)"); break; &#125; else if (weekSecond == 'U' || weekSecond == 'u') &#123; System.out.println("星期日(Sunday)"); break; &#125; else &#123; System.out.println("Error!"); break; &#125; &#125; else &#123; System.out.println("输入错误，只能输入一个字母，程序结束！"); break; &#125; default: System.out.println("输入错误，不能识别的星期值第一个字母，程序结束！"); break; &#125; &#125; else &#123; System.out.println("输入错误，只能输入一个字母，程序结束！"); &#125; &#125;&#125; 【程序27】 题目：求100之内的素数 12345678910111213public class Demo27 &#123; public static void main(String args[]) &#123; int sum, i; for (sum = 2; sum &lt;= 100; sum++) &#123; for (i = 2; i &lt;= sum / 2; i++) &#123; if (sum % i == 0) break; &#125; if (i &gt; sum / 2) System.out.println(sum + "是素数"); &#125; &#125;&#125; 或 12345678910111213public class Demo27&#123; public static void main(String args[])&#123; int w=1; for(int i=2;i&lt;=100;i++)&#123; for(int j=2;j&lt;i;j++)&#123; w=i%j; if(w==0)break; &#125; if(w!=0) System.out.println(i+"是素数"); &#125; &#125;&#125; 【程序28】题目：对10个数进行排序。 程序分析：可以利用选择法，即从后9个比较过程中，选择一个最小的与第一个元素交换，下次类推，即用第二个元素与后8个进行比较，并进行交换。 123456789101112131415161718192021222324本例代码为生成随机10个数排序，并输入1个数，插入重排序输出：import java.util.Arrays;import java.util.Random;import java.util.Scanner;public class Demo28 &#123; public static void main(String[] args) &#123; int arr[] = new int[11]; Random r = new Random(); for (int i = 0; i &lt; 10; i++) &#123; arr[i] = r.nextInt(100) + 1; //得到10个100以内的整数 &#125; Arrays.sort(arr); for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i] +"\t"); &#125; System.out.print("\nPlease Input a int number:" ); Scanner in = new Scanner(System.in); arr[10] = in.nextInt(); Arrays.sort(arr); for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i] +"\t"); &#125; &#125;&#125; 【程序29】题目：求一个3*3矩阵主对角线元素之和。 程序分析：利用双重for循环控制输入二维数组，再将a[i][i]累加后输出。 12345678910111213141516171819public class Demo29 &#123; public static void main(String[] args) &#123; double sum = 0; int array[][] = &#123; &#123; 1, 2, 3 &#125;, &#123; 4, 5, 6 &#125;, &#123; 7, 7, 8 &#125; &#125;; for (int i = 0; i &lt; 3; i++) for (int j = 0; j &lt; 3; j++) &#123; if (i == j) sum = sum + array[i][j]; &#125; System.out.println(sum); &#125;&#125;主负对角线： for(i=0;i&lt;n;i++) for(j=0;j&lt;n;j++)&#123; if(i==j) sum1+=a[i][j]; if(i+j==n-1) sum2+=a[i][j]; &#125; &#125; 【程序30】题目：有一个已经排好序的数组。现输入一个数，要求按原来的规律将它插入数组中。 程序分析：首先判断此数是否大于最后一个数，然后再考虑插入中间的数的情况，插入后此元素之后的数，依次后移一个位置。 123456789101112131415161718192021222324252627282930313233import java.util.Random;public class Demo30 &#123; public static void main(String[] args) &#123; int temp = 0; int arr[] = new int[12]; Random r = new Random(); for (int i = 0; i &lt;= 10; i++) arr[i] = r.nextInt(1000); for (int i = 0; i &lt;= 10; i++) System.out.print(arr[i] + "\t"); for (int i = 0; i &lt;= 9; i++) for (int k = i + 1; k &lt;= 10; k++) if (arr[i] &gt; arr[k]) &#123; temp = arr[i]; arr[i] = arr[k]; arr[k] = temp; &#125; System.out.println(); for (int k = 0; k &lt;= 10; k++) System.out.print(arr[k] + "\t"); arr[11] = r.nextInt(1000); for (int k = 0; k &lt;= 10; k++) if (arr[k] &gt; arr[11]) &#123; temp = arr[11]; for (int j = 11; j &gt;= k + 1; j--) arr[j] = arr[j - 1]; arr[k] = temp; &#125; System.out.println(); for (int k = 0; k &lt;= 11; k++) System.out.print(arr[k] + "\t"); &#125;&#125; 【程序31】题目：将一个数组逆序输出。 程序分析：用第一个与最后一个交换。 123456789用逆序循环控制变量输出：public class Demo31 &#123; public static void main(String[] args) &#123; int[] a = &#123; 1, 2, 3, 4, 5, 6, 7, 8, 9, 0 &#125;; for (int i = a.length - 1; i &gt;= 0; i--) &#123; System.out.print(a[i] + " "); &#125; &#125;&#125; 【程序32】题目：取一个整数a从右端开始的第4～7位数字。 123456789101112131415import java.util.*;public class Demo32 &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); System.out.print("请输入一个7位以上的正整数："); long l = in.nextLong(); String str = Long.toString(l); char[] ch = str.toCharArray(); int j=ch.length; if (j&lt;7)&#123;System.out.println("输入错误！"); &#125; else &#123; System.out.println("截取从右端开始的4～7位是："+ch[j-7]+ch[j-6]+ch[j-5]+ch[j-4]); &#125; &#125;&#125; 或 12345678910import java.util.Scanner;public class Demo32&#123; public static void main(String[] args) &#123; int a = 0; Scanner s = new Scanner(System.in); long b = s.nextLong(); a = (int) (b % 10000000 / 1000); System.out.println(a); &#125;&#125; 【程序33】题目：打印出杨辉三角形（要求打印出10行如下图） 123456789101112131415161718192021222324252627282911 11 2 11 3 3 11 4 6 4 11 5 10 10 5 1public class Demo33 &#123; public static void main(String args[]) &#123; int i, j; int a[][]; int n = 10; a = new int[n][n]; for (i = 0; i &lt; n; i++) &#123; a[i][i] = 1; a[i][0] = 1; &#125; for (i = 2; i &lt; n; i++) &#123; for (j = 1; j &lt;= i - 1; j++) &#123; a[i][j] = a[i - 1][j - 1] + a[i - 1][j]; &#125; &#125; for (i = 0; i &lt; n; i++) &#123; for (j = 0; j &lt;= i; j++) &#123; System.out.printf(a[i][j] + "\t"); &#125; System.out.println(); &#125; &#125;&#125; 【程序34】 题目：输入数组，最大的与第一个元素交换，最小的与最后一个元素交换，输出数组。 1234567891011121314151617181920212223242526272829303132333435import java.util.*;public class Demo34 &#123; public static void main(String[] args) &#123; int i, min=0, max=0, n, temp1, temp2; int a[]; System.out.println("定义数组的长度:"); Scanner in = new Scanner(System.in); n = in.nextInt(); a = new int[n]; for (i = 0; i &lt; n; i++) &#123; System.out.print("输入第" + (i + 1) + "个数据:"); a[i] = in.nextInt(); &#125; for (i = 1; i &lt; n; i++) &#123; if (a[i] &gt; a[max]) max = i; if (a[i] &lt; a[min]) min = i; &#125; temp1 = a[0]; a[0] = a[max]; a[max] = temp1; temp2 = a[min]; if (min != 0) &#123; // 如果最小值不是a[0]，执行下面 a[min] = a[n - 1]; a[n - 1] = temp2; &#125; else &#123; //如果最小值是a[0],执行下面 a[max] = a[n - 1]; a[n - 1] = temp1; &#125; for (i = 0; i &lt; n; i++) &#123; System.out.print(a[i] + " " ); &#125; &#125;&#125; 【程序35】题目：有n个整数，使其前面各数顺序向后移m个位置，最后m个数变成最前面的m个数 1234567891011121314151617181920212223242526272829import java.util.LinkedList;import java.util.List;import java.util.Scanner;public class Demo35 &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); System.out.println("输入数字个数n："); int n = in.nextInt(); System.out.println("输入后移位数m："); int m = in.nextInt(); LinkedList&lt;Integer&gt; list = new LinkedList&lt;Integer&gt;(); for (int i = 0; i &lt; n; i++) &#123; System.out.println("请输入第"+(i+1)+"个数:"); list.add(in.nextInt()); &#125; System.out.println("原数据排序为："); for (int t : list) &#123; System.out.print(t + " " ); &#125; System.out.println(); List&lt;Integer&gt; temp1 = list.subList(list.size() - m, list.size()); List&lt;Integer&gt; temp2 = list.subList(0, list.size() - m); temp2.addAll(0, temp1); System.out.println("移动后排序为;"); for (int t : temp2) &#123; System.out.print(t + " " ); &#125; &#125;&#125; 或 12345678910111213141516171819202122232425262728293031323334353637import java.util.*;public class Demo35&#123; public static void main(String[] args)&#123; Scanner in=new Scanner(System.in); System.out.println("请定义数组的长度："); int n=in.nextInt(); System.out.println("请输入移动的位数："); int m=in.nextInt(); int [] arr=new int [n]; int [] brr=new int [n]; for(int i=0;i&lt;n;i++)&#123; System.out.println("请输入第"+(i+1)+"个数："); arr[i]=in.nextInt(); &#125; System.out.println("排序前："); for(int i=0;i&lt;n;i++)&#123; System.out.print(arr[i]+" "); &#125; System.out.println(); for(int i=0;i&lt;m;i++)&#123; brr[i]=arr[n-m+i]; &#125; for(int i=0;i&lt;n-m;i++)&#123; arr[m+i]=arr[i]; &#125; for(int i=0;i&lt;m;i++)&#123; arr[i]=brr[i]; &#125; System.out.println("排序后："); for(int i=0;i&lt;n;i++)&#123; System.out.print(arr[i]+" "); &#125; &#125;&#125; 【程序36】题目：有n个人围成一圈，顺序排号。从第一个人开始报数（从1到3报数），凡报到3的人退出圈子，问最后留下的是原来第几号的那位。（约瑟夫环问题，百度百科有时间复杂度最简单的数学方法）原例代码： 12345678910111213141516171819202122232425262728293031323334import java.util.Scanner;public class Demo36 &#123; public static void main(String[] args) &#123; System.out.println("请输人数n："); Scanner in = new Scanner(System.in); int n = in.nextInt(); boolean[] arr = new boolean[n]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = true; //下标为TRUE时说明还在圈里 &#125; int leftCount = n; int countNum = 0; int index = 0; while (leftCount &gt; 1) &#123; if (arr[index] == true) &#123; //当在圈里时 countNum++; //报数递加 if (countNum == 3) &#123; //报数为3时 countNum = 0; //从零开始继续报数 arr[index] = false; //此人退出圈子 leftCount--; //剩余人数减一 &#125; &#125; index++; //每报一次数，下标加一 if (index == n) &#123; //是循环数数，当下标大于n时，说明已经数了一圈， index = 0; //将下标设为零重新开始。 &#125; &#125; for (int i = 0; i &lt; n; i++) &#123; if (arr[i] == true) &#123; System.out.println(i); &#125; &#125; &#125;&#125; 个人代码1： 12345678910111213141516171819202122232425262728293031323334import java.util.Scanner;public class Demo36 &#123; public static void main(String[] args) &#123; System.out.println("请输入人数："); Scanner in = new Scanner(System.in); int[] a = new int[in.nextInt()]; for (int i = 0; i &lt; a.length; i++) &#123; a[i] = 1; &#125; int left = a.length; int j = 0; int num = 0; while (left &gt; 1) &#123; if (a[j] == 1) &#123; num++; &#125; if (num == 3) &#123; a[j] = 0; num = 0; left--; &#125; j++; if (j == a.length) &#123; j = 0; &#125; &#125; for (int i = 0; i &lt; a.length; i++) &#123; if (a[i] == 1) &#123; System.out.println("最后留下的人是"+ (i + 1) + "号"); break; &#125; &#125; &#125;&#125; 个人代码2： 1234567891011121314151617181920212223242526272829303132333435import java.util.LinkedList;import java.util.Scanner;public class Demo36 &#123; public static void main(String[] args) &#123; LinkedList&lt;Integer&gt; l = new LinkedList&lt;Integer&gt;(); System.out.println("请输入人数："); Scanner in = new Scanner(System.in); int len = in.nextInt(); for (int i = 0; i &lt; len; i++) &#123; l.add(i + 1); &#125; int sum = 0; int temp = 0; for (int i = 0; sum != len - 1;) &#123; if (l.get(i) != 0) &#123; temp++; &#125; if (temp == 3) &#123; l.remove(i); l.add(i, 0); temp = 0; sum++; &#125; i++; if (i == l.size()) &#123; i = 0; &#125; &#125; for (int t : l) &#123; if (t != 0) &#123; System.out.println("最后留下的人是" + t + "号"); &#125; &#125; &#125;&#125; 【程序37】题目:写一个函数，求一个字符串的长度，在main函数中输入字符串，并输出其长度。 123456789101112import java.util.Scanner;public class Demo37 &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); System.out.println("请输入一个字符串："); String mys = in.next(); System.out.println(str_len(mys)); &#125; public static int str_len(String x) &#123; return x.length(); &#125;&#125; 或 123456789import java.util.Scanner;public class Demo38 &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); System.out.println("请输入一个字符串："); String mys = in.next(); System.out.println(mys.length()); &#125;&#125; 【程序38】题目：编写一个函数，输入n为偶数时，调用函数求1/2+1/4+…+1/n,当输入n为奇数时，调用函数1/1+1/3+…+1/n 123456789101112131415161718192021222324252627import java.util.Scanner;public class Demo38 &#123; public static double ouShu(int n) &#123; double result = 0; for (int i = 2; i &lt;= n; i = i + 2) &#123; result += 1 / (double) i; &#125; return result; &#125; public static double jiShu(int n) &#123; double result = 0; for (int i = 1; i &lt;= n; i = i + 2) &#123; result += 1 / (double) i; &#125; return result; &#125; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); System.out.println("输入n的值："); int n = in.nextInt(); if (n % 2 == 0) &#123; //偶数，1/2+1/4+...+1/n System.out.println(ouShu(n)); &#125; else &#123; //奇数，1/1+1/3+...+1/n System.out.println(jiShu(n)); &#125; &#125;&#125; 【程序39】 题目：有五个学生，每个学生有3门课的成绩，从键盘输入以上数据（包括学生号，姓名，三门课成绩），计算出平均成绩，把原有的数据和计算出的平均分数存放在磁盘文件中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.File;import java.io.FileWriter;import java.util.Scanner;class Student &#123; private int number = 0; private String name = ""; private double[] a = new double[3]; public double getAve() &#123; return (a[0] + a[1] + a[2]) / 3; &#125; public Student(int number, String name, double[] a) &#123; super(); this.number = number; this.name = name; this.a = a; &#125; @Override public String toString() &#123; return "学号：" + this.number + "\t姓名：" + this.name + "\r\n各科成绩：\r\n" + a[0] + "\t" + a[1] + "\t" + a[2] + "\r\n平均成绩：\r\n" + this.getAve(); &#125;&#125;public class Demo39 &#123; public static Student input() &#123; Scanner s = new Scanner(System.in); System.out.println("请输入学号："); int num = s.nextInt(); System.out.println("请输入姓名："); String name = s.next(); System.out.println("请分别输入3门成绩："); double[] a = new double[3]; for (int i = 0; i &lt; 3; i++) &#123; a[i] = s.nextDouble(); &#125; return new Student(num, name, a); &#125; public static void main(String[] args) throws Exception &#123; Student[] st = new Student[2]; for (int i = 0; i &lt; st.length; i++) &#123; st[i] = input(); &#125; File f = new File("d:" + File.separator + "123.txt"); FileWriter output = new FileWriter(f); for (int i = 0; i &lt; st.length; i++) &#123; output.write(st[i].toString() + "\r\n"); output.write("\r\n"); &#125; output.close(); &#125;&#125; 【程序40】 题目：字符串排序。 1234567891011121314（利用容器类中的sort方法）import java.util.*;public class Demo40 &#123; public static void main(String[] args) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add("010102"); list.add("010003"); list.add("010201"); Collections.sort(list); for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125;&#125; 或 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.util.*;public class Demo40 &#123; public static void main(String[] args)&#123; Scanner in=new Scanner(System.in); System.out.println("请定义字符串的个数："); int n=in.nextInt(); String[] str=new String[n]; for(int i=0;i&lt;str.length;i++)&#123; System.out.println("请输入第"+(i+1)+"字符串："); str[i]=in.next(); &#125; strSort(n,str); System.out.println("字符串排序后："); for(int i=0;i&lt;str.length;i++)&#123; System.out.print(str[i]+" "); &#125; &#125; public static void strSort(int n,String[] arr)&#123; for(int i=0; i&lt;n; i++) &#123; for(int j=i+1; j&lt;n; j++) &#123; if(compare(arr[i], arr[j]) == false) &#123; String temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; &#125; &#125; &#125; static boolean compare(String s1, String s2) &#123; boolean result = true; for(int i=0; i&lt;s1.length() &amp;&amp; i&lt;s2.length(); i++) &#123; if(s1.charAt(i) &gt; s2.charAt(i)) &#123; result = false; break; &#125; else if(s1.charAt(i) &lt;s2.charAt(i)) &#123; result = true; break; &#125; else &#123; if(s1.length() &lt; s2.length()) &#123; result = true; &#125; else &#123; result = false; &#125; &#125; &#125; return result; &#125; &#125; 【程序41】 题目：海滩上有一堆桃子，五只猴子来分。第一只猴子把这堆桃子平均分为五份，多了一个，这只猴子把多的一个扔入海中，拿走了一份。第二只猴子把剩下的桃子又平均分成五份，又多了一个，它同样把多的一个扔入海中，拿走了一份，第三、第四、第五只猴子都是这样做的，问海滩上原来最少有多少个桃子？本题源码： 12345678910111213141516171819202122232425262728public class Demo41 &#123; static int ts = 0;// 桃子总数 int fs = 1;// 记录分的次数 static int hs = 5;// 猴子数 int tsscope = 5000;// 桃子数的取值范围，太大容易溢出。 public int fT(int t) &#123; if (t == tsscope) &#123; // 当桃子数到了最大的取值范围时取消递归 System.out.println("结束"); return 0; &#125; else &#123; if ((t - 1) % hs == 0 &amp;&amp; fs &lt;= hs) &#123; if (fs == hs) &#123; System.out.println("桃子数=" + ts + "时满足分桃条件"); &#125; fs += 1; return fT((t - 1) / 5 * 4);// 返回猴子拿走一份后的剩下的总数 &#125; else &#123; // 没满足条件 fs = 1;// 分的次数重置为1 return fT(ts += 1);// 桃子数加+1 &#125; &#125; &#125; public static void main(String[] args) &#123; new Demo41().fT(0); &#125;&#125; 个人修改： 1234567891011121314151617public class Demo41 &#123; public static void main(String[] args) &#123; int sum = 0; for (int i = 6;; i++) &#123;// 最少6个分最后一次 sum = i;// 桃子数 for (int j = 0; j &lt; 5; j++) &#123;// 分的次数循环 if ((sum - 1) % 5 == 0 &amp;&amp; j &lt; 5) &#123;// 如果扔一个后能均分5份，继续分 sum = (sum - 1) / 5 * 4;// 每分一次剩余桃子数 if (j == 4) &#123;// 如果已分5次，且仍能除尽，输出，退出程序 System.out.println(i); System.exit(0); &#125; &#125; &#125; &#125; &#125;&#125; 【程序42】 题目：809a=800a+9a+1。其中a代表的两位数,8a的结果为两位数，9a的结果为3位数。求a代表的两位数，及809a后的结果。（本题为无解，去掉1有解） 123456789101112public class Demo42 &#123; public static void main(String[] args) &#123; for (int i = 10; i &lt; 100; i++) &#123; if (809 * i == (800 * i + 9 * i + 1) &amp;&amp; 8 * i &gt;= 10 &amp;&amp; 8 * i &lt; 100 &amp;&amp; 9 * i &gt;= 100 &amp;&amp; 9 * i &lt; 1000) &#123; System.out.println("a =" + i); System.out.println("809*a="+ 809 * i); System.exit(0); &#125; &#125; &#125;&#125; 【程序43】题目：求0—7所能组成的奇数个数。数学算法： 123456789101112public class Demo43 &#123; public static void main(String[] args) &#123; // 因为是奇数，所以个位只能是1，3，5，7共4种，前面可随便排列 int count = 4;// 个位的4种 // 2位时，十位有8种，个位4种，8×4 // 3位时，8×8×4…… for (int i = 1; i &lt; 8; i++) &#123; count = 8 * count; System.out.println("count:" + count); &#125; &#125;&#125; 个人算法： 123456789101112131415161718//组成1位数是4个。//组成2位数是7*4个。//组成3位数是7*8*4个。//组成4位数是7*8*8*4个。//......public class Demo43 &#123; public static void main (String[] args) &#123; int sum=4; int j; System.out.println("组成1位数是 "+sum+" 个"); sum=sum*7; System.out.println("组成2位数是 "+sum+" 个"); for(j=3;j&lt;=9;j++)&#123; sum=sum*8; System.out.println("组成"+j+"位数是 "+sum+" 个"); &#125; &#125;&#125; 【程序44】题目：一个偶数总能表示为两个素数之和。（注：哥德巴赫猜想是想证明对任何大于6的自然数n之内的所有偶数可以表示为两个素数之和） 1234567891011121314151617181920212223public class Demo44 &#123; public static boolean isSuShu(int x) &#123; if (x == 0 || x == 1) &#123; return false; &#125; for (int i = 2; i &lt;= Math.sqrt(x); i++) &#123; if (x % i == 0) &#123; return false; &#125; &#125; return true; &#125; public static void main(String[] args) &#123; // 求了下100以内的情况 for (int i = 0; i &lt; 100; i = i + 2) &#123; for (int j = 0; j &lt;= (i + 1) / 2; j++) &#123; if (isSuShu(j) &amp;&amp; isSuShu(i - j)) &#123; System.out.println(i + "=" + j + "+" + (i - j)); &#125; &#125; &#125; &#125;&#125; 或 123456789101112131415161718public class Demo44&#123; public static void main(String[] args)&#123; for (int i=6;i&lt;=100 ;i+=2 )&#123; for (int j=2;j&lt;100 ;j++ )&#123; if(!isPrime(j)||!isPrime(i-j)||j&gt;=i) continue; System.out.println(i+"="+j+"+"+(i-j)); break; &#125; &#125; &#125; public static boolean isPrime(int n)&#123; for (int i=2;i&lt;n ;i++ )&#123; if(n%i==0)return false; &#125; return true; &#125;&#125; 【程序45】题目：某个公司采用公用电话传递数据，数据是四位的整数，在传递过程中是加密的，加密规则如下：每位数字都加上5，然后用和除以10的余数代替该数字，再将第一位和第四位交换，第二位和第三位交换。 12345678910111213141516171819202122232425import java.util.Scanner;public class Demo45&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); System.out.println("请输入一个4位数字："); String str = in.next(); if (!((str).matches("\\d&#123;4&#125;"))) &#123; System.out.println("输入的不是4位数字！"); System.exit(0); &#125; char[] c = str.toCharArray(); int[] a = new int[4]; for (int i = 0; i &lt; a.length; i++) &#123; a[i] = ((int) (c[i] - '0') + 5) % 10; &#125; int t; t = a[0]; a[0] = a[3]; a[3] = t; t = a[1]; a[1] = a[2]; a[2] = t; System.out.println("结果是：" + a[0] + a[1] + a[2] + a[3]); &#125;&#125; 或 123456789101112131415161718192021222324import java.util.*; public class Demo45 &#123; public static void main(String args[]) &#123; Scanner s = new Scanner(System.in); int num=0,temp; do&#123; System.out.print("请输入一个4位正整数："); num = s.nextInt(); &#125;while (num&lt;1000||num&gt;9999); int a[]=new int[4]; a[0] = num/1000; //取千位的数字 a[1] = (num/100)%10; //取百位的数字 a[2] = (num/10)%10; //取十位的数字 a[3] = num%10; //取个位的数字 for(int j=0;j&lt;4;j++) &#123; a[j]+=5; a[j]%=10; &#125; for(int j=0;j&lt;=1;j++) &#123; temp = a[j]; a[j] = a[3-j]; a[3-j] =temp; &#125; System.out.print("加密后的数字为："); for(int j=0;j&lt;4;j++) System.out.print(a[j]); &#125; &#125;]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http协议&Servlet入门]]></title>
    <url>%2F2019%2F04%2F25%2F1_Http%E5%8D%8F%E8%AE%AE%26Servlet%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[#Http协议&amp;Servlet入门 #Http协议 什么是协议 双方在交互、通讯的时候， 遵守的一种规范、规则。 http协议 针对网络上的客户端 与 服务器端在执行http请求的时候，遵守的一种规范。 其实就是规定了客户端在访问服务器端的时候，要带上哪些东西， 服务器端返回数据的时候，也要带上什么东西。 版本 1.0 请求数据，服务器返回后， 将会断开连接 1.1 请求数据，服务器返回后， 连接还会保持着。 除非服务器 | 客户端 关掉。 有一定的时间限制，如果都空着这个连接，那么后面会自己断掉。 ###演示客户端 如何 与服务器端通讯。 在地址栏中键入网络地址 回车 或者是平常注册的时候，点击了注册按钮 ， 浏览器都能显示出来一些东西。那么背地里到底浏览器和服务器是怎么通讯。 它们都传输了哪些数据。 安装抓包工具 HttpWatch (IE插件) 打开tomcat. 输入localhost:8080 打开首页 在首页上找到Example 字样 6.x 和 7.x 的文档页面有所不同，但是只要找到example就能够找到例子工程 选择 servlet 例子 —&gt; Request Parameter ![img01.png-45.4kB][1] 接着点击Request Parameters 的 Execute超链接 ![img02.png-14.9kB][2] 执行tomcat的例子，然后查看浏览器和 tomcat服务器的对接细节 ![img03.png-47.1kB][3] ###Http请求数据解释 请求的数据里面包含三个部分内容 ： 请求行 、 请求头 、请求体 请求行 POST /examples/servlets/servlet/RequestParamExample HTTP/1.1 POST ： 请求方式 ，以post去提交数据 /examples/servlets/servlet/RequestParamExample 请求的地址路径 ， 就是要访问哪个地方。 HTTP/1.1 协议版本 请求头 Accept: application/x-ms-application, image/jpeg, application/xaml+xml, image/gif, image/pjpeg, application/x-ms-xbap, */* Referer: http://localhost:8080/examples/servlets/servlet/RequestParamExample Accept-Language: zh-CN User-Agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E) Content-Type: application/x-www-form-urlencoded Accept-Encoding: gzip, deflate Host: localhost:8080 Content-Length: 31 Connection: Keep-Alive Cache-Control: no-cache Accept: 客户端向服务器端表示，我能支持什么类型的数据。 Referer ： 真正请求的地址路径，全路径 Accept-Language: 支持语言格式 User-Agent: 用户代理 向服务器表明，当前来访的客户端信息。 Content-Type： 提交的数据类型。经过urlencoding编码的form表单的数据 Accept-Encoding： gzip, deflate ： 压缩算法 。 Host ： 主机地址 Content-Length： 数据长度 Connection : Keep-Alive 保持连接 Cache-Control ： 对缓存的操作 请求体 浏览器真正发送给服务器的数据 发送的数据呈现的是key=value ,如果存在多个数据，那么使用 &amp; firstname=zhang&amp;lastname=sansan###Http响应数据解析 请求的数据里面包含三个部分内容 ： 响应行 、 响应头 、响应体 HTTP/1.1 200 OK Server: Apache-Coyote/1.1 Content-Type: text/html;charset=ISO-8859-1 Content-Length: 673 Date: Fri, 17 Feb 2017 02:53:02 GMT ...这里还有很多数据... 响应行 HTTP/1.1 200 OK 协议版本 状态码 咱们这次交互到底是什么样结果的一个code. 200 : 成功，正常处理，得到数据。 403 : for bidden 拒绝 404 ： Not Found 500 ： 服务器异常 OK 对应前面的状态码 响应头 Server: 服务器是哪一种类型。 Tomcat Content-Type ： 服务器返回给客户端你的内容类型 Content-Length ： 返回的数据长度 Date ： 通讯的日期，响应的时间 ##Get 和 Post请求区别 ![img04.png-28.6kB][4] post 1. 数据是以流的方式写过去，不会在地址栏上面显示。 现在一般提交数据到服务器使用的都是POST 2. 以流的方式写数据，所以数据没有大小限制。 get 1. 会在地址栏后面拼接数据，所以有安全隐患。 一般从服务器获取数据，并且客户端也不用提交上面数据的时候，可以使用GET 2. 能够带的数据有限， 1kb大小 ###Web资源 在http协议当中，规定了请求和响应双方， 客户端和服务器端。与web相关的资源。 有两种分类 静态资源 html 、 js、 css 动态资源 servlet/jsp ##Servlet servlet是什么? 其实就是一个java程序，运行在我们的web服务器上，用于接收和响应 客户端的http请求。 更多的是配合动态资源来做。 当然静态资源也需要使用到servlet，只不过是Tomcat里面已经定义好了一个 DefaultServlet ###Hello Servlet 得写一个Web工程 ， 要有一个服务器。 测试运行Web工程 1. 新建一个类， 实现Servlet接口 2. 配置Servlet ， 用意： 告诉服务器，我们的应用有这么些个servlet。 在webContent/WEB-INF/web.xml里面写上以下内容。123456789101112&lt;!-- 向tomcat报告， 我这个应用里面有这个servlet， 名字叫做HelloServlet , 具体的路径是com.zhiyou100.servlet.HelloServlet --&gt;&lt;servlet&gt; &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.zhiyou100.servlet.HelloServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;!-- 注册servlet的映射。 servletName : 找到上面注册的具体servlet， url-pattern: 在地址栏上的path 一定要以/打头 --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/a&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 3. 在地址栏上输入 http://localhost:8080/项目名称/a ###Servlet执行过程 ![img05.png-45.3kB][5] ###Servlet的通用写法 Servlet (接口) | | GenericServlet | | HttpServlet （用于处理http的请求） 定义一个类，继承HttpServlet 复写doGet 和 doPost ![img06.png-16.7kB][6] ##Servlet的生命周期 生命周期 从创建到销毁的一段时间 生命周期方法 从创建到销毁，所调用的那些方法。 init方法 在创建该servlet的实例时，就执行该方法。 一个servlet只会初始化一次， init方法只会执行一次 默认情况下是 ： 初次访问该servlet，才会创建实例。 service方法 只要客户端来了一个请求，那么就执行这个方法了。 该方法可以被执行很多次。 一次请求，对应一次service方法的调用 destroy方法 servlet销毁的时候，就会执行该方法 1. 该项目从tomcat的里面移除。 2. 正常关闭tomcat就会执行 shutdown.bat doGet 和 doPost不算生命周期方法，所谓的生命周期方法是指，从对象的创建到销毁一定会执行的方法， 但是这两个方法，不一定会执行。 ###让Servlet创建实例的时机 提前。 默认情况下，只有在初次访问servlet的时候，才会执行init方法。 有的时候，我们可能需要在这个方法里面执行一些初始化工作，甚至是做一些比较耗时的逻辑。 那么这个时候，初次访问，可能会在init方法中逗留太久的时间。 那么有没有方法可以让这个初始化的时机提前一点。 在配置的时候， 使用load-on-startup元素来指定， 给定的数字越小，启动的时机就越早。 一般不写负数， 从2开始即可。 123456&lt;servlet&gt; &lt;servlet-name&gt;HelloServlet04&lt;/servlet-name&gt; &lt;servlet-class&gt;com.itheima.servlet.HelloServlet04&lt;/servlet-class&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt; &lt;/servlet&gt; ##ServletConfig Servlet的配置，通过这个对象，可以获取servlet在配置的时候一些信息 先说 ， 在写怎么用， 最后说有什么用。 1234567891011121314151617181920212223//1. 得到servlet配置对象 专门用于在配置servlet的信息ServletConfig config = getServletConfig();//获取到的是配置servlet里面servlet-name 的文本内容String servletName = config.getServletName();System.out.println("servletName="+servletName);//2、。 可以获取具体的某一个参数。 String address = config.getInitParameter("address");System.out.println("address="+address);//3.获取所有的参数名称Enumeration&lt;String&gt; names = config.getInitParameterNames();//遍历取出所有的参数名称while (names.hasMoreElements()) &#123; String key = (String) names.nextElement(); String value = config.getInitParameter(key); System.out.println("key==="+key + " value="+value); &#125; ###为什么需要有这个ServletConfig 未来我们自己开发的一些应用，使用到了一些技术，或者一些代码，我们不会。 但是有人写出来了。它的代码放置在了自己的servlet类里面。 刚好这个servlet 里面需要一个数字或者叫做变量值。 但是这个值不能是固定了。 所以要求使用到这个servlet的公司，在注册servlet的时候，必须要在web.xml里面，声明init-params 在开发当中比较少用。 刚才的这个实验， 希望基础好或者感兴趣的同学，回去自己做一下。 ##总结 Http协议 1. 使用HttpWacht 抓包看一看http请求背后的细节。 2. 基本了解 请求和响应的数据内容 请求行、 请求头 、请求体 响应行、响应头、响应体 3. Get和Post的区别 Servlet【重点】 1. 会使用简单的servlet 1.写一个类，实现接口Servlet 2. 配置Servlet 3. 会访问Setvlet 2. Servlet的生命周期 init 一次 创建对象 默认初次访问就会调用或者可以通过配置，让它提前 load-on-startup service 多次，一次请求对应一次service destory 一次 销毁的时候 从服务器移除 或者 正常关闭服务器 3. ServletConfig 获取配置的信息， params]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Servlet</tag>
        <tag>Http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hibernate]]></title>
    <url>%2F2019%2F04%2F25%2F1_Hibernate%2F</url>
    <content type="text"><![CDATA[Hibernate一. 开发遇到的问题什么是框架,框架从何而来,为什么使用框架? 框架: 1. 是一系列jar包,其本质是对JDK功能的拓展. 2. 框架是一组程序的集合,包含了一系列的最佳实践,作用是解决某一个领域的问题.最佳实践(Best Practice)实际上是无数程序员经历过无数次尝试之后,总结出来的处理特定问题的特定方法.如果把程序员的自由发挥看作是一条通往成功的途径,最佳实践就是其中的最短路径,能极大的解放生产力.Web开发中的最佳实践: 1. 分层开发模式(技术层面的&quot;分而治之&quot;) 2. JavaEE开发根据职责的纵向划分:表现层,业务层,持久层: - 表现层(Predentation Layer):web/mvc: 负责处理与界面交互的相关操作 (Struts2/Spring MVC) - 业务层(Business Layer) :service: 负责复杂的业务逻辑计算和判断 (Spring) - 持久层(Persistent Layer) :dao: 负责将业务逻辑数据进行持久化存储 (Hibernate/iBatis/MyBatis) 持久层封装了数据访问的细节,为业务逻辑层提供了面向对象的API,完善的持久层应该达到: - 代码重用性高,可以完成所有的数据访问操作. - 可以支持多种关系型数据库. - 具有相对独立性,当持久层变化时,不会影响上一层实现. ![image_1cihfknec16052jcb1r9ho1i1q1p.png-93kB][1] 二. ORM对象关系映射（Object Relational Mapping，简称ORM）是一种为了解决面向对象与关系数据库存在的互不匹配的现象的技术。 简单的说，ORM是通过使用描述对象和数据库之间映射的元数据，将java程序中的对象自动持久化到关系数据库中。避免直接使用SQL语句对关系型数据库中的数据进行操作.减少代码编写量,提高产品质量. ORM 主要解决对象-关系的映射： 面向对象概念 面向关系概念类 表对象 表的行（记录）属性 表的列（字段） ORM的实现思想：将关系数据库中表中的记录映射成为对象，以对象的形式展现，程序员可以把对数据库的操作转化为对对象的操作。因此ORM的目的是为了方便开发人员以面向对象的思想来实现对数据库的操作。 ORM 采用元数据来描述对象-关系映射细节：元数据通常采用 XML 格式，并且存放在专门的对象-关系映射文件中。 目前流行的ORM框架：1.JPA:本身是一种ORM规范,不是ORM框架.由各大ORM框架提供实现.2.Hibernate:目前最流行的ORM框架.设计灵巧,性能优秀,文档丰富.3.MyBatis:本是apache的一个开源项目iBatis,提供的持久层框架包括SQL Maps和DAO,允许开发人员直接编写SQL.等]]></content>
      <categories>
        <category>Java框架</category>
      </categories>
      <tags>
        <tag>Hibernate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase详细]]></title>
    <url>%2F2019%2F04%2F25%2F1_HBase%E8%AF%A6%E7%BB%86%2F</url>
    <content type="text"><![CDATA[HBaseHBase简介HBase起源HBase 的原型是 Google 的 BigTable 论文，受到了该论文思想的启发，目前作为 Hadoop 的子项目来开发维护，用于支持结构化的数据存储。 2006 年 Google 发表 BigTable 白皮书 2006 年开始开发 HBase 2008 年北京成功开奥运会，程序员默默地将 HBase 弄成了 Hadoop 的子项目 2010 年 HBase 成为 Apache 顶级项目 现在很多公司二次开发出了很多发行版本。 什么是HBaseHBase（Hadoop DataBase）是一个高可靠性、高性能、面向列、可伸缩、实时读写的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。HBASE的目标是存储并处理大型的数据，更具体来说是仅需使用普通的硬件配置，就能够处理由成千上万的行和列所组成的大型数据。HBase是Google Bigtable的开源实现，但是也有很多不同之处。比如：Google Bigtable利用GFS作为其文件存储系统，HBASE利用Hadoop HDFS作为其文件存储系统；Google运行MAPREDUCE来处理Bigtable中的海量数据，HBASE同样利用Hadoop MapReduce来处理HBASE中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为其分布式协调服务。用来存储非结构化和半结构化的松散数据。 与传统数据库的对比 传统数据库遇到的问题 数据量很大的时候无法存储 没有很好的备份机制 数据达到一定数量开始缓慢，很大的话基本无法支撑 HBase优势 线性扩展，随着数据量增多可以通过节点扩展进行支撑 数据存储在hdfs上，备份机制健全 通过zookeeper协调查找数据，访问速度块。 HBase的数据模型![image_1cs4fmo7c9ncq77hvn14nt1c7f8c.png-61.7kB][1] ROW KEY: 决定一行数据 按照字典顺序排序的。 Row key只能存储64k的字节数据 Timestamp时间戳: 在HBase每个cell存储单元对同一份数据有多个版本，根据唯一的时间戳来区分每个版本之间的差异，不同版本的数据按照时间倒序排序，最新的数据版本排在最前面。 时间戳的类型是 64位整型。 时间戳可以由HBase(在数据写入时自动)赋值，此时时间戳是精确到毫秒的当前系统时间。 时间戳也可以由客户显式赋值，如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。 Column Family列族 &amp; qualifier列: HBase表中的每个列都归属于某个列族，列族必须作为表模式(schema)定义的一部分预先给出。如 create ‘test’, ‘info’； 列名以列族作为前缀，每个“列族”都可以有多个列成员(column)；如info:name, info:age, 新的列族成员（列）可以随后按需、动态加入 权限控制、存储以及调优都是在列族层面进行 HBase把同一列族里面的数据存储在同一目录下，由几个文件保存。 Cell单元格: 由行和列的坐标交叉决定 单元格是有版本的 单元格的内容是未解析的字节数组 由{row key， column( = +)， version} 唯一确定的单元 cell中的数据是没有类型的，全部是字节码形式存贮。 HLog(WAL log): HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是” 写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。 HLog SequeceFile的Value是HBase的KeyValue对象，即对应HFile中的KeyValue。 HBase集群中的角色(进程) 一个或者多个主节点，Hmaster 监控 RegionServer 处理 RegionServer 故障转移 处理元数据的变更 处理 region 的分配或移除 在空闲时间进行数据的负载均衡 通过 Zookeeper 发布自己的位置给客户端 多个从节点，HregionServer 负责存储 HBase 的实际数据 处理分配给它的 Region 刷新缓存到 HDFS 维护 HLog 执行压缩 负责处理 Region 分片 基本原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HBase 一种是作为存储的分布式文件系统，另一种是作为数据处理模型的 MR 框架。因为日常开发人员比较熟练的是结构化的数据进行处理，但是在 HDFS 直接存储的文件往往不具有结构化，所以催生出了 HBase 在 HDFS 上的操作。如果需要查询数据，只需要通过键值便可以成功访问。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HBase 内置有 Zookeeper，但一般我们会有其他的 Zookeeper 集群来监管 master 和 regionserver，Zookeeper 通过选举，保证任何时候，集群中只有一个活跃的 HMaster，HMaster 与 HRegionServer 启动时会向 ZooKeeper 注册，存储所有 HRegion 的寻址入口，实时监控 HRegionserver 的上线和下线信息。并实时通知给 HMaster，存储 HBase 的 schema 和 table 元数据，默认情况下，HBase 管理 ZooKeeper 实例，Zookeeper 的引入使得 HMaster 不再是单点故障。一般情况下会启动两个 HMaster，非 Active 的 HMaster 会定期的和 Active HMaster 通信以获取其最新状态，从而保证它是实时更新的，因而如果启动了多个 HMaster 反而增加了 Active HMaster 的负担。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个 RegionServer 可以包含多个 HRegion，每个 RegionServer 维护一个 HLog，和多个 HFiles以及其对应的 MemStore。RegionServer 运行于 DataNode 上，数量可以与 DatNode 数量一致，![image_1clfil80u13cl2tkpib1fuv1jj39.png-336.1kB][2] 组件说明: Write-Ahead logsHBase 的修改记录，当对 HBase 读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在写入一个叫做 Write-Ahead logfile的文件中，再写到内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 HFile这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。 StoreHFile 存储在 Store 中，一个 Store 对应 HBase 表中的一个列族。 MemStore顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在 WAL 中之后，RegsionServer 会在内存中存储键值对。 RegionHbase 表的分片，HBase 表会根据 RowKey 值被切分成不同的 region 存储在 RegionServer 中，在一个 RegionServer 中可以有多个不同的 region。 HBase 安装部署下载地址[点击下载][3]解压1[root@master HBase]# tar -zxvf hbase-1.4.6-bin.tar.gz修改配置文件12345678910111213141516171819202122232425262728293031323334353637[root@master conf]# vim hbase-env.sh##添加以下内容 export JAVA_HOME=/opt/apps/Java/jdk1.8.0_172 export HBASE_HOME=/opt/apps/HBase/hbase-1.4.6 export HBASE_CLASSPATH=$CLASSPATH:$HBASE_HOME/lib export HBASE_LOG_DIR=$&#123;HBASE_HOME&#125;/logs export HBASE_MANAGES_ZK=false[root@master conf]# vim hbase-site.xml## 添加以下内容 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181,slave3:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/apps/Zookeeper/data&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [root@master conf]# vim regionservers ## 添加以下内容 slave1 slave2 slave3 master解决Jar包问题123456789101112131415161718 [root@master lib]# rm -rf hadoop-* [root@master lib]# rm -rf zookeeper-3.4.10.jar [root@master lib]# cp ./* /opt/apps/HBase/hbase-1.4.6/lib/ hadoop-annotations-2.7.6.jarhadoop-auth-2.7.6.jarhadoop-client-2.7.6.jarhadoop-common-2.7.6.jarhadoop-hdfs-2.7.6.jarhadoop-mapreduce-client-app-2.7.6.jarhadoop-mapreduce-client-common-2.7.6.jarhadoop-mapreduce-client-core-2.7.6.jarhadoop-mapreduce-client-jobclient-2.7.6.jarhadoop-mapreduce-client-shuffle-2.7.6.jarhadoop-yarn-api-2.7.6.jarhadoop-yarn-client-2.7.6.jarhadoop-yarn-common-2.7.6.jarhadoop-yarn-server-common-2.7.6.jarzookeeper-3.4.12.jar在HBase中添加Hadoop的配置文件123## 通过软连接的方式创建[root@master conf]# ln -s /opt/apps/Hadoop/hadoop-2.7.6/etc/hadoop/core-site.xml /opt/apps/HBase/hbase-1.4.6/conf/core-site.xml[root@master conf]# ln -s /opt/apps/Hadoop/hadoop-2.7.6/etc/hadoop/hdfs-site.xml /opt/apps/HBase/hbase-1.4.6/conf/hdfs-site.xml分发: 访问WEB UIhttp://master:16010/HBase 的 Shell 操作命令操作: 123456789101112131415161718192021222324252627282930313233## 进入到HBase的Shell[root@master ~]# hbase shell## 查看帮助的命令（可以用查看HBASE支持的所有的命令）hbase(main):001:0&gt; help## 查看当前的数据库中有哪些表hbase(main):002:0&gt; list0 row(s) in 0.2300 seconds=&gt; []## 创建表hbase(main):058:0&gt; create 'student','info'## 插入数据hbase(main):058:0&gt; put 'student','1000001','info:name','wangguo'## 扫描整个表的数据hbase(main):058:0&gt; scan 'student'## 扫描指定范围的数据hbase(main):058:0&gt; scan 'student',&#123;STARTROW=&gt;'a',ENDROW=&gt;'ab'&#125;## 查看指定行中的某一列的数据hbase(main):058:0&gt; get 'student','1000001','info:sex'## 查看指定行的所有数据hbase(main):058:0&gt; get 'student','1000001'## 查看表的结构hbase(main):058:0&gt; describe 'student'## 统计这个表有多少行hbase(main):058:0&gt; count 'student'## 删除某一行的某一列hbase(main):058:0&gt; delete 'student','1000001','info:sex'## 直接删除某一行hbase(main):058:0&gt; deleteall 'student','1000001'## 清空表数据hbase(main):058:0&gt; truncate 'student'## 删除表hbase(main):058:0&gt; disable 'student'hbase(main):058:0&gt; drop 'student' HBase 的 API 操作HBASE的读写流程画图 读数据流程: region server 保存着meta表以及数据，要想访问数据，客户端必须先通过Zookeeper获取-ROOT-的位置信息 通过-ROOT-来获取meta中的region的位置， 客户端通过meta获取数据的region位置 通过region的位置获取数据 写数据的流程: 客户端先访问zookeeper，找到元数据信息 确定要写入的数据是在哪一个region上 然后客户端向该region server发送写数据的请求 客户端先把数据写入到Hlog中，以及所需要的操作，防止数据的丢失 然后写入Memstore 如果Hlog和Memstore均写入成功，则表示该数据写入成功。如果在这个过程中，Memstore的数据达到了阈值，就会将Memstore中的数据刷新到storefile storefile过多的时候，region就会越来越大，如果达到阈值，那么region会被master一分为2 storefile最后会不断的溢出成Hfile 在region server空闲的时候，会将HFile这些小文件进行合并 HBase 的 MR通过 HBase 的相关 JavaAPI，我们可以实现伴随 HBase 操作的 MapReduce 过程，比如使用 MapReduce 将数据从本地文件系统导入到 HBase 的表中，比如我们从 HBase 中读取一些原始数据后使用 MapReduce 做数据分析。 官方HBase MR的应用统计Hbase表中的行数123456## 统计Hbase表中的行数[root@master lib]# yarn jar hbase-server-1.4.6.jar rowcounter student## 报错## Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/hase/filter/Filter## 解决方案## 在环境变量中添加HADOOP_CLASSPATH变量，将HBase的jar包添加进去导入HDFS上的文件到HBase12345678910111213 [root@master lib]# vim input_hbase.tsv ## 添加数据 11111 zhangsan 1811112 lisi 1711113 wangwu 9911114 zhaoliu 100 ## 上传至Hadoop [root@master lib]# hadoop fs -mkdir /hbase/mr/ [root@master lib]# hadoop fs -put input_hbase.tsv /hbase/mr/ ## 在HBase上创建相应的表，否则会出现表不存在的异常 hbase(main):001:0&gt; create 'people','info' ## 执行 [root@master lib]# yarn jar hbase-server-1.4.6.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:age people hdfs://master:9000/hbase/mr自定义HBase MR扩充-hbase过滤器FilterListFilterList 代表一个过滤器列表，可以添加多个过滤器进行查询，多个过滤器之间的关系有：与关系（符合所有）：FilterList.Operator.MUST_PASS_ALL或关系（符合任一）：FilterList.Operator.MUST_PASS_ONE 使用方法1234567891011121314FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ONE); Scan s1 = new Scan(); filterList.addFilter(new SingleColumnValueFilter( Bytes.toBytes(“f1”), Bytes.toBytes(“c1”), CompareOp.EQUAL,Bytes.toBytes(“v1”))); filterList.addFilter(new SingleColumnValueFilter( Bytes.toBytes(“f1”), Bytes.toBytes(“c2”), CompareOp.EQUAL,Bytes.toBytes(“v2”))); // 添加下面这一行后，则只返回指定的cell，同一行中的其他cell不返回 s1.addColumn(Bytes.toBytes(“f1”), Bytes.toBytes(“c1”)); s1.setFilter(filterList); //设置filter ResultScanner ResultScannerFilterList = table.getScanner(s1);//返回结果列表过滤器的种类: 列值过滤器—SingleColumnValueFilter 过滤列值的相等、不等、范围等 列名前缀过滤器—ColumnPrefixFilter 过滤指定前缀的列名 多个列名前缀过滤器—MultipleColumnPrefixFilter过滤多个指定前缀的列名 rowKey过滤器—RowFilter 通过正则，过滤rowKey值。 列值过滤器—SingleColumnValueFilterSingleColumnValueFilter 列值判断相等 (CompareOp.EQUAL ),不等(CompareOp.NOT_EQUAL),范围 (e.g., CompareOp.GREATER)…………下面示例检查列值和字符串’values’ 相等… 123456SingleColumnValueFilter f = new SingleColumnValueFilter( Bytes.toBytes("cFamily"), Bytes.toBytes("column"), CompareFilter.CompareOp.EQUAL, Bytes.toBytes("values"));s1.setFilter(f);注意：如果过滤器过滤的列在数据表中有的行中不存在，那么这个过滤器对此行无法过滤。 列名前缀过滤器—ColumnPrefixFilter过滤器—ColumnPrefixFilterColumnPrefixFilter 用于指定列名前缀值相等 12ColumnPrefixFilter f = new ColumnPrefixFilter(Bytes.toBytes("values"));s1.setFilter(f);多个列值前缀过滤器—MultipleColumnPrefixFilterMultipleColumnPrefixFilter 和 ColumnPrefixFilter 行为差不多，但可以指定多个前缀 123byte[][] prefixes = new byte[][] &#123;Bytes.toBytes("value1"),Bytes.toBytes("value2")&#125;;Filter f = new MultipleColumnPrefixFilter(prefixes);s1.setFilter(f);rowKey过滤器—RowFilterRowFilter 是rowkey过滤器通常根据rowkey来指定范围时，使用scan扫描器的StartRow和StopRow方法比较好。 1234Filter f = new RowFilter( CompareFilter.CompareOp.EQUAL, new RegexStringComparator("^1234")); //匹配以1234开头的rowkeys1.setFilter(f);HBase与Hive的区别Hive: 数据仓库Hive 的本质其实就相当于将 HDFS 中已经存储的文件在 Mysql 中做了一个映射关系，以方便使用 HQL 去管理查询。 用于数据分析、清洗Hive 适用于离线的数据分析和清洗，延迟较高。 基于 HDFS 、MapReduceHive 存储的数据依旧在 DataNode 上，编写的 HQL 语句终将是转换为 MapReduce 代码执行。 HBase: 数据库是一种面向列存储的分布式的非关系型数据库。 用于存储结构化和非结构化的数据适用于单表非关系型数据的存储，不适合做关联查询，类似 JOIN 等操作。 基于 HDFS数据持久化存储的体现形式是 Hfile，存放于 DataNode 中，被 ResionServer 以 region 的形式进行管理。 延迟较低，适合接入在线业务使用面对大量的企业数据，HBase 可以实现单表大量数据的存储，同时提供了高效的数据访问速度。 Hive与HBase的集成操作配置: 替换相应JAR包 修改配置文件1 简单操作1在Hive中导入数据的时候，直接关联HBase表，插入到Hive中的数据可以直接同步到HBase中。1234567hive (default)&gt; create table hive_hbase_people(id int,name string,age int) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties("hbase.columns.mapping"=":key,info:name,info:age") tblproperties("hbase.table.name"="hbase_hive_people");## 创建完成之后，HBASE中的表会自动创建## 关联表要想插入数据，不能使用load方式加载 简单操作2假设HBase中的某一个表中，已经存储了一些数据，现在需要使用Hive的外部表来关联HBase的这个表，可以借助Hive进行离线数据分析。12345678910## 在HBase 创建相应的表hbase(main):003:0&gt; create 'zhiyou:student','haha'## hive中创建关联的外部表hive (default)&gt; create external table hive_external_hbase_student(id int,name string) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties ("hbase.columns.mapping"=":key,haha:name") tblproperties("hbase.table.name"="zhiyou:student");hive (default)&gt; select * from hive_external_hbase_student; sqoop与HBase的集成操作配置123456[root@master ~]# vim /opt/apps/Sqoop/sqoop-1.4.7/conf/sqoop-env.sh#set the path to where bin/hbase is availableexport HBASE_HOME=/opt/apps/HBase/hbase-1.4.6#Set the path for where zookeper config dir isexport ZOOKEEPER_HOME=/opt/apps/Zookeeper/zookeeper-3.4.12export ZOOCFGDIR=$ZOOKEEPER_HOME/conf相关参数 参数 描述 –column-family Sets the target column family for the import设置导入的目标列族。 –hbase-create-table If specified, create missing HBase tables 是否自动创建不存在的 HBase 表（这就意味着，不需要手动提前在 HBase 中先建立表） –hbase-row-key Specifies which input column to use as the row key.In case, if input table contains composite key, then must be in the form of a comma-separated list of composite key attributes. mysql 中哪一列的值作为 HBase 的 rowkey，如果rowkey是个组合键，则以逗号分隔。 （注：避免 rowkey 的重复） –hbase-table Specifies an HBase table to use as the target instead of HDFS.指定数据将要导入到 HBase 中的哪张表中。 –hbase-bulkload Enables bulk loading.是否允许 bulk 形式的导入。 简单使用12345678910[root@master ~]# sqoop import \--connect jdbc:mysql://master:3306/mysql_bigdata \--username root \--password 123456 \--table product \--columns "id,name,price" \--hbase-create-table \--hbase-row-key "id" \--hbase-table "hbase_sqoop_product_1" \--column-family "info"Hbase shell的其他命令数据的备份与恢复备份停止 HBase 服务后，使用 distcp 命令运行 MapReduce 任务进行备份，将数据备份到另一个地方，可以是同一个集群，也可以是专用的备份集群。即，把数据转移到当前集群的其他目录下（也可以不在同一个集群中）恢复非常简单，与备份方法一样，将数据整个移动回来即可。节点的管理服役（commissioning ）当启动 regionserver 时，regionserver 会向 HMaster 注册并开始接收本地数据，开始的时候，新加入的节点不会有任何数据，平衡器开启的情况下，将会有新的 region 移动到开启的RegionServer 上。如果启动和停止进程是使用 ssh 和 HBase 脚本，那么会将新添加的节点的主机名加入到 conf/regionservers 文件中。 退役（decommissioning) : 顾名思义，就是从当前 HBase 集群中删除某个 RegionServer 停止负载均衡器 停止region server Hbase 的优化 高可用 : 在 HBase 中 Hmaster 负责监控 RegionServer 的生命周期，均衡 RegionServer 的负载，如果 Hmaster 挂掉了，那么整个 HBase 集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以 HBase 支持对 Hmaster 的高可用配置。 12345[root@master conf]# vim backup-mastersmasterslave1slave2##远程拷贝 Hadoop 高可用 : 1234567891011 ## 配置多个namenode ##core-site.xml&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hacluster&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181,slave3:2181,slave4:2181&lt;/value&gt;&lt;/property&gt;## hdfs-site.xmlLinux优化123456789101112## 1. 开启文件预读缓存：ra：readaheadblockdev --setra 1024 /dev/sda## 2. 关闭进程睡眠池：不允许后台进程进入睡眠状态，如果这个进程是空闲的，那么直接kill掉sysctl -w vm.swappiness=0## 调整允许打开最大的文件数和线程数ulimit -u ## 允许打开最大文件数ulimit -n ## 查看允许最大的进程数##可以在下面的文件中修改/etc/security/limits.conf## 3. 补丁更新zookeeper优化session.timeout设置为30秒HBase优化预分区默认情况下，在创建HBase表的时候会自动创建一个region分区，当导入数据的时候，所有的HBase客户端都向这一个region写数据，直到这个region足够大了才进行切分。一种可以加快批量写入速度的方法是通过预先创建一些空的regions，这样当数据写入HBase时，会按照region分区情况，在集群内做数据的负载均衡。rowkey的设计 该数据被分到哪一个region中看的是rowkey。HBase中row key用来检索表中的记录，支持以下三种方式： 通过单个row key访问：即按照某个row key键值进行get操作 通过row key的range进行scan：即通过设置startRowKey和endRowKey，在这个范围内进行扫描； 全表扫描：即直接扫描整张表中所有行记录。 在HBase中，row key可以是任意字符串，最大长度64KB，实际应用中一般为10~100bytes，存为byte[]字节数组，一般设计成定长的。row key是按照字典序存储，因此，设计row key时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。 举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为row key的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为row key，这样能保证新写入的数据在读取时可以被快速命中。 生成随机数，hash，散列值1001，sha1：dd01903921ea24941c26a48f2cec24e0bb0e8cc7 1002，sha1：a5b1d7e217aa227d5b2b8a84920780cf637960e2 字符串反转201808231000000001反转后100000000132808102 字符串拼接原来的字符串拼接一个随机字符串 rowkey长度原则不要太长，限定在100个字节以内 rowkey唯一原则 列族不要在一张表里定义太多的column family。目前Hbase并不能很好的处理超过2~3个column family的表。因为某个column family在flush的时候，它邻近的column family也会因关联效应被触发flush，最终导致系统产生更多的I/O。[点击这里有说明][4] In Memory创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到RegionServer的缓存中，保证在读取的时候被cache命中。 Max Version创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。 Time To Live创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 * 24 * 60 * 60)。 Hbase和DBMS比较：查询数据不灵活： 不能使用column之间过滤查询 不支持全文索引。使用solr和hbase整合完成全文搜索。 使用MR批量读取hbase中的数据，在solr里面建立索引（no store）之保存rowkey的值。 根据关键词从索引中搜索到rowkey（分页） 根据rowkey从hbase查询所有数据]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-Hive]]></title>
    <url>%2F2019%2F04%2F25%2F1_Hadoop%E4%B9%8BHive%2F</url>
    <content type="text"><![CDATA[Hadoop之HiveHive的安装常用命令: 初始化 1[root@master hive-2.3.3]# schematool -initSchema -dbType derby 查看数据库 123456789101112131415161718192021222324252627282930#查看有哪些数据库hive&gt; show databases;OKdefaultTime taken: 4.746 seconds, Fetched: 1 row(s)#进入到那个数据库hive&gt; use default;OKTime taken: 0.023 secondshive&gt; show tables;OKTime taken: 0.055 secondshive&gt; create table student(id int,name string);OKTime taken: 0.827 secondshive&gt; show tables;OKstudentTime taken: 0.019 seconds, Fetched: 1 row(s)hive&gt; desc student;OKid int name string Time taken: 0.245 seconds, Fetched: 2 row(s)hive&gt; select * form student;FAILED: ParseException line 1:9 missing EOF at 'form' near '*'hive&gt; select * from student;OKTime taken: 1.413 secondshive&gt; quit; YkNWIr-_g6G8 1hive -e "select * from bigdata.student;" 不进入Hive的命令行窗口就可以执行SQL语句 1[root@master test]# hive -f hive-seclet.sql 不进入Hive的命令行窗口就可以执行文件中的SQL 12hive (default)&gt; quit;hive (default)&gt; exit; exit:先提交数据，然后退出quit:不提交数据，直接退出。 1hive (default)&gt; dfs -ls /hive; 使用Hive的命令行窗口执行HDFS文件系统的查询 1hive (default)&gt; ! ls /opt; 使用Hive的命令窗口查看本地文件系统的文件和目录 1[root@master ~]# cat .hivehistory 在本地文件系统中会存储hive中执行的命令的历史（在当前用户主目录） Hive中的数据类型基本数据类型 Hive数据类型 长度 Java中对应的类型 example tinyint 1字节 byte 20 int 4字节 int 111111 smallint 2字节 short 222 bigint 8字节 long 111111111111 boolean 布尔 boolean true float 4字节 float 1.1 double 8字节 double 1.11111111 string string “hhahaha”,’hhh’ timstamp 日期类型 Timestamp date 日期类型 date binary 字节数组 集合类型|数据类型|描述||—|—| |struct|比如某一列的数据类型是struct{first string,sc string,th int},.sc来拿到第2个元素| |map|一组键值对集合，[‘键名’]| |array|数组，[‘11’,’222’,’33’],[1],表示拿到第二个元素|1234567891011121314151617181920212223242526272829## 数据John Doe,100000.0,Mary Smith_Todd Jones,Federal Taxes:0.2_State Taxes:0.05_Insurance:0.1,1 Michigan Ave._Chicago_1L_60600Tom Smith,90000.0,Jan_Hello Ketty,Federal Taxes:0.2_State Taxes:0.05_Insurance:0.1,Guang dong._China_0.5L_60661## 创建表CREATE TABLE employees(name STRING,sa1ary FLOAT,subordinates ARRAY&lt;STRING&gt;,deductions MAP&lt;STRING, FLOAT&gt;,address STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;)ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','COLLECTION ITEMS TERMINATED BY '_'MAP KEYS TERMINATED BY ':'LINES TERMINATED BY '\n';## 查询hive (list)&gt; select * from employees;OKemployees.name employees.sa1ary employees.subordinates employees.deductions employees.addressJohn Doe 100000.0 ["Mary Smith","Todd Jones"] &#123;"Federal Taxes":0.2,"State Taxes":0.05,"Insurance":0.1&#125; &#123;"street":"1 Michigan Ave.","city":"Chicago","state":"1L","zip":60600&#125;Tom Smith 90000.0 ["Jan","Hello Ketty"] &#123;"Federal Taxes":0.2,"State Taxes":0.05,"Insurance":0.1&#123;"street":"Guang dong.","city":"China","state":"0.5L","zip":60661&#125;Time taken: 0.131 seconds, Fetched: 2 row(s)hive (list)&gt; select subordinates[1], deductions['Federal Taxes'],address.city from list.employees;OK_c0 _c1 cityTodd Jones 0.2 ChicagoHello Ketty 0.2 ChinaTime taken: 0.121 seconds, Fetched: 2 row(s)类型之间的转换和Java类型，有自动类型转换，int——&gt;double强制转换：castSQL 语法 SQL 不区分大小写 SQL 可以写成一行也可以写成多行 SQL 的关键字不能分行来写，不能拆开，不能简写 增加缩进 SQL 不要全部写成一行，字句一定要分开写 DDL 的简单操作创建数据库: 在HDFS上创建数据库：warehouse/wangguo.db 1hive (bigdata)&gt; create database wangguo; 建议：以后加上if not exists判断 12345 hive (bigdata)&gt; create database wangguo;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database wangguo already existshive (bigdata)&gt; create database if not exists wangguo;OKTime taken: 0.011 seconds ![image_1ckf1iom41vkp1h0dm0f9j2oil9.png-40.3kB][1] 修改数据库 : alter可以修改数据库的属性 1hive (bigdata)&gt; alter database wangguo set dbproperties('createtime'='20180808'); 注意:数据库中的一些属性可以修改，但是数据库的名字和数据库所在目录不可更改。 查看数据库 : 1234567891011121314151617181920212223242526##显示所有的数据库hive (default)&gt; show databases; OK database_name bigdata default lisi wangguo Time taken: 4.259 seconds, Fetched: 4 row(s) ## 条件查询数据库 hive (default)&gt; show databases like 'li*'; OK database_name lisi Time taken: 0.047 seconds, Fetched: 1 row(s) ## 查询数据库的详细信息 hive (default)&gt; desc database bigdata; OK db_name comment location owner_name owner_type parameters bigdata hdfs://master:9000/hive/warehouse/bigdata.db root USER Time taken: 0.026 seconds, Fetched: 1 row(s) ## 使用数据库 hive (default)&gt; use bigdata; OK Time taken: 0.02 seconds hive (bigdata)&gt; 删除数据库 : 1234567891011121314151617181920212223242526## 删除数据库 hive (bigdata)&gt; drop database if exists lisi; OK Time taken: 0.19 seconds ## 再次查看所有数据库 hive (bigdata)&gt; show databases; OK database_name bigdata default wangguo Time taken: 0.009 seconds, Fetched: 3 row(s) ## 删除非空数据库 hive (bigdata)&gt; drop database bigdata; FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database bigdata is not empty. One or more tables exist.) ## 强制删除非空数据库 hive (bigdata)&gt; drop database bigdata cascade; OK Time taken: 1.539 seconds## 再次查看所有数据库 hive (bigdata)&gt; show databases; OK database_name default wangguo Time taken: 0.009 seconds, Fetched: 2 row(s) 创建表 : [建表语法][2] 1234567891011121314151617CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name [(col_name data_type [COMMENT col_comment], ... )] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] [AS select_statement]; 说明： CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 COMMENT：为表和列添加注释。 PARTITIONED BY 创建分区表 CLUSTERED BY 创建分桶表 SORTED BY 不常用 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。 STORED AS 指定存储文件类型常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 LOCATION ：指定表在 HDFS 上的存储位置。 LIKE 允许用户复制现有的表结构，但是不复制数据。 TBLPROPERTIES表的属性 AS select_statement将其他查询结果作为这个表的数据导入 内部表默认创建的表都是所谓的内部表，有时也被称为管理表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项 hive.metastore.warehouse.dir(例如，/hive/warehouse)所定义的目录的子目录下。 当我们删除一个内部表时，Hive 也会删除这个表中数据。内部表不适合和其他工具共享数据。创建内部表 12345678910111213141516171819202122232425262728293031323334## 创建表基本语法hive (wangchen)&gt; use wangchen;OKTime taken: 0.034 secondshive (wangchen)&gt; create table if not exists teacher(id int,name string) &gt; row format delimited fields terminated by '\t' &gt; stored as textfile &gt; location '/wang/wangchen/teacher';OKTime taken: 0.11 secondshive (wangchen)&gt; show tables;OKtab_nameteacherTime taken: 0.026 seconds, Fetched: 1 row(s)## 根据查询结果创建表hive (wangchen)&gt; create table if not exists student2 as select * from student;Total MapReduce CPU Time Spent: 1 seconds 980 msecOKstudent.id student.nameTime taken: 35.467 secondshive (wangchen)&gt; select * from student2;OKstudent2.id student2.name10001 zhangsan10002 lisi10003 kongchenlei10004 zhoushengnanTime taken: 0.442 seconds, Fetched: 4 row(s)## 根据已经存在的表创建表create table if not exists student2 as select * from student;## 查询表的类型desc formatted student3;外部表创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。 创建外部表 12345678910111213141516171819202122232425262728293031## 创建表的基本语法hive (wangchen)&gt; create external table area(id int,aname string) &gt; row format delimited fields terminated by '\t';OKTime taken: 0.092 seconds## 导入数据hive (wangchen)&gt; load data local inpath '/opt/test/student' into table area;Loading data to table wangchen.areaOKTime taken: 0.626 seconds## 查看数据hive (wangchen)&gt; select * from area;OKarea.id area.aname10001 zhangsan10002 lisi10003 kongchenlei10004 zhoushengnanTime taken: 0.201 seconds, Fetched: 4 row(s)## 查询表的类型hive (wangchen)&gt; desc formatted area;## 删除表hive (wangchen)&gt; drop table area;OKTime taken: 0.095 secondshive (wangchen)&gt; drop table student2;Moved: 'hdfs://master:9000/hive/warehouse/wangchen.db/student2' to trash at: hdfs://master:9000/user/root/.Trash/CurrentOKTime taken: 0.125 seconds分区表分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。创建分区表 12345678910111213141516171819202122## 创建分区表的基本语法hive&gt; create table student(id int,name string)&gt; partitioned by (day string comment 'dddd')&gt; row format delimited fields terminated by '\t';## 加载数据到分区表中hive&gt; load data local inpath 'wuqing' into table student partition (day='03');## 查询分区表中数据-单分区hive&gt; select * from student where day='02';## 查询分区表中数据-多分区## 增加分区hive&gt; alter table student add partition(day='05');## 增加多分区## 删除分区hive&gt; alter table student drop partition (day='03');## 查看分区## 查看分区表的结构hive&gt; desc formatted student;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## 创建二级分区hive&gt; create table teacher(id int,name string)&gt; partitioned by(month string,day string)&gt; row format delimited fields terminated by '\t';## 加载数据hive&gt; load data local inpath 'wuqing' into table teacher partition(month='11',day='02');## 查询hive&gt; select * from teacher;## 分区数据关联的三种方式## 方式一：先上传，后修复hive&gt; dfs -put wuqing /user/hive/warehouse/bigdata.db/student/day=04;hive&gt; select * from student;OK1001 qwqwerq 021002 adfasd 021003 sadfas 021001 qwqwerq 051002 adfasd 051003 sadfas 051001 qwqwerq 051002 adfasd 051003 sadfas 05Time taken: 0.151 seconds, Fetched: 9 row(s)# 执行修复hive&gt; msck repair table student;OKPartitions not in metastore: student:day=04Repair: Added partition to metastore student:day=04Time taken: 0.368 seconds, Fetched: 2 row(s)hive&gt; select * from student;OK1001 qwqwerq 021002 adfasd 021003 sadfas 021001 qwqwerq 041002 adfasd 041003 sadfas 041001 qwqwerq 051002 adfasd 051003 sadfas 051001 qwqwerq 051002 adfasd 051003 sadfas 05Time taken: 0.178 seconds, Fetched: 12 row(s)## 方式二：先上传，后修改hive&gt; dfs -mkdir /user/hive/warehouse/bigdata.db/student/day=06;hive&gt; dfs -put wuqing /user/hive/warehouse/bigdata.db/student/day=06;hive&gt; alter table student add partition(day='06');## 方式三：先上传，后加载修改表12345678## 表的重命名hive&gt; alter table student rename to student100;## 添加列hive&gt; alter table student100 add columns(haha string);## 更改列：是将某一列更改了hive&gt; alter table student100 change column hehe haha string;## 替换列：后面跟的是新的列名hive&gt; alter table student100 replace columns(ids int,names string);删除表12## 删除操作dropDML操作数据导入: Load模式 1LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] - load data:表示加载数据 - local:表示从本地加载数据到 hive 表；否则从 HDFS 加载数据到 hive 表 - inpath:表示加载数据的路径 - into table:表示加载到哪张表 - tablename:表示具体的表 - overwrite:表示覆盖表中已有数据，否则表示继续添加 - partition:表示上传到指定分区 123456789 ## 加载数据到Hive hive (bigdata)&gt; create table student(id int,name string) row format delimited fields terminated by '\t'; OKTime taken: 0.495 secondshive (bigdata)&gt; load data local inpath '/opt/test/student' into table bigdata.student;Loading data to table bigdata.studentOKTime taken: 1.131 secondshive (bigdata)&gt; dfs -put /opt/test/student /aaa.txt; insert模式 12INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement; 123456789101112## 基本insert intohive (bigdata)&gt; insert into table student values('10006','lallalalalalla');hive (bigdata)&gt; insert into table student select * from wangchen.teacher;## 基本的insert overwritehive (bigdata)&gt; insert overwrite table student select * from wangchen.teacher;## 将一个表中的数据插入到多个表中hive (bigdata)&gt; from wangchen.teacher &gt; insert overwrite table student select * &gt; insert overwrite table student1 select *；hive (bigdata)&gt; from student &gt; insert overwrite table student_part partition(day='20180813') select * &gt; insert overwrite table student_part partition(day='20180814') select * ; as select模式 1hive (bigdata)&gt; create table student2 as select * from student; Location模式 1234567891011121314151617 hive (bigdata)&gt; create table student3(id int,name string) &gt; row format delimited fields terminated by '\t' &gt; location '/haha/lala/hehe';OKTime taken: 0.059 secondshive (bigdata)&gt; select * from student3;OKstudent3.id student3.nameTime taken: 0.105 secondshive (bigdata)&gt; dfs -put /opt/test/student /haha/lala/hehe;hive (bigdata)&gt; select * from student3;OKstudent3.id student3.name10001 zhangsan10002 lisi10003 kongchenlei10004 zhoushengnan Import模式 12## 导入的路径必须是export的路径hive (bigdata)&gt; import table student from '/opt/export'; 数据导出: insert模式 123456789101112## 导出到本地hive (bigdata)&gt; insert overwrite local directory '/opt/test/export' select * from student1;hive (bigdata)&gt; insert overwrite local directory '/opt/test/export' &gt; row format delimited fields terminated by '\t' &gt; collection items terminated by '\n' &gt; select * from student1;## 导出到HDFS上hive (bigdata)&gt; insert overwrite directory '/opt/test/export' &gt; row format delimited fields terminated by '\t' &gt; collection items terminated by '\n' &gt; select * from student1; export模式 12## 导出路径必须为不存在的hive (bigdata)&gt; export table student1 to '/opt/export/'; dfs -get模式 1hive (bigdata)&gt; dfs -get /hive/warehouse/bigdata.db/student1/000000_0 /opt/test/export/aaa.txt; hive -e 模式 1[root@master export]# hive -e 'select * from bigdata.student1;' &gt; /opt/test/export/e.txt; Sqoop 导出 数据删除注意：Truncate 只能删除管理表(内部表)，不能删除外部表中数据 1hive (bigdata)&gt; truncate table student1;DQL操作基本查询12345678910111213141516 hive (bigdata)&gt; select * from student3;OKstudent3.id student3.name10001 zhangsan10002 lisi10003 kongchenlei10004 zhoushengnanTime taken: 0.121 seconds, Fetched: 4 row(s)hive (bigdata)&gt; select id from student3;OKid10001100021000310004Time taken: 0.12 seconds, Fetched: 4 row(s)常用函数12345678+ - * / % &amp; | ^ ~hive (bigdata)&gt; select score+10 from chengji;hive (bigdata)&gt; select count(*) count from chengji;hive (bigdata)&gt; select max(score) max_score from chengji;hive (bigdata)&gt; select sum(score) sum_score from chengji;hive (bigdata)&gt; show functions;hive (bigdata)&gt; select * from chengji limit 4;hive (bigdata)&gt; select * from chengji limit 3,4;条件查询: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869hive (bigdata)&gt; select * from chengji where score &gt; 60;OKchengji.id chengji.name chengji.kemu chengji.score1001 张三 语文 991002 李四 语文 881003 张三 数学 881004 王五 英语 881005 哈哈 数学 1001006 李四 数学 941008 李四 英语 100hive (bigdata)&gt; select * from chengji where score&gt;50 and score&lt;80;OKchengji.id chengji.name chengji.kemu chengji.score1012 哈哈 语文 60Time taken: 0.126 seconds, Fetched: 1 row(s)hive (bigdata)&gt; select * from chengji where score between 50 and 80;OKchengji.id chengji.name chengji.kemu chengji.score1012 哈哈 语文 60Time taken: 0.106 seconds, Fetched: 1 row(s)hive (bigdata)&gt; select * from chengji;OKchengji.id chengji.name chengji.kemu chengji.score1001 张三 语文 991002 李四 语文 881003 张三 数学 881004 王五 英语 881005 哈哈 数学 1001006 李四 数学 941007 张三 英语 201008 李四 英语 1001009 王五 语文 401010 王五 数学 201011 哈哈 英语 441012 哈哈 语文 601013 lala hehe NULLTime taken: 0.105 seconds, Fetched: 13 row(s)hive (bigdata)&gt; select * from chengji where score is null;OKchengji.id chengji.name chengji.kemu chengji.score1013 lala hehe NULLTime taken: 0.11 seconds, Fetched: 1 row(s)hive (bigdata)&gt; select * from chengji where score in(88,44);OKchengji.id chengji.name chengji.kemu chengji.score1002 李四 语文 881003 张三 数学 881004 王五 英语 881011 哈哈 英语 44Time taken: 0.105 seconds, Fetched: 4 row(s)hive (bigdata)&gt; select * from chengji where score in(88,44,66);OKchengji.id chengji.name chengji.kemu chengji.score1002 李四 语文 881003 张三 数学 881004 王五 英语 881011 哈哈 英语 44Time taken: 0.109 seconds, Fetched: 4 row(s)hive (bigdata)&gt; select * from chengji where score in(88,44,99);OKchengji.id chengji.name chengji.kemu chengji.score1001 张三 语文 991002 李四 语文 881003 张三 数学 881004 王五 英语 881011 哈哈 英语 44Time taken: 0.105 seconds, Fetched: 5 row(s)模糊查询like,Rlike- % : 代表任意字符（任意多个或0个） - _ : 表示一个字符 Rlike： Hive中对like的一个扩展，可以直接使用Java中正则表达式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051hive (bigdata)&gt; select * from chengji where score like '9%';OKchengji.id chengji.name chengji.kemu chengji.score1001 张三 语文 991006 李四 数学 94Time taken: 0.094 seconds, Fetched: 2 row(s)hive (bigdata)&gt; select * from chengji where score like '_9%';OKchengji.id chengji.name chengji.kemu chengji.score1001 张三 语文 99Time taken: 0.117 seconds, Fetched: 1 row(s)hive (bigdata)&gt; select * from chengji where id like '__1%';OKchengji.id chengji.name chengji.kemu chengji.score1010 王五 数学 201011 哈哈 英语 441012 哈哈 语文 601013 lala hehe NULLTime taken: 0.125 seconds, Fetched: 4 row(s)hive (bigdata)&gt; select * from chengji where id like '%2%';OKchengji.id chengji.name chengji.kemu chengji.score1002 李四 语文 881012 哈哈 语文 60Time taken: 0.142 seconds, Fetched: 2 row(s)hive (bigdata)&gt; select * from chengji where id like '%1%';OKchengji.id chengji.name chengji.kemu chengji.score1001 张三 语文 991002 李四 语文 881003 张三 数学 881004 王五 英语 881005 哈哈 数学 1001006 李四 数学 941007 张三 英语 201008 李四 英语 1001009 王五 语文 401010 王五 数学 201011 哈哈 英语 441012 哈哈 语文 601013 lala hehe NULLTime taken: 0.108 seconds, Fetched: 13 row(s)hive (bigdata)&gt; select * from chengji where id rlike '[1]';FAILED: SemanticException [Error 10016]: Line 1:28 Argument type mismatch 'id': regexp only takes STRING_GROUP types as 1st argument, got INThive (bigdata)&gt; select * from chengji where name rlike '[三]';OKchengji.id chengji.name chengji.kemu chengji.score1001 张三 语文 991003 张三 数学 881007 张三 英语 20Time taken: 0.094 seconds, Fetched: 3 row(s) 逻辑运算符的应用12345678910111213141516171819202122232425262728 hive (bigdata)&gt; select * from chengji where kemu='01' and score&gt;60; OK chengji.id chengji.name chengji.kemu chengji.score 1001 01 01 99 1002 02 01 88 Time taken: 0.118 seconds, Fetched: 2 row(s) hive (bigdata)&gt; select * from chengji where kemu='01' or score&gt;60;OKchengji.id chengji.name chengji.kemu chengji.score1001 01 01 991002 02 01 881003 01 02 881004 03 03 881005 04 02 1001006 02 02 941008 02 03 1001009 03 01 401012 04 01 60Time taken: 0.122 seconds, Fetched: 9 row(s)hive (bigdata)&gt; select * from chengji where kemu not in('01','02');OKchengji.id chengji.name chengji.kemu chengji.score1004 03 03 881007 01 03 201008 02 03 1001011 04 03 441013 lala hehe NULLTime taken: 0.097 seconds, Fetched: 5 row(s)分组查询: Group By语句GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。 12345678910111213141516171819202122232425hive (bigdata)&gt; select c.kemu,avg(c.score) avg_score from chengji c group by c.kemu;OKc.kemu avg_score01 71.7502 75.503 63.0hehe NULLhive (bigdata)&gt; select c.kemu,max(c.score) max_score from chengji c group by c.kemu;OKc.kemu max_score01 9902 10003 100hehe NULLTime taken: 26.277 seconds, Fetched: 4 row(s)## 每个人成绩最高的科目hive (bigdata)&gt; select name,max(score) from chengji group by name;name _c1lala NULL哈哈 100张三 99李四 100王五 88 Having 语句having 与 where 不同点 where 针对表中的列发挥作用，查询数据；having 针对查询结果中的列发挥作用，筛选数据。 where 后面不能写分组函数，而 having 后面可以使用分组函数。 having 只用于 group by 分组统计语句。 123456789101112131415hive (bigdata)&gt; select c.name,avg(c.score) avg_score from chengji c group by c.name;OKc.name avg_score01 69.002 94.003 49.33333333333333604 68.0lala NULLTime taken: 27.018 seconds, Fetched: 5 row(s)hive (bigdata)&gt; select c.name,avg(c.score) avg_score from chengji c group by c.name having avg_score&gt;68;OKc.name avg_score01 69.002 94.0Time taken: 28.275 seconds, Fetched: 2 row(s) join操作Hive 支持通常的 SQL JOIN 语句，但是只支持等值连接，不支持非等值连接。 12345678910111213141516171819202122## 使用join查询员工的信息：ID，姓名，部门名字，职位，薪资hive (bigdata)&gt; select y.id,y.name,b.name,y.zhiwei,y.xinzi from yuan y &gt; join bumen b on y.bumen=b.id;## 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会显示出来。## 左外连接：将左侧都显示，右侧不满足条件的替换为nullhive (bigdata)&gt; select y.id,y.name,b.name,y.zhiwei,y.xinzi from yuan y &gt; left join bumen b on y.bumen=b.id;## 右外连接：将右侧显示，左侧不满足条件的替换为nullhive (bigdata)&gt; select y.id,y.name,b.name,y.zhiwei,y.xinzi from yuan y &gt; right join bumen b on y.bumen=b.id;## 满外链接：两个表都显示，不满足条件的都替换为nullhive (bigdata)&gt; select y.id,y.name,b.name,y.zhiwei,y.xinzi from yuan y &gt; full join bumen b on y.bumen=b.id;## 笛卡尔积连接（不要使用）hive (bigdata)&gt; set hive.mapred.mode='strict';hive (bigdata)&gt; set hive.strict.checks.cartesian.product=false;hive (bigdata)&gt; select y.id,y.name,b.name,y.zhiwei,y.xinzi from yuan y &gt; join bumen b;排序查询全排序（这个结果排序），部分排序（区内排序），二次排序（compareTo），辅助排序（组内排序） 12345678910111213141516171819## 成绩表按照成绩升序排序（asc）hive (bigdata)&gt; select * from chengji order by score;## 成绩表按照成绩降序排序（desc）hive (bigdata)&gt; select * from chengji order by score desc;## 成绩表按照多列进行排序hive (bigdata)&gt; select * from chengji order by score,name;hive (bigdata)&gt; select * from chengji order by score desc,name asc;## 部分内部排序（sort by）hive (bigdata)&gt; set mapreduce.job.reduces=4;hive (bigdata)&gt; select * from chengji sort by score desc;## 分区排序## distribute by：类似于MR中分区，先分区后排序.必须结合sort by 使用hive (bigdata)&gt; select * from chengji distribute by kemu sort by score desc;## cluster by## distribute by+ sort by 可以使用cluster by（只能按照升序）代替（排序的字段和分区的字段是一样的）hive (bigdata)&gt; select * from chengji cluster by kemu;分桶操作分区针对的是数据的存储目录；分桶针对的是数据文件。分区分区提供的是一级一级的目录，这个划分并没有任何的数据大小的限制，会导致某一个分区的数据量很大，某一个分区的数据量很小。分桶分桶是将数据文件直接分成若干份 分桶表数据存储 12345678910## 创建分桶表hive (bigdata)&gt; create table student_bucket(id int,name string) &gt; clustered by(id) into 4 buckets &gt; row format delimited fields terminated by '\t';## 直接导入数据，会发现并未分桶hive (bigdata)&gt; load data local inpath '/opt/test/student' into table student_bucket;## 解决方案，设置并使用insert方式导入数据hive (bigdata)&gt; set hive.enforce.bucketing=true;hive (bigdata)&gt; insert into table student_bucket select * from student cluster by(id); 分桶抽样查询对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive 可以通过对表进行抽样来满足这个需求。select * from tablename tablesample(bucket x out of y on id);y 必须是 table 总 bucket 数的倍数或者因子。hive 根据 y 的大小，决定抽样的比例。例如，table 总共分了 4 份，当 y=2 时，抽取(4/2=)2 个 bucket 的数据，当 y=8 时，抽取(4/8=)1/2个 bucket 的数据。x 表示从哪个 bucket 开始抽取。例如，table 总 bucket 数为 4，tablesample(bucket 4 out of 4)，表示总共抽取（4/4=）1 个 bucket 的数据，抽取第 4 个 bucket 的数据。注意：x 的值必须小于等于 y 的值，否则会报错 123456hive (bigdata)&gt; select * from student_bucket tablesample(bucket 2 out of 4 on id);hive (bigdata)&gt; select * from chengji_buck tablesample(bucket 4 out of 2 on id);hive (bigdata)&gt; select * from chengji_buck tablesample(bucket 1 out of 2 on id);hive (bigdata)&gt; select * from chengji_buck tablesample(bucket 4 out of 4 on id);hive (bigdata)&gt; select * from chengji_buck tablesample(bucket 4 out of 8 on id);hive (bigdata)&gt; select * from chengji_buck tablesample(bucket 1 out of 8 on id); 百分比抽样Hive 提供了另外一种按照百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行的抽样。注意：这种抽样方式不一定适用于所有的文件格式。另外，这种抽样的最小抽样单元是一个 HDFS 数据块。因此，如果表的数据大小小于普通的块大小 128M 的话，那么将会返回所有行。 函数123456## 查看所有的函数hive (bigdata)&gt; show functions;## 如何查看函数的用法hive (bigdata)&gt; desc function avg;## 查看更详细的信息hive (bigdata)&gt; desc function extended avg; 自定义函数Hive 本身有一些函数，但是实际应用过程中，这些函数还不够，可能需要自定义一些便于使用。UDF：User-defined-functions：一进一出UDAF: user-defined-aggregation-functions聚集函数：多进一出UDTF：user-defined-table generating -functions：一进多出如何自定义函数: 创建一个类让这个类继承UDF 写一个方法：evaluate 将写完的程序打成jar包发送发HIve的lib目录 将jar包添加到Hive的CLASSPATH中 1hive (bigdata)&gt; add jar /opt/apps/Hive/hive-2.3.3/lib/myudf.jar; 创建函数与java 的class进行关联 1hive (bigdata)&gt; create temporary function myLower as "com.zhiyou100.hadoop.hive.udf.Lower"; 应用这个函数 1hive (bigdata)&gt; select name,mylower(name) from test_udf; 如何自定义UDAF: 压缩存储开启 Map 输出阶段压缩开启 map 输出阶段压缩可以减少 job 中 map 和 Reduce task 间数据传输量。具体配置如下：123456## 开启 hive 中间传输数据压缩功能set hive.exec.compress.intermediate=true;## 开启 mapreduce 中 map 输出压缩功能set mapreduce.map.output.compress=true;## 设置 mapreduce 中 map 输出数据的压缩方式set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec; 开启 Reduce 输出阶段压缩当 Hive 将输出写入到表中时，输出内容同样可以进行压缩。属性 hive.exec.compress.output 控制着这个功能。用户可能需要保持默认设置文件中的默认值 false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为 true，来开启输出结果压缩功能。12345678## 开启 hive 最终输出数据压缩功能set hive.exec.compress.output=true;## 开启 mapreduce 最终输出数据压缩set mapreduce.output.fileoutputformat.compress=true;## 设置 mapreduce 最终数据输出压缩方式set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;## 设置 mapreduce 最终数据输出压缩为块压缩set mapreduce.output.fileoutputformat.compress.type=BLOCK; 文件存储格式Hive 支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。列式存储和行式存储 ![image_1cknb37dhsk0m3v1o0hj0kpht1m.png-62.7kB][3]行存储的特点： 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。列存储的特点： 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；ORC和PARQUET是基于列式存储的。1. TEXTFILE格式 默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。 2. ORC格式 Orc (Optimized Row Columnar)是 hive 0.11 版里引入的新的存储格式。 每个 Orc 文件由 1 个或多个 stripe 组成，每个 stripe250MB 大小，这个 Stripe 实际相当于 RowGroup 概念，不过大小由 4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个 Stripe 里有三部分组成，分别是 Index Data,Row Data,Stripe Footer： ![image_1ckpt5kq5133k17101v331tfacj919.png-144kB][4] - Index Data：一个轻量级的 index。这里做的索引应该只是记录某行的各字段在 Row Data 中的 offset。 - Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个 Stream 来存储。 - Stripe Footer：存的是各个 Stream 的类型，长度等信息。每个文件有一个 File Footer，这里面存的是每个 Stripe 的行数，每个 Column 的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter 的长度信息等。在读取文件时，会 seek 到文件尾部读 PostScript，从里面解析到 File Footer 长度，再读 FileFooter，从里面解析到各个 Stripe 信息，再读各个 Stripe，即从后往前读。 3. PARQUET格式 Parquet 是面向分析型业务的列式存储格式，由 Twitter 和 Cloudera 合作开发，2015 年 5 月从 Apache 的孵化器里毕业成为 Apache 顶级项目。 Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此 Parquet 格式文件是自解析的。 ![image_1ckptj64a1qic1h8bcstkdcbgr1m.png-69.3kB][5]存储文件格式压缩比较: 上传文件到master 创建表：文件格式为TEXTFILE 123456 hive (bigdata)&gt; create table log_textfile(time string,url string,session_id string,referer string,ip string,user_id string,city_id string) &gt; row format delimited fields terminated by '\t' &gt; stored as textfile;hive (bigdata)&gt; load data local inpath '/opt/test/log.data' into table log_textfile; hive (bigdata)&gt; dfs -du -h /hive/warehouse/bigdata.db/log_textfile/log.data; 18.1 M /hive/warehouse/bigdata.db/log_textfile/log.data 创建表：文件格式为ORC 123456hive (bigdata)&gt; create table log_orc(time string,url string,session_id string,referer string,ip string,user_id string,city_id string) &gt; row format delimited fields terminated by '\t' &gt; stored as orc;hive (bigdata)&gt; insert into table log_orc select * from log_textfile;hive (bigdata)&gt; dfs -du -h /hive/warehouse/bigdata.db/log_orc/;2.8 M /hive/warehouse/bigdata.db/log_orc/000000_0 创建表：文件格式为PARQUET123456 hive (bigdata)&gt; create table log_parquet(time string,url string,session_id string,referer string,ip string,user_id string,city_id string) &gt; row format delimited fields terminated by '\t' &gt; stored as parquet; hive (bigdata)&gt; insert into table log_parquet select * from log_textfile; hive (bigdata)&gt; dfs -du -h /hive/warehouse/bigdata.db/log_parquet/;13.1 M /hive/warehouse/bigdata.db/log_parquet/000000_0 存储文件的压缩比总结：ORC &gt; Parquet &gt; textFile 文件格式的查询效率123456789## textfilehive (bigdata)&gt; select count(*) from log_textfile;Time taken: 28.759 seconds, Fetched: 1 row(s)## orchive (bigdata)&gt; select count(*) from log_orc;Time taken: 0.122 seconds, Fetched: 1 row(s)## parquethive (bigdata)&gt; select count(*) from log_parquet;Time taken: 0.113 seconds, Fetched: 1 row(s)Hive优化Fetch 抓取Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。例如：SELECT * FROM student;在这种情况下，Hive 可以简单地读取 student 对应的存储目录下的文件，然后输出查询结果到控制台。hive.fetch.task.conversion属性的设置该属性默认为 more 以后，在全局查找、字段查找、limit 查找等都不走MR本地模式大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过，有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际 job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。hive.exec.mode.local.auto属性。让 Hive 在适当的时候自动启动这个优化123456## 开启本地模式set hive.exec.mode.local.auto=true; ## 设置本地模式的最大输入数据量，当输入数据量小于这个值时采用本地模式，默认为 134217728，即 128Mset hive.exec.mode.local.auto.inputbytes.max=1024000;## 设置本地模式的最大输入文件个数，当输入文件个数小于这个值时采用本地模式，默认为 4set hive.exec.mode.local.auto.input.files.max=4; 表的优化: 小表、大表 Join将 key 相对分散，并且数据量小的表放在 join 的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用 Group 让小的维度表（1000 条以下的记录条数）先进内存。在 map 端完成 reduce。但是在新版的 hive 已经对小表 JOIN 大表和大表 JOIN 小表进行了优化。小表放在左边和右边已经没有明显区别。 大表 Join 大表 空 KEY 过滤有时 join 超时是因为某些 key 对应的数据太多，而相同 key 对应的数据都会发送到相同的 reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下，这些 key 对应的数据是异常数据，我们需要在 SQL 语句中进行过滤。例如 key 对应的字段为空， 1234 ## 过滤空key select * from table1 t1 left join table2 t2 on t1.id = t2.id; select * from (select * from table1 where id is not null) n left join table2 t2 on n.id = t2.id; 空 key 转换有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在 join 的结果中，此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。 123456select * from table1 t1 full join table2 t2 on case when t1.id is null then concat('hahahaha',rand()) else t1.id = t2.id; MapJoin如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在 Reduce 阶段完成 join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 map 端进行 join，避免 reducer 处理。hive.auto.convert.join属性设置大表小表的阀值设置（默认 25M 一下认为是小表）hive.mapjoin.smalltable.filesize Group By默认情况下，Map 阶段同一 Key 数据分发给一个 reduce，当一个 key 数据过大时就倾斜了。并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果。开启 Map 端聚合参数设置 123456## 是否在 Map 端进行聚合，默认为 Truehive.map.aggr = true## 在 Map 端进行聚合操作的条目数目hive.groupby.mapaggr.checkinterval = 100000## 有数据倾斜的时候进行负载均衡（默认是 false）,当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。hive.groupby.skewindata = true Count(Distinct) 去重统计数据量小的时候无所谓，数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个 Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成，一般 COUNT Distinct 使用先 GROUP BY 再 COUNT 的方式替换： 笛卡尔积尽量避免笛卡尔积，join 的时候不加 on 条件，或者无效的 on 条件，Hive 只能使用 1 个 reducer 来完成笛卡尔积 行列过滤列处理：在 SELECT 中，只拿需要的列，如果有，尽量使用分区过滤，少用 SELECT *。行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 Where 后面，那么就会先全表关联，之后再过滤，不好。应该先过滤再关联 12345## 这种方式不好select * from table1 t1 join table2 t2 on t1.id = t2.id where t2.id&lt;100## 下面的方式较优select * from table1 t1 join (select id from table2 where id&lt;100) n on t1.id=n.id 动态分区调整关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用 Hive 的动态分区，需要进行相应的配置。开启动态分区参数设置 123456789101112## 开启动态分区功能（默认 true，开启）hive.exec.dynamic.partition=true## 设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。）hive.exec.dynamic.partition.mode=nonstrict## 在所有执行 MR 的节点上，最大一共可以创建多少个动态分区。hive.exec.max.dynamic.partitions=1000## 在每个执行 MR 的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错。hive.exec.max.dynamic.partitions.pernode=100## 整个 MR Job 中，最大可以创建多少个 HDFS 文件。hive.exec.max.created.files=100000## 当有空分区生成时，是否抛出异常。一般不需要设置。hive.error.on.empty.partition=false 数据倾斜: 合理设置 Map 数 通常情况下，作业会通过 input 的目录产生一个或者多个 map 任务。主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。 是不是 map 数越多越好？答案是否定的。如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。 是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？答案也是不一定。比如有一个 127m的文件，正常会用一个 map 去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果 map 处理的逻辑比较复杂，用一个 map 任务去做，肯定也比较耗时。 小文件进行合并在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat 默认有对小文件合并功能。set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 复杂文件增加 Map 数当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。增加 map 的方法为：根据 computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M 公式，调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。 合理设置 Reduce 数 每个 Reduce 处理的数据量默认是 256MB（参数1）hive.exec.reducers.bytes.per.reducer=256000000 每个任务最大的 reduce 数，默认为 1009（参数2）hive.exec.reducers.max=1009 计算 reducer 数的公式N=min(参数 2，总输入数据量/参数 1) reduce 个数并不是越多越好 过多的启动和初始化 reduce 也会消耗时间和资源； 另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；在设置 reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 reduce 数；使单个 reduce 任务处理数据量大小要合适； 并行执行Hive 会将一个查询转化成一个或者多个阶段。这样的阶段可以是 MapReduce 阶段、抽样阶段、合并阶段、limit 阶段。或者 Hive 执行过程中可能需要的其他阶段。默认情况下，Hive 一次只会执行一个阶段。不过，某个特定的 job 可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个 job 的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么 job 可能就越快完成。通过设置参数 hive.exec.parallel 值为 true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果 job 中并行阶段增多，那么集群利用率就会增加。 1234## 打开任务并行执行set hive.exec.parallel=true; ## 同一个 sql 允许最大并行度，默认为 8。set hive.exec.parallel.thread.number=10;当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。 严格模式Hive 提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。JVM 重用JVM 重用是 Hadoop 调优参数的内容，其对 Hive 的性能具有非常大的影响，特别是对于很难避免小文件的场景或 task 特别多的场景，这类场景大多数执行时间都很短。Hadoop 的默认配置通常是使用派生 JVM 来执行 map 和 Reduce 任务的。这时 JVM 的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM 重用可以使得 JVM 实例在同一个 job 中重新使用 N 次。N 的值可以在 Hadoop 的 mapred-site.xml 文件中进行配置。通常在 10-20 之间，具体多少需要根据具体业务场景测试得出。mapreduce.job.jvm.numtasks这个功能的缺点是，开启 JVM 重用将一直占用使用到的 task 插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job 中有某几个 reduce task 执行的时间要比其他 Reduce task 消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的 job 使用，直到所有的 task 都结束了才会释放。推测执行: 执行计划用于帮助分析SQL语句 基本语法EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query 查看下面这条语句的执行计划hive (default)&gt; explain select * from student; 面试题order by，sort by，distribute by，cluster by的区别: order by会对输入做全局排序，因此只有一个Reducer(多个Reducer无法保证全局有序)，然而只有一个Reducer，会导致当输入规模较大时，消耗较长的计算时间。 sort by不是全局排序，其在数据进入reducer前完成排序，因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只会保证每个reducer的输出有序，并不保证全局有序。sort by不同于order by，它不受hive.mapred.mode属性的影响，sort by的数据只能保证在同一个reduce中的数据可以按指定字段排序。使用sort by你可以指定执行的reduce个数(通过set mapred.reduce.tasks=n来指定或者reduces)，对输出的数据再执行归并排序，即可得到全部结果。 distribute by 是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。sort by为每个reduce产生一个排序文件。在有些情况下，你需要控制某个特定行应该到哪个reducer，这通常是为了进行后续的聚集操作。distribute by刚好可以做这件事。因此，distribute by经常和sort by配合使用。注：Distribute by和sort by的使用场景 Map输出的文件大小不均。 Reduce输出文件大小不均。 小文件过多。 文件超大。 cluster by 除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒叙排序，不能指定排序规则为ASC或者DESC。 行列转换: 行转列 name constellation blood_type 孙悟空 白羊座 A 大海 射手座 A 宋宋 白羊座 B 猪八戒 白羊座 A 凤姐 射手座 A 需求：把星座和血型一样的人归类到一起 12345678910111213## 将后两列的数据拼接起来，concat('','','',''),比如concat('A','B','C') = ABCselect name,concat(star,"-",blood) sb from person## concat_ws('A','B','C','D')=BACADselect t2.sb,concat_ws(',',collect_set(t2.name)) from (select name,concat(star,"-",blood) sb from person ) t2 group by t2.sb; 列转行 movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 《Lie to me》 悬疑,警匪,动作,心理,剧情 《战狼 2》 战争,动作,灾难 需求：将电影分类中的数组数据展开 1234567891011select t1.t,concat_ws(',',collect_set(t1.name)) from ( select t,name from movie lateral view explode(type) table_tmp as t ) t1 group by t1.t; Hive的Java API操作本机模式简单来说，就是在Hive安装的机器上开启一个客户端，直接使用HQL语句进行Hive命令的操作，不需要指定端口号和IP地址远程模式简单来说，其实就是将装hive的机器看做一个服务器，通过IP和端口号来远程连接Hive，然后操作HQL语句。12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 12345678&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;!--（HiveServer2远程连接的端口，默认为10000）--&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;master&lt;/value&gt;&lt;!--（hive所在集群的IP地址）--&gt;&lt;/property&gt; 123先启动元数据库，在命令行中键入：hive --service metastore &amp;接下来开启hiveserver2服务：在命令行中键入：hive --service hiveserver2 &amp; 通过HiveServer或者HiveServer2，客户端可以在不启动CLI的情况下对Hive中的数据进行操作，两者都允许远程客户端使用多种编程语言如Java、Python向Hive提交请求，取回结果。HiveServer或者HiveServer2都是基于Thrift的，但HiveSever有时被称为Thrift server，而HiveServer2却不会。既然已经存在HiveServer为什么还需要HiveServer2呢？这是因为HiveServer不能处理多于一个客户端的并发请求，这是由于HiveServer使用的Thrift接口所导致的限制，不能通过修改HiveServer的代码修正。因此在Hive-0.11.0版本中重写了HiveServer代码得到了HiveServer2，进而解决了该问题。HiveServer2支持多客户端的并发和认证，为开放API客户端如JDBC、ODBC提供了更好的支持。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-sqoop]]></title>
    <url>%2F2019%2F04%2F25%2F1_Hadoop-sqoop%2F</url>
    <content type="text"><![CDATA[Hadoop-sqoopsqoop 简介算是一个Hadoop和其他关系型数据库存储之间的一个数据传输工具。 sqoop的原理通过shell命令，底层会将命令转换成MapReduce程序实现。主要针对InputFormat和outputformat sqoop的安装下载地址http://mirrors.hust.edu.cn/apache/sqoop/1.4.7/安装步骤: sqoop上传至集群的某一台机器 解压sqoop[root@master Sqoop]# tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 重命名[root@master Sqoop]# mv sqoop-1.4.7.bin__hadoop-2.6.0 sqoop-1.4.7 修改配置文件sqoop-env.sh(没有需要cp) 12345[root@master conf]# vim sqoop-env.sh添加以下内容export HIVE_HOME=/opt/apps/Hive/hive-2.3.3export HADOOP_COMMON_HOME=/opt/apps/Hadoop/hadoop-2.7.6export HADOOP_MAPRED_HOME=/opt/apps/Hadoop/hadoop-2.7.6 拷贝mysql的驱动到 sqoop lib目录 配置环境变量 123456[root@master lib]# vim /etc/profile添加以下内容#Sqoop环境变量export SQOOP_HOME=/opt/apps/Sqoop/sqoop-1.4.7export PATH=$PATH:$SQOOP_HOME/bin[root@master lib]# source /etc/profile 验证环境变量是否配置成功 123456[root@master lib]# sqoop version出现以下信息18/08/15 10:47:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7Sqoop 1.4.7git commit id 2328971411f57f0cb683dfb79d19d4d19d185dd8Compiled by maugli on Thu Dec 21 15:59:58 STD 2017 简单验证sqoop是否能够连接mysql数据库 1[root@master lib]# sqoop list-databases --connect jdbc:mysql:///?useSSL=false --username root --password 123456 数据的导入最简单的导入操作从关系型数据库向大数据集群HDFS，Hive，HBase中传输数据，叫做导入。使用import关键字 123456789[root@master lib]# sqoop import \--connect jdbc:mysql://master:3306/mysql_bigdata?useSSL=false \--username root \--password 123456 \--table student \--target-dir /hive/mysql_bigdata \--delete-target-dir \--num-mappers 1 \--fields-terminated-by "\t"查询导入通过SQL语句将查询结果导入到HDFS 123456789[root@master lib]# sqoop import \&gt; --connect jdbc:mysql://master:3306/mysql_bigdata?useSSL=false \&gt; --username root \&gt; --password 123456 \&gt; --target-dir /hive/mysql_bigdata1 \&gt; --delete-target-dir \&gt; --num-mappers 1 \&gt; --fields-terminated-by '\t' \&gt; --query 'select * from student where id=1001 and $CONDITIONS'**注意：** - 使用query选项就不能使用table - 使用where的时候必须加上and $CONDITIONS - 不要使用双引号导入指定的列就是通过sqoop（SQL HADOOP）直接将mysql中的表中的某些列数据直接导入HDFS 12345678910[root@master lib]# sqoop import \&gt; --connect jdbc:mysql://master:3306/mysql_bigdata?useSSL=false \&gt; --username root \&gt; --password 123456 \&gt; --table student \&gt; --target-dir /hive/mysql_bigdata2 \&gt; --delete-target-dir \&gt; --num-mappers 1 \&gt; --fields-terminated-by '\t' \&gt; --columns id注意：columns如果有多列，那么就使用逗号分隔(id,name,hahah)，分割时不要添加空格(id, name, hahah) 关键字筛选导入数据where的用法 12345678910[root@master ~]# sqoop import \&gt; --connect jdbc:mysql://master:3306/mysql_bigdata?useSSL=false \&gt; --username root \&gt; --password 123456 \&gt; --target-dir /hive/mysql_bigdata3 \&gt; --delete-target-dir \&gt; --fields-terminated-by '\t' \&gt; --table student \&gt; --where "id=1001" \&gt; -m 1数据导入到Hive: 1234567891011[root@master ~]# sqoop import --connect jdbc:mysql://master:3306/mysql_bigdata?useSSL=false --username root --password 123456 --table student -m 1 --hive-import --fields-terminated-by '\t' --hive-overwrite --delete-target-dir --hive-table mysql_import_student 注意:本过程分成两个步骤：第一步，将数据导入到HDFS上，第二步，创建表，然后将数据迁移到Hive库中 数据导出12345678[root@master ~]# sqoop export \&gt; --connect jdbc:mysql://master:3306/mysql_bigdata?useSSL=false \&gt; --username root \&gt; --password 123456 \&gt; --table student1 \&gt; -m 1 \&gt; --export-dir /hive/mysql_bigdata \&gt; --input-fields-terminated-by '\t' 注意：mysql中的表如果不存在，不会自动创建。数据是追加的。主键唯一的情况容易出现问题。 脚本打包使用opt文件打包sqoop的命令。opt文件内容 123456789101112131415export--connectjdbc:mysql://master:3306/mysql_bigdata?useSSL=false--usernameroot--password123456--tablestudent-m1--export-dir/hive/mysql_bigdata--input-fields-terminated-by'\t' 注意：原本要按照空格分隔的直接换行输入即可，不需要执行脚本 1[root@master sqoop]# sqoop --options-file job_HDFS2MYSQL.opt Sqoop常用命令 命令 描述 import 将数据导入到集群，HDFS，HIve，HBase export 将集群的数据导出到传统数据库中 job 用来生成一个sqoop任务的，生成后，该任务不执行，等待使用命令执行 list-databases 显示所有的数据库名字 list-tables 显示某个数据库下的所有的表的名字 codegen 获取数据库某张表数据生java 并且打包成jar import-all-tables 将某一个数据库下的所有的表导入到集群 merge 将HDFS下不同目录下的数据合并在一起，然后存放到指定目录 metastore 记录sqoop job的元数据信息，如果不启动metastore实例，可以在sqoop-site.xml中配置相关路径 create-hive-table 创建Hive表 eval 查看SQL的运行结果 import-mainframe 从其他服务器上导入数据到HDFS 参数详解公共参数：数据库连接|参数|描述||—|—||–connect|连接数据库的URL||–connection-manager|使用管理类||–driver|手动指定要使用的JDBC驱动程序类||–hadoop-mapred-home|覆盖$ HADOOP_MAPRED_HOME||–help|打印使用说明||–password-file|设置包含验证密码的文件的路径||-P|从控制台读取密码||–password|设置验证密码||–username|设置认证用户名||–verbose|工作时打印更多信息 公共参数：import|参数|描述| |—|—||–fields-terminated-by |设定每个字段以什么样的符号结果，默认为逗号||–lines-terminated-by |每一行以什么样的字符结束，默认为\n||–append|将数据附加到HDFS中的现有数据集||–as-textfile|以纯文本格式导入数据（默认）||–columns &lt;col,col,col…&gt;|要从表导入的列||–delete-target-dir|删除导入目标目录（如果存在）||–table |要阅读的表格||–target-dir |HDFS目的地目录||–where |导入期间要使用的WHERE子句||-z,–compress|启用压缩||–compression-codec |使用Hadoop编解码器（默认gzip）||-m,–num-mappers |使用n个 map任务并行导入||-e,–query | 导入结果statement。||–optionally-enclosed-by |给有双引号或者单引号的字段前后加上指定的字符||–enclosed-by|给字段的值前后加上指定的字符||–escaped-by|对字段中的双引号加转义符| 公用参数：export|参数|描述||—|—||–input-fields-terminated-by|导出数据中字段分隔符||–input-lines-terminated-by|导出数据中行分隔符| 公用参数：hive|参数|描述||—|—||–hive-import|将数据从传统数据库中导入到Hive表中||–hive-overwrite|覆盖已存在的数据||–create-hive-table|more是false，如果表已经存在，则会创建失败||–hive-table|hive中表的名字||–hive-partition-key|创建分区，后面直接跟的就是分区名，类型默认为string||–hive-partition-value|导入数据的时候，指定一下是哪个分区| 案例实操 增量导入 123456789[root@master sqoop]# sqoop import \&gt; --connect jdbc:mysql://master:3306/mysql_bigdata?useSSL=false \&gt; --username root \&gt; --password 123456 \&gt; --table student \&gt; -m 1 \&gt; --fields-terminated-by '\t' \&gt; --target-dir /hive/mysql_bigdata5 \&gt; --append 错误解决办法 HIVE_CONF_DIR 123export HIVE_CONF_DIR=$HIVE_HOME/confClassNotFoundexport HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/* NoSuchMethod 1234##删除一下jar包，sqoop的lib目录jackson-core-2.3.1.jarjackson-databind-2.3.1jackson-annotations-2.3.1.jar register相关问题 12在这个文件/opt/apps/Java/jdk1.8.0_172/jre/lib/security/java.policy最后的&#125;之前添加permission javax.management.MBeanTrustPermission "register"; ![image_1ckugsidd4j61pmm1vrg1q0e5lo9.png-30.2kB][1]]]></content>
      <categories>
        <category>sqoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-Mapreduce]]></title>
    <url>%2F2019%2F04%2F25%2F1_Hadoop-Mapreduce%2F</url>
    <content type="text"><![CDATA[Hadoop-Mapreduce1. MapReduce入门1.1 MapReduce 定义Mapreduce 是一个分布式运算程序的编程框架，是用户开发“基于 hadoop 的数据分析应用”的核心框架。Mapreduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 hadoop 集群上。1.2 MapReduce 优缺点: 优点- MapReduce 易于编程。它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得 MapReduce 编程变得非常流行。 - 良好的 扩展性。当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 - 高容错性。MapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop 内部完成的。 - 适合 PB 级以上海量数据的离线处理。它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果, MapReduce 很难做到。 缺点MapReduce 不擅长做实时计算、流式计算、DAG（有向图 ）计算。 实时计算。MapReduce 无法像 Mysql 一样，在毫秒或者秒级内返回结果。 流式计算。流式计算的输入数据是动态的，而 MapReduce 的输入数据集是静态的，不能动态变化。这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。 DAG （有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce 并不是不能做，而是使用后，每个 MapReduce 作业的输出结果都会写入到磁盘，会造成大量的磁盘 IO，导致性能非常的低下。 2. MapReduce的编程思想图解![image_1cjoi1fe0phatn11dm8188il049.png-126.6kB][1]简单说明: 分布式的运算程序往往需要分成至少 2 个阶段。 第一个阶段的 maptask 并发实例，完全并行运行，互不相干。 第二个阶段的 reduce task 并发实例互不相干，但是他们的数据依赖于上一个阶段的所有 maptask 并发实例的输出。 MapReduce 编程模型只能包含一个 map 阶段和一个 reduce 阶段，如果用户的业务逻辑非常复杂，那就只能多个 mapreduce 程序，串行运行。 MapReduce的进程一个完整的 mapreduce 程序在分布式运行时有三类实例进程- MrAppMaster：负责整个程序的过程调度及状态协调。 - MapTask：负责 map 阶段的整个数据处理流程。 - ReduceTask：负责 reduce 阶段的整个数据处理流程。MapReduce 编程规范用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 mr 程序的客户端) Mapper 阶段 用户自定义的 Mapper 要继承自己的父类 Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义） Mapper 中的业务逻辑写在 map()方法中 Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义） map()方法（maptask 进程）对每一个&lt;K,V&gt;调用一次 Reducer 阶段 用户自定义的 Reducer 要继承自己的父类 Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV Reducer 的业务逻辑写在 reduce()方法中 Reducetask 进程对每一组相同 k 的&lt;k,v&gt;组调用一次 reduce()方法 Driver 阶段整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象 案例分析统计单词的个数 案例1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120 package com.zhiyou100.hadoop.mr.wordcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;/** * KEYIN: 这个是Map读取到的一行文本的起始偏移量，Long。 * 在hadoop中，已经有了自己的更简单的序列化接口，所以使用LongWritable类 * VALUEIN：默认情况下， Map读取到的一行文本数据，类型是：Text，其实就是String * * KEYOUT：根据map中的业务逻辑运算的结果的输出的Key，Text类型，其实就是一个一个的单词 * * VALUEOUT：完成后输出的value的数据类型，int。在Hadoop中应该是IntWritable * @author root * */public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; /** * 输入：0，zhangsan lisi haha */ @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123; //打印Key到控制台 //System.out.println(key); //将value转换成字符串 String line = value.toString(); //根据\t进行切分 String[] words = line.split("\t"); //输出指定的单词，并将单词出现的次数标记为1 for(String word:words) &#123; //将单词传递出去：zhangsan，1 lisi，1 haha，1 context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125;package com.zhiyou100.hadoop.mr.wordcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;/** * KEYIN： ValueIN：对应的是Mapper的输出的Key与value */public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123; int count = 0; for(IntWritable value:values) &#123; count+=value.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125;package com.zhiyou100.hadoop.mr.wordcount;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCount &#123; public static void main(String[] args) throws Exception &#123; //1. 获取配置信息 Configuration conf = new Configuration(); //2. 设置额外的配置 //2.1 Permission denied: user=zhang, access=WRITE, inode="/wc":root:supergroup:drwxr-xr-x //没用：conf.set("HADOOP_USER_NAME", "root"); System.setProperty("HADOOP_USER_NAME", "root"); FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), conf, "root"); //2.2 Exception message: /bin/bash: 第 0 行:fg: 无任务控制 // 设置跨平台 conf.set("mapreduce.app-submission.cross-platform", "true"); //2.3 Error: Java heap space conf.set("mapred.child.java.opts", "-Xmx512m"); //3. 创建Job对象 Job job = Job.getInstance(conf, "wc"); //4. 指定job的主程序 job.setJarByClass(WordCount.class); //4.1 java.lang.ClassNotFoundException: Class com.zhiyou100.hadoop.mr.wordcount.WordCountMapper not found job.setJar("F:\\work\\mr\\wc.jar"); //5. 指定Job的Map和Reduce业务类 job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); //6. 指定Job工作的数据类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //7. 指定Job的输入及输出目录 FileInputFormat.setInputPaths(job, new Path("hdfs://master:9000/wc/input/")); Path outPath = new Path("hdfs://master:9000/wc/output/"); if(fs.exists(outPath)) &#123; fs.delete(outPath, true); &#125; FileOutputFormat.setOutputPath(job, outPath); //8. 提交Job System.out.println(job.waitForCompletion(true)?0:1);; &#125;&#125; 2. MapReduce自定义数据类型及排序2.1 如何实现自定义数据类型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实就是自定义的 Key，value 类型，说白了就是自定义一个 Java 类。那么在 Hadoop 中，自定义的数据类型必须实现 Hadoop 指定的序列化接口。2.2 什么是序列化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。2.3 为什么要序列化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。然而序列化可以存储“活的” 对象，可以将“活的”对象发送到远程计算机。2.4 为什么不用 Java 的序列化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop 自己开发了一套序列化机制（Writable），精简、高效。2.5 为什么序列化对 Hadoop 很重要&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因为 Hadoop 在集群之间进行通讯或者 RPC 调用的时候，需要序列化，而且要求序列化要快，且体积要小，占用带宽要小。所以必须理解 Hadoop 的序列化机制。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;序列化和反序列化在分布式数据处理领域经常出现：进程通信和永久存储。然而 Hadoop 中各个节点的通信是通过远程调用（RPC）实现的，那么 RPC 序列化要求具有以下特点：- 紧凑：紧凑的格式能让我们充分利用网络带宽，而带宽是数据中心最稀缺的资源 - 快速：进程通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序列化的性能开销，这是基本的 - 可扩展：协议为了满足新的需求变化，所以控制客户端和服务器过程中，需要直接引进相应的协议，这些是新协议，原序列化方式能支持新的协议报文 - 互操作：能支持不同语言写的客户端和服务端进行交互2.6 Hadoop常用序列化数据类型与Java中类型对比|Java 类型|Hadoop Writable 类型| |—|—| |boolean|BooleanWritable| |byte|ByteWritable| |int|IntWritable| |float|FloatWritable| |long|LongWritable| |double|DoubleWritable| |string|Text| |map|MapWritable| |array|ArrayWritable|2.7 自定义数据类型步骤: 必须实现 Writable 接口 反序列化时，需要反射调用空参构造函数，所以必须有空参构造 重写序列化方法 —— write 重写反序列化方法 —— readFields 注意反序列化的顺序和序列化的顺序完全一致 如果需要将自定义的 bean 放在 key 中传输，则还需要实现 comparable 接口，因为 mapreduce 框中的 shuffle 过程一定会对 key 进行排序。 2.8 案例实战2.8.1 案例需求统计每一个手机号耗费的总上行流量、下行流量、总流量2.8.2 案例输入格式 11533136934908 13101780174 0a:70:ce:39:11:9f:2G 139.210.235.34 79 21 5883 42147 200&gt; 时间戳 手机号码 Mac地址 IP地址 上传包 下载包 上传流量 下载流量 状态码 2.8.3 案例输出格式 1213101780174 5883 42147 99999手机号 上行总流量 下行总流量 总流量 2.8.4 功能分析 2.8.5 案例代码 1 3. MapReduce架构原理3.1 基本原理看PPT3.2 执行流程使用断点执行waitForCompletion()进入Job中的1308行submit();然后Job中的1282行 ensureState(JobState.DEFINE);判断集群的状态然后Job中的1283行setUseNewAPI();属于新旧API的转换然后Job中的1284行connect();建立连接然后进入Cluster.class 82行 initialize(jobTrackAddr, conf);然后进入到这个方法中，在100行打断点查看clientProtocol为LocalJobRunner然后进入Job中的1290行然后进入到JobSubmiter 中的257行 checkSpecs(Job job)查看JobConf jConf然后进入到JobSubmiter 190行 copyAndConfigureFiles(job, submitJobDir);3.3 切片的具体设置3.3.1 FileInputFormat切片设置123456789101112131415161718192021222324 long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); long maxSize = getMaxSplitSize(job); mapreduce.input.fileinputformat.split.minsize mapreduce.input.fileinputformat.split.maxsize long splitSize = computeSplitSize(blockSize, minSize, maxSize); protected long computeSplitSize(long blockSize, long minSize, long maxSize) &#123; return Math.max(minSize, Math.min(maxSize, blockSize));&#125;long bytesRemaining = length; while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123; int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining); splits.add(makeSplit(path, length-bytesRemaining, splitSize, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts())); bytesRemaining -= splitSize; &#125; if (bytesRemaining != 0) &#123; int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining); splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts())); &#125; 3.3.2 CombineTextInputFormat切片设置 : 下周讲 3.3.3 ReduceTask工作机制 : 一个MapTask或者一个切片对应于一个Mapper对象 一个切片可能对应于多个MapTask（如果某一个MapTask没有执行完成就失败了，那么该切片会重新被分配Map执行） 一个ReduceTask对应于一个输出文件，对应于一个Reducer对象，一个Reducer对象的：setup方法（一次），reduce方法（多次，根据分组的数量来确定），cleanup方法（一次） 分区数可以完全自定义，但是注意以下规则： 如果reduce个数为1，则分区数无所谓，但是分区的编号返回必须为0,1,2,3,4,5 如果reduce个数不为1，那么分区数必须小于等于reduce的个数 如果分区数大于reduce的个数，而reduce个数不为1，那么就会出现异常 reduce个数大于分区的话，会出现一些空文件 MapTask的并行度由谁决定？客户端决定——&gt;切片数——&gt;Map数Map数&gt;=切片数。 ReduceTask的并行度由谁决定？设置ReduceTask任务数job.setNumReduceTasks(3)注意：ReduceTask = 0，表示没有Reduce阶段，那么输出的文件内容为map的输出ReduceTask的默认值为1，因此默认只有一个文件输出如果数据分布不均匀，那么会出现什么问题——数据倾斜ReduceTask的数量也不能随意的设置：节点数，业务需求如果分区数不是1，但是reduce数目为1，这个时候分区只有一个。如果分区数&gt;reduce数目，但是reduce不为1，会出现下面的错误Error: java.io.IOException: Illegal partition for如果分区数=reduce数目，没有任何问题如果分区数&lt;reduce数目,则后续的几个文件中的数据为空 3.4 Combine过程: combiner 是 MR 程序中 Mapper 和 Reducer 之外的一种组件。 combiner 组件的父类就是 Reducer。 combiner 和 reducer 的区别在于运行的位置：Combiner 是在每一个 maptask 所在的节点运行;Reducer 是接收全局所有 Mapper 的输出结果； combiner 的意义就是对每一个 maptask 的输出进行局部汇总，以减小网络传输量。 combiner 能够应用的前提是不能影响最终的业务逻辑，而且，combiner 的输出 kv 应该跟 reducer 的输入 kv 类型要对应起来。 3.5 分组对 reduce 阶段的数据根据某一个或几个字段进行分组。3.5.1 案例 现在需要求出每一个订单中最贵的商品。 |订单id| 商品 id| 成交金额| |—|—|—| |0000001| Pdt_01 |222.8| |0000001| Pdt_06 |25.8| |0000002| Pdt_03 |522.8| |0000002| Pdt_04 |122.4| |0000002| Pdt_05 |722.4| |0000003| Pdt_01| 222.8| |0000003| Pdt_02 |33.8|文件1：1 222.8 文件2：2 722.8 文件3：3 222.83.6 WritableComparable 排序&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;排序是 MapReduce 框架中最重要的操作之一。Map Task 和 Reduce Task 均会对数据（按照 key）进行排序。该操作属于 Hadoop 的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于 Map Task，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次排序，并将这些有序数据写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行一次合并，以将这些文件合并成一个大的有序文件。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于 Reduce Task，它从每个 Map Task 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则放到磁盘上，否则放到内存中。如果磁盘上文件数目达到一定阈值，则进行一次合并以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据写到磁盘上。当所有数据拷贝完毕后，Reduce Task 统一对内存和磁盘上的所有数据进行一次合并。排序的分类：- 部分排序：MapReduce 根据输入记录的键对数据集排序。保证输出的每个文件内部排序。 - 全排序 如何用 Hadoop 产生一个全局排序的文件？最简单的方法是使用一个分区。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了 MapReduce 所提供的并行架构。替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为上述文件创建 3 个分区，在第一分区中，记录的单词首字母 a-g，第二分区记录单词首字母 h-n, 第三分区记录单词首字母 o-z。 - 辅助排序：（GroupingComparator 分组） Mapreduce 框架在记录到达 reducer 之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的 map 任务且这些 map 任务在不同轮次中完成时间各不相同。一般来说，大多数 MapReduce 程序会避免让 reduce 函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。 - 二次排序：在自定义排序过程中，如果 compareTo 中的判断条件为两个即为二次排序。 3.7 Shuffle机制Mapreduce 确保每个 reducer 的输入都是按键排序的。系统执行排序的过程（即将 map 输出作为输入传给 reducer）称为 shuffle。4. 自定义输入输出格式4.1 案例1：出现故障和警告出现次数: 4.2 案例2自定义输出类型，把输出类型按照日期，输出到不同的日志文档里面，把同一个月的写在同一个文件夹下，同一天的写在同一个文件中作业： 去重，TopK，自动读取K V值 5. MapReduce高级操作5.1 Join操作Reduce join: 原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Map 端的主要工作：为来自不同表(文件)的 key/value 对打标签以区别不同来源的记录。然后用连接字段作为 key，其余部分和新加的标志作为 value，最后进行输出。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reduce 端的主要工作：在 reduce 端以连接字段作为 key 的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在 map 阶段已经打标志)分开，最后进行合并就 ok 了。 该方法的缺点&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方式的缺点很明显就是会造成 map 和 reduce 端也就是 shuffle 阶段出现大量的数据传输，效率很低。 Map join (Distributedcache 分布式缓存): 使用场景：一张表十分小、一张表很大。 解决方案在 map 端缓存多张表，提前处理业务逻辑，这样增加 map 端业务，减少 reduce 端数据的压力，尽可能的减少数据倾斜。 具体办法：采用 distributedcache 在 mapper 的 setup 阶段，将文件读取到缓存集合中。 在驱动函数中加载缓存。 5.2 MR串联一个稍复杂点的处理逻辑往往需要多个mapreduce程序串联处理，多job的串联可以借助mapreduce框架的JobControl实现 5.3 倒排索引5.4 计数器应用在实际生产代码中，常常需要将数据处理过程中遇到的不合规数据行进行全局计数，类似这种需求可以借助mapreduce框架中提供的全局计数器来实现 5.5 Hadoop 数据压缩5.5.1 概述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在 Hadoop 下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下，I/O 操作和网络数据传输要花大量的时间。还有，Shuffle 与 Merge 过程同样也面临着巨大的 I/O 压力。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;鉴于磁盘 I/O 和网络带宽是 Hadoop 的宝贵资源，数据压缩对于节省资源、最小化磁盘 I/O 和网络传输非常有帮助。不过，尽管压缩与解压操作的 CPU 开销不高，其性能的提升和资源的节省并非没有代价。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果磁盘 I/O 和网络带宽影响了 MapReduce 作业性能，在任意 MapReduce 阶段启用压缩都可以改善端到端处理时间并减少 I/O 和网络流量。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;压缩 Mapreduce 的一种优化策略：通过压缩编码对 Mapper 或者 Reducer 的输出进行压缩，以减少磁盘 IO ，提高 MR 程序运行速度（但相应增加了 cpu 运算负担）。注意：压缩特性运用得当能提高性能，但运用不当也可能降低性能。基本原则 运算密集型的 job，少用压缩 IO 密集型的 job，多用压缩 5.5.2 MR 支持的压缩编码|压缩格式| hadoop 自带？| 算法 |文件扩展名 |是否可切分|换成压缩格式后，原来的程序是否需要修改| |—|—|—|—|—|—| |DEFAULT|是，直接使用|DEFAULT|.deflate|否|和文本处理一样，不需要修改| |Gzip|是，直接使用|DEFAULT|.gz|否|和文本处理一样，不需要修改| |bzip2|是，直接使用|bzip2|.bz2|是|和文本处理一样，不需要修改| |LZO|否，需要安装|LZO|.lzo|是|需要建索引，还需要指定输入格式| |Snappy|否，需要安装|Snappy|.snappy|否|和文本处理一样，不需要修改| 为了支持多种压缩/解压缩算法，Hadoop 引入了编码/解码器，如下表所示 |压缩格式|对应的编码/解码器| |—|—| |DEFLATE|org.apache.hadoop.io.compress.DefaultCodec| |gzip|org.apache.hadoop.io.compress.GzipCodec| |bzip2|org.apache.hadoop.io.compress.BZip2Codec| |LZO|com.hadoop.compression.lzo.LzopCodec| |Snappy|org.apache.hadoop.io.compress.SnappyCodec| 压缩性能的比较 |压缩算法|原始文件大小|压缩文件大小|压缩速度|解压速度| |—|—|—|—|—| |gzip|8.3GB| 1.8GB| 17.5MB/s| 58MB/s| |bzip2| 8.3GB| 1.1GB| 2.4MB/s |9.5MB/s| |LZO| 8.3GB |2.9GB| 49.3MB/s |74.6MB/s| ![image_1ck7vuneq13ultn7q6k8ud1kqq9.png-224.1kB][2]5.5.3 压缩方式的选择: gzip 压缩 优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop 本身支持，在应用中处理 gzip 格式的文件就和直接处理文本一样；大部分 linux 系统都自带 gzip 命令，使用方便。缺点：不支持 split。应用场景：当每个文件压缩之后在 130M 以内的（1 个块大小内），都可以考虑用 gzip压缩格式。例如说一天或者一个小时的日志压缩成一个 gzip 文件，运行 mapreduce 程序的时候通过多个 gzip 文件达到并发。hive 程序，streaming 程序，和 java 写的 mapreduce 程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。 bzip2 压缩 优点：支持 split；具有很高的压缩率，比 gzip 压缩率都高；hadoop 本身支持，但不支持 native；在 linux 系统下自带 bzip2 命令，使用方便。缺点：压缩/解压速度慢；不支持 native。应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为 mapreduce 作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持 split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 LZO 压缩 优点：压缩/解压速度也比较快，合理的压缩率；支持 split，是 hadoop 中最流行的压缩格式：可以在 linux 系统下安装 lzop 命令，使用方便。缺点：压缩率比 gzip 要低一些；hadoop 本身不支持，需要安装；在应用中对 lzo 格式的文件需要做一些特殊处理（为了支持 split 需要建索引，还需要指定 inputformat 为 lzo 格式）。应用场景：一个很大的文本文件，压缩之后还大于 200M 以上的可以考虑，而且单个文件越大，lzo 优点越越明显。 snappy 压缩 优点：高速压缩速度和合理的压缩率。缺点：不支持 split；压缩率比 gzip 要低；hadoop 本身不支持，需要安装；应用场景：当 Mapreduce 作业的 Map 输出的数据比较大的时候，作为 Map 到 Reduce 的中间数据的压缩格式；或者作为一个 Mapreduce 作业的输出和另外一个 Mapreduce 作业的输入。 5.5.4 压缩位置选择压缩可以在 MapReduce 作用的任意阶段启用。 输入端采用压缩在有大量数据并计划重复处理的情况下，应该考虑对输入进行压缩。然而，你无须显示指定 使 用 的 编 解 码 方 式 。Hadoop自动检查文件扩展名，如果扩展名能够匹配，就会用恰当的编解码方式对文件进行压缩和解压。否则，Hadoop就不会使用任何编解码器。 mapper 输出采用压缩当map任务输出的中间数据量很大时，应考虑在此阶段采用压缩技术。这能显著改善内部数据 Shuffle 过程 ， 而 Shuffle 过程在 Hadoop 处理过程中是资源消耗最多的环节。如果发现数据量大造成网络传输缓慢，应该考虑使用压缩技术。可用于压缩mapper输出的快速编解码器包括LZO或者Snappy。注： LZO是供Hadoop压缩数据用的通用压缩编解码器。其设计目标是达到与硬盘读取速度相当的压缩速度，因此速度是优先考虑的因素，而不是压缩率。与gzip编解码器相比，它的压缩速度是gzip的5倍，而解压速度是gzip的2倍。同一个文件用LZO压缩后比用gzip压缩后大50%，但比压缩前小25%~50%。这对改善性能非常有利，map阶段完成时间快4倍。 reducer 输出采用压缩在此阶段启用压缩技术能够减少要存储的数据量，因此降低所需的磁盘空间。当mapreduce作业形成作业链条时，因为第二个作业的输入也已压缩，所以启用压缩同样有效。 5.5.5 压缩配置参数要在 Hadoop 中启用压缩，可以配置如下参数 12345678910111213141516171819202122232425262728293031323334353637 &lt;!-- 输入压缩 Hadoop 使用文件扩展名判断是否支持某种编解码器 --&gt; &lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec &lt;/value&gt;&lt;/property&gt;&lt;!--mapper 输出,在 mapred-site.xml中配置--&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;这个参数设为 true 启用压缩 &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt; &lt;description&gt;使用 LZO 或 snappy 编解码器在此阶段压缩数据&lt;/description&gt;&lt;/property&gt;&lt;!--Reduce输出--&gt;&lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;这个参数设为 true 启用压缩&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress.type&lt;/name&gt; &lt;value&gt;RECORD&lt;/value&gt; &lt;description&gt;SequenceFile 输出使用的压缩类型 ：NONE 和 BLOCK&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt; &lt;description&gt;使用标准工具或者编解码器，如gzip 和 bzip2&lt;/description&gt;&lt;/property&gt;5.5.6 案例实战: 数据流的压缩和解压缩CompressionCodec 有两个方法可以用于轻松地压缩或解压缩数据。要想对正在被写入一个输出流的数据进行压缩，我们可以使用 createOutputStream(OutputStreamout) 方法创建一个 CompressionOutputStream ，将其以压缩格式写入底层的流。相反，要想对从输入流读取而来的数据进行解压缩，则调用 createInputStream(InputStreamin) 函数，从而获得一个 CompressionInputStream，从而从底层的流读取未压缩的数据。 Map 输出端采用压缩 Reduce 输出端采用压缩 5.6 Hadoop优化5.6.1 MapReduce 跑的慢的原因Mapreduce 程序效率的瓶颈在于两点： 计算机性能CPU、内存、磁盘健康、网络 I/O 操作优化- 数据倾斜 - map 和 reduce 数设置不合理 - map 运行时间太长，导致 reduce 等待过久 - 小文件过多 - 大量的不可分块的超大文件 - spill 次数过多 - merge 次数过多等 5.6.2 MapReduce 优化方法MapReduce 优化方法主要从六个方面考虑：数据输入、Shuffle 过程、Reduce 阶段、IO 传输、数据倾斜问题和常用的调优参数。 数据输入 - 合并小文件：在执行 mr 任务前将小文件进行合并，大量的小文件会产生大量的map 任务，增大 map 任务装载次数，而任务的装载比较耗时，从而导致 mr 运行较慢。 - 采用 CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 Shuffle 过程 - 减少溢写(spill)次数：通过调整 io.sort.mb 及 sort.spill.percent 参数值，增大触发 spill 的内存上限，减少 spill 次数，从而减少磁盘 IO。 - 减少合并(merge)次数：通过调整 io.sort.factor 参数，增大 merge 的文件数目，减少 merge 的次数，从而缩短 mr 处理时间。 - 在 map 之后，在不影响业务逻辑前提下，先进行 combine 处理，减少 I/O。 Reduce 阶段 - 合理设置 map 和 reduce 的个数：两个都不能设置太少，也不能设置太多。太少，会导致 task 等待，延长处理时间；太多，会导致 map、reduce 任务间竞争资源，造成处理超时等错误。 - 设置 map 、reduce 共存：调整 slowstart.completedmaps 参数，使 map 运行到一定程度后，reduce 也开始运行，减少 reduce 的等待时间。 - 规避使用 reduce ：因为 reduce 在用于连接数据集的时候将会产生大量的网络消耗。 - 合理设置 reduce 端的 buffer 。默认情况下，数据达到一个阈值的时候，buffer 中的数据就会写入磁盘，然后 reduce 会从磁盘中获得所有的数据。也就是说，buffer 和 reduce 是没有直接关联的，中间多个一个写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得 buffer 中的一部分数据可以直接输送到 reduce，从而减少 IO 开销：mapred.job.reduce.input.buffer.percent，默认为 0.0。当值大于 0 的时候，会保留指定比例的内存读 buffer 中的数据直接拿给 reduce 使用。这样一来，设置 buffer 需要内存，读取数据需要内存，reduce 计算也要内存，所以要根据作业的运行情况进行调整。 IO 传输 - 采用数据压缩的方式，减少网络 IO 的的时间。安装 Snappy 和 LZO 压缩编码器。 - 使用 SequenceFile 二进制文件。 数据倾斜问题 数据倾斜现象 数据频率倾斜——某一个区域的数据量要远远大于其他区域。 数据大小倾斜——部分记录的大小远远大于平均值。 如何收集倾斜数据 在 reduce 方法中加入记录 map 输出键的详细情况的功能。 减少数据倾斜的方法 抽样和范围分区可以通过对原始数据进行抽样得到的结果集来预设分区边界值。 自定义分区基于输出键的背景知识进行自定义分区。例如，如果 map 输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分 reduce 实例。而将其他的都发送给剩余的 reduce 实例。 Combine使用 Combine 可以大量地减小数据倾斜。在可能的情况下，combine 的目的就是聚合并精简数据。 采用 Map Join，尽量避免 Reduce Join 。 常用的调优参数 - **资源相关参数** 以下参数是在用户自己的 mr 应用程序中配置就可以生效（mapred-default.xml） |配置参数|参数说明| |---|---| |mapreduce.map.memory.mb|一个 Map Task 可使用的资源上限（单位:MB），默认为 1024。如果 Map Task 实际使用的资源量超过该值，则会被强制杀死。||mapreduce.reduce.memory.mb|一个 Reduce Task 可使用的资源上限（单位:MB），默认为 1024。如果 Reduce Task实际使用的资源量超过该值，则会被强制杀死。||mapreduce.map.cpu.vcores|每个 Map task 可使用的最多 cpu core 数目，默认值: 1||mapreduce.reduce.cpu.vcores|每个 Reduce task 可使用的最多 cpu core 数目，默认值: 1||mapreduce.reduce.shuffle.parallelcopies|每个 reduce 去 map 中拿数据的并行数。默认值是 5||mapreduce.reduce.shuffle.merge.percentbuffer| 中的数据达到多少比例开始写入磁盘。默认值 0.66||mapreduce.reduce.shuffle.input.buffer.percentbuffer| 大小占 reduce 可用内存的比例。默认值 0.7||mapreduce.reduce.input.buffer.percent|指定多少比例的内存用来存放 buffer 中的数据，默认值是 0.0|应该在 yarn 启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml） 配置参数 参数说明 yarn.scheduler.minimum-allocation-mb 1024 给应用程序 container 分配的最小内存 yarn.scheduler.maximum-allocation-mb 8192 给应用程序 container 分配的最大内存 yarn.scheduler.minimum-allocation-vcores 1 每个 container 申请的最小 CPU 核数 yarn.scheduler.maximum-allocation-vcores 32 每个 container 申请的最大 CPU 核数 yarn.nodemanager.resource.memory-mb 8192 给 containers 分配的最大物理内存 shuffle 性能优化的关键参数，应在 yarn 启动之前就配置好（mapred-default.xml） 配置参数 参数说明 — — mapreduce.task.io.sort.mb 100 shuffle 的环形缓冲区大小，默认 100m mapreduce.map.sort.spill.percent 0.8 环形缓冲区溢出的阈值，默认 80% - 容错相关参数(mapreduce 性能优化) 配置参数 参数说明 — — mapreduce.map.maxattempts 每个 Map Task 最大重试次数，一旦重试参数超过该值，则认为 Map Task 运行失败，默认值：4。 mapreduce.reduce.maxattempts 每个 Reduce Task 最大重试次数，一旦重试参数超过该值，则认为 Map Task 运行失败，默认值：4。 mapreduce.task.timeout Task 超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个 task 在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该 task 处于 block 状态，可能是卡住了，也许永远会卡主，为了防止因为用户程序永远 block 住不退出，则强制设置了一个该超时时间（单位毫秒），默认是 600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该 参 数 过 小 常 出 现 的 错 误 提 示 是 “AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by theApplicationMaster.”。 5.6.3 HDFS 小文件优化方法: HDFS 小文件弊端HDFS 上每个文件都要在 namenode 上建立一个索引，这个索引的大小约为 150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用 namenode 的内存空间，另一方面就是索引文件过大是的索引速度变慢。 解决方案 Hadoop Archive是一个高效地将小文件放入 HDFS 块中的文件存档工具，它能够将多个小文件打包成一个 HAR 文件，这样就减少了 namenode 的内存使用。 Sequence filesequence file 由一系列的二进制 key/value 组成，如果 key 为文件名，value 为文件内容，则可以将大批小文件合并成一个大文件。 CombineFileInputFormatCombineFileInputFormat 是一种新的 inputformat，用于将多个文件合并成一个单独的 split，另外，它会考虑数据的存储位置。 开启 JVM 重用对于大量小文件 Job，可以开启 JVM 重用会减少 45%运行时间。JVM 重用理解：一个 map 运行一个 jvm，重用的话，在一个 map 在 jvm 上运行完毕后，jvm 继续运行其他 map。具体设置：mapreduce.job.jvm.numtasks 值在 10-20 之间。 5.7 Hadoop的HA在HBase结束后讲解 6. Yarn架构原理6.1 Hadoop1.x 和 和 Hadoop2.x 架构区别在 Hadoop1.x 时代，Hadoop 中的 MapReduce 同时处理业务逻辑运算和资源的调度，耦合性较大。在 Hadoop2.x 时代，增加了 Yarn。Yarn 只负责资源的调度，MapReduce 只负责运算。6.2 Yarn 是什么Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，类似于任务管理器，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序。6.3 Yarn 基本组件YARN 主要由 ResourceManager、NodeManager、ApplicationMaster 和 Container 等组件构成。6.4 Yarn工作机制工作机制详解 Mr 程序提交到客户端所在的节点。 Yarnrunner 向 Resourcemanager 申请一个 Application。 rm将该应用程序的资源路径返回给 yarnrunner。 该程序将运行所需资源提交到 HDFS 上。 程序资源提交完毕后，申请运行 mrAppMaster。 RM 将用户的请求初始化成一个 task。 其中一个 NodeManager 领取到 task 任务。 该 NodeManager 创建容器 Container，并产生 MRAppmaster。 Container 从 HDFS 上拷贝资源到本地。 MRAppmaster 向 RM 申请运行 maptask 资源。 RM 将运行 maptask 任务分配给另外两个 NodeManager，另两个 NodeManager 分别领取任务并创建容器。 MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个 NodeManager 分别启动 maptask，maptask 对数据分区排序。 MrAppMaster 等待所有 maptask 运行完毕后，向 RM 申请容器，运行 reduce task。 reduce task 向 maptask 获取相应分区的数据。 程序运行完毕后，MR 会向 RM 申请注销自己。 6.5 资源调度器目前，Hadoop 作业调度器主要有三种：FIFO、Capacity Scheduler 和 Fair Scheduler。Hadoop2.7.6 默认的资源调度器是 Capacity Scheduler。具体设置详见：yarn-default.xml 文件 12345 &lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt;- 先进先出调度器（FIFO） - 容量调度器（Capacity Scheduler） - 公平调度器（Fair Scheduler）6.6 任务的推测执行: 作业完成时间取决于最慢的任务完成时间一个作业由若干个 Map 任务和 Reduce 任务构成。因硬件老化、软件 Bug 等，某些任务可能运行非常慢。典型案例：系统中有 99%的 Map 任务都完成了，只有少数几个 Map 老是进度很慢，完不成，怎么办？ 推测执行机制发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。 执行推测任务的前提条件 每个 task 只能有一个备份任务 当前 job 已完成的 task 必须不小于 0.05（5%） 开启推测执行参数设置。Hadoop2.7.6 mapred-site.xml 文件中默认是打开的。12345678910111213&lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; 不能启用推测执行机制情况 任务间存在严重的负载倾斜 特殊任务，比如任务向数据库中写数据 ![image_1cjq3hkab12uruaf9mqrob2eo9.png-45.5kB][3]]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-HDFS]]></title>
    <url>%2F2019%2F04%2F25%2F1_Hadoop-HDFS%2F</url>
    <content type="text"><![CDATA[Hadoop-HDFS1. HDFS的概念和特性 首先，它是一个文件系统，用于存储文件，通过统一的命名空间——目录树来定位文件 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色； 重要特性如下: HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在Hadoop2.x版本中是128M，Hadoop1.x版本中是64M HDFS文件系统会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://master:port/dir-a/dir-b/dir-c/file.data,http://master:50070 目录结构及文件分块信息(元数据)的管理由master节点承担namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的slave服务器文件的 各个block的存储管理由datanode节点承担。datanode是HDFS集群从节点(数据节点)，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication） HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改 注: 适合用来做数据分析 优点: 高容错性数据自动保存多个副本。它通过增加副本的形式，提高容错性。某一个副本丢失以后，它可以自动恢复。 适合大数据处理数据规模：能够处理数据规模达到 GB、TB、甚至 PB 级别的数据。文件规模：能够处理百万规模以上的文件数量，数量相当之大。 流式数据访问一次写入，多次读取，不能修改，只能追加。它能保证数据的一致性。 可构建在廉价机器上，通过多副本机制，提高可靠性。 缺点: 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。 无法高效的对大量小文件进行存储存储大量小文件的话，它会占用 NameNode 大量的内存来存储文件、目录和块信息。这样是不可取的，因为 NameNode 的内存总是有限的。小文件存储的寻址时间会超过读取时间，它违反了 HDFS 的设计目标。 并发写入、文件随机修改一个文件只能有一个写，不允许多个线程同时写。仅支持数据 append（追加），不支持文件的随机修改。 2. HDFS设计思想分而治之：将大文件、大批量文件，分布式存放在大量服务器上，以便于采取分而治之的方式对海量数据进行运算分析在大数据系统中作用为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务重点概念文件切块，副本存放，元数据3. HDFS的shell(命令行客户端)操作3.1 HDFS命令行客户端使用HDFS提供shell命令行客户端，使用方法如下： 1[root@master ~]# hadoop fs -ls / 3.2 常用命令参数介绍-help功能：输出这个命令参数手册-ls功能：显示目录信息示例： hadoop fs -ls hdfs://master:9000/备注：这些参数中，所有的hdfs路径都可以简写–&gt;hadoop fs -ls / 等同于上一条命令的效果-mkdir功能：在hdfs上创建目录示例：hadoop fs -mkdir -p /a/b/c/d-moveFromLocal功能：从本地剪切粘贴到hdfs示例：hadoop fs - moveFromLocal /home/hadoop/a.txt /aaa/bbb/cc/dd-moveToLocal功能：从hdfs剪切粘贴到本地示例：hadoop fs - moveToLocal /aaa/bbb/cc/dd /home/hadoop/a.txt -appendToFile功能：追加一个文件到已经存在的文件末尾示例：hadoop fs -appendToFile ./hello.txt hdfs://master:9000/hello.txt可以简写为：Hadoop fs -appendToFile ./hello.txt /hello.txt-cat功能：显示文件内容示例：hadoop fs -cat /hello.txt-tail功能：显示一个文件的末尾示例：hadoop fs -tail /hello.txt-text功能：以字符形式打印一个文件的内容示例：hadoop fs -text /hello.txt-chmod功能：linux文件系统中的用法一样，对文件所属权限示例：hadoop fs -chmod 666 /hello.txt-copyFromLocal功能：从本地文件系统中拷贝文件到hdfs路径去示例：hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/-copyToLocal功能：从hdfs拷贝到本地示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz-cp功能：从hdfs的一个路径拷贝hdfs的另一个路径示例： hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2-mv功能：在hdfs目录中移动文件示例： hadoop fs -mv /aaa/jdk.tar.gz /-get功能：等同于copyToLocal，就是从hdfs下载文件到本地示例：hadoop fs -get /aaa/jdk.tar.gz-getmerge功能：合并下载多个文件示例：比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,…hadoop fs -getmerge /aaa/log.* ./log.sum-put功能：等同于copyFromLocal示例：hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2-rm功能：删除文件或文件夹示例：hadoop fs -rm -r /aaa/bbb/-rmdir功能：删除空目录示例：hadoop fs -rmdir /aaa/bbb/ccc-df功能：统计文件系统的可用空间信息示例：hadoop fs -df -h /-du功能：统计文件夹的大小信息示例：hadoop fs -du -s -h /aaa/*-count功能：统计一个指定目录下的文件节点数量示例：hadoop fs -count /aaa/-setrep功能：设置hdfs中文件的副本数量示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz3.3 HDFS 的Java 客户端的操作: 在windows上创建HADOOP_HOME环境变量 修改bin目录的文件 复制hadoop.dll文件到c:\windows\system32目录 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243package com.zhiyou100.hadoop.hdfs;import java.io.FileInputStream;import java.io.FileOutputStream;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.BlockLocation;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.LocatedFileStatus;import org.apache.hadoop.fs.Path;import org.apache.hadoop.fs.RemoteIterator;import org.apache.hadoop.io.IOUtils;import org.junit.Before;import org.junit.Test;/** * 配置文件的生效顺序(优先级) * set("","")&gt;src&gt;hadoop-hdfs-2.7.6.jar * @author zhang * */public class HDFS_Operate &#123; FileSystem fs = null; Configuration conf = null; @Before public void init() throws Exception&#123; //获取配置对象 conf = new Configuration(); conf.set("dfs.replication", "2"); //获取文件系统对象 fs = FileSystem.get(new URI("hdfs://master:9000"), conf, "root"); &#125; /** * 上传文件 * @throws Exception */ @Test public void upload() throws Exception &#123; fs.copyFromLocalFile(new Path("F:\\work\\hadoop-2.7.6.tar.gz"), new Path("/")); fs.close(); &#125; /** * 下载文件 * @throws Exception */ @Test public void download() throws Exception &#123; /* * boolean delSrc:是否删除源文件 * Path src：要下载的文件的原路径 * Path dst：文件下载下来要存放的路径 * boolean useRawLocalFileSystem：是否允许使用本地文件系统创建crc文件来进行文件的校验 */ fs.copyToLocalFile(false, new Path("/aaaaaa.txt"), new Path("G:/"), true); fs.close(); &#125; /** * 用来创建目录 * @throws Exception * @throws IllegalArgumentException */ @Test public void createDir() throws Exception &#123; fs.mkdirs(new Path("/zhangsan/lisi1")); fs.close(); &#125; /** * 用来删除目录 * true:无论是否为空，均要删除 * false:如果目录不为空，则报错 * @throws Exception */ @Test public void deleteDir() throws Exception &#123; fs.delete(new Path("/zhangsan/lisi1"), true); fs.close(); &#125; /** * 重命名 * @throws Exception */ @Test public void rename() throws Exception &#123; fs.rename(new Path("/aaa.txt"), new Path("/bbb.txt")); fs.close(); &#125; /** * 遍历一个目录下的所有文件 * @throws Exception */ @Test public void listFileInfo() throws Exception &#123; FileStatus[] statuses = fs.listStatus(new Path("/")); for(FileStatus status:statuses) &#123; //获取文件名 System.out.println(status.getPath().getName()); //获取文件的长度 System.out.println(status.getLen()); //获取权限 System.out.println(status.getPermission()); //获取块的大小 System.out.println(status.getBlockSize()); &#125; &#125; @Test public void listFileBlock() throws Exception&#123; RemoteIterator&lt;LocatedFileStatus&gt; files = fs.listFiles(new Path("/"), true); //判断是否有下一个文件 while(files.hasNext()) &#123; //LocatedFileStatus继承了上述的FileStatus //获取的是所有的文件及子目录中的文件 LocatedFileStatus status = files.next(); System.out.println(status.getPath().getName()); //获取存储的Block信息 BlockLocation[] bs = status.getBlockLocations(); for(BlockLocation b:bs) &#123; System.out.println("下面是该块所属的Block主机"); //得到这个Block所属的datanode节点的Host String[] hosts = b.getHosts(); for(String host:hosts) &#123; System.out.println(host); &#125; &#125; System.out.println("-------------"); &#125; fs.close(); &#125; /** * 判断该文件是文件还是文件夹 * @throws Exception */ @Test public void getFileType() throws Exception &#123; // FileStatus[] files = fs.listStatus(new Path("/")); for(FileStatus file : files) &#123; //判断是否为文件 if(file.isFile()) &#123; System.out.println("文件："+file.getPath()); &#125;else &#123; System.out.println("文件夹："+file.getPath()); &#125; &#125; fs.close(); &#125; /** * 通过普通的IO操作HDFS * 文件上传 * @throws Exception */ @Test public void putFileToHDFS() throws Exception &#123; //创建输入流 FileInputStream is = new FileInputStream("D:\\aaaaaa.txt"); //创建输出流 FSDataOutputStream fos = fs.create(new Path("/王果.txt")); //流拷贝 IOUtils.copyBytes(is, fos, conf); //关闭流 IOUtils.closeStream(is); IOUtils.closeStream(fos); &#125; /** * 文件下载 * @throws Exception */ @Test public void getFileFromHDFS() throws Exception &#123; //创建输入流 FSDataInputStream fis = fs.open(new Path("/王果.txt")); //创建输出流 FileOutputStream fos = new FileOutputStream("F:/王果.txt"); //流拷贝 IOUtils.copyBytes(fis, fos, conf); IOUtils.closeStream(fis); IOUtils.closeStream(fos); &#125; /** * 文件下载——分块下载 * @throws Exception */ @Test public void getBlockFile() throws Exception &#123; //获取输入流 FSDataInputStream fis = fs.open(new Path("/hadoop-2.7.6.tar.gz")); //创建一个输出流 FileOutputStream os = new FileOutputStream("F:/hadoop-2.7.6.tar.gz.part1"); //流拷贝 byte[] buffer = new byte[1024]; for(int i = 0;i&lt;1024*128;i++) &#123; fis.read(buffer); os.write(buffer); &#125; os.flush(); os.close(); fis.close(); &#125; /** * 文件下载——分块下载——第2块 * @throws Exception */ @Test public void getBlockFile2() throws Exception &#123; //获取输入流 FSDataInputStream fis = fs.open(new Path("/hadoop-2.7.6.tar.gz")); //设置输入流读取的数据位置 fis.seek(1024*1024*128); //创建一个输出流 FileOutputStream os = new FileOutputStream("F:/hadoop-2.7.6.tar.gz.part2"); //流拷贝 IOUtils.copyBytes(fis, os, conf); //关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(os); &#125; &#125; 4. hdfs的工作机制4.1 概述: HDFS集群分为两大角色：NameNode进程、DataNode进程（secondary namenode） NameNode 负责管理整个文件系统的元数据 DataNode 负责管理用户的文件数据块 文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上 每一个文件块可以有多个副本，并存放在不同的datanode上 Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量 HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行 4.2 HDFS写数据流程客户端要向HDFS写数据，首先要跟namenode通信以确认可以写文件并获得接收文件block的datanode，然后，客户端按顺序将文件逐个block传递给相应datanode，并由接收到block的datanode负责向其他datanode复制block的副本![image_1cjj6r8ng19kaqu91ekteot11ud9.png-153.9kB][1]详细步骤解析 根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在 namenode返回是否可以上传 client请求第一个 block该传输到哪些datanode服务器上 namenode返回3个datanode服务器ABC client请求3台dn中的一台A上传数据（建立pipeline），A收到请求会继续调用B，然后B调用C，将一个pipeline建立完成，逐级返回客户端 client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答 当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。 4.2 网络拓扑: 4.3. HDFS读数据流程客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件。![image_1cjj8j28fiav1vaar38146dov8m.png-73.6kB][2] 详细步骤解析 客户端通过 Distributed FileSystem 向 namenode 请求下载文件, 跟 namenode 通信查询元数据，找到文件块所在的datanode服务器 挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流 datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验） 客户端以packet为单位接收，现在本地缓存，然后写入目标文件 4.3 一致性模型案例 1234567891011public void put() throws IOException &#123; //创建一个输出流 FSDataOutputStream fos = fs.create(new Path("/王晨3.txt")); //写入数据 fos.write("你好，王晨".getBytes()); //一致性刷新 fos.hflush(); fos.close(); fs.close();&#125;5. Namenode工作机制5.1 镜像文件与编辑日志: 概念namenode 被格式化之后，将在/opt/apps/Hadoop/hadoop-2.7.6/hdfs/name/current 目录中产生如下文件fsimage_0000000000000000000fsimage_0000000000000000000.md5seen_txidVERSION - Fsimage 文件：HDFS 文件系统元数据的一个永久性的检查点，其中包含 HDFS 文件系统的所有目录和文件 idnode 的序列化信息。 - Edits 文件：存放 HDFS 文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到 edits 文件中。 - seen_txid 文件保存的是一个数字，就是最后一个 edits_的数字 - 每次 Namenode 启动的时候都会将 fsimage 文件读入内存，并从 00001 开始到 seen_txid 中记录的数字依次执行每个 edits 里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成 Namenode 启动的时候就将 fsimage 和 edits 文件进行了合并。 oiv 查看 fsimage 文件 - 查看 oiv 和 oev 命令 - 基本语法 1hdfs oiv -p 返回的文件类型 -i 镜像文件 -o 转换后文件输出的路径 - 案例 1[root@master current]# hdfs oiv -p XML -i fsimage_0000000000000000187 -o /opt/data/fsimage.xml oev 查看 edits 文件 - 基本语法 1hdfs oev -p 返回的文件类型 -i 编辑日志 -o 转换后文件输出的路径 - 案例 1[root@master current]# hdfs oev -p xml -i edits_0000000000000000003-0000000000000000016 -o /opt/data/edit.xml 5.2 Namenode和SecondaryNamenode原理: namenode 启动 namenode 格式化后，创建 fsimage 文件。 如果是第一次启动，那么就会加载镜像文件到内存中 如果不是第一次启动，直接加载编辑日志和镜像文件到内存,同时还要做合并操作。 客户端对元数据进行增删改的请求。 namenode 记录操作日志，更新滚动日志。 namenode 在内存中对数据进行增删改查。 Secondary NameNode 工作 Secondary NameNode 询问 namenode 是否需要 checkpoint。直接带回 namenode 是否检查结果。 Secondary NameNode 请求执行 checkpoint。 namenode 滚动正在写的 edits 日志。 将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。 Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。 生成新的镜像文件 fsimage.chkpoint。 拷贝 fsimage.chkpoint 到 namenode。 namenode 将 fsimage.chkpoint 重新命名成 fsimage。 原理图 5.3 滚动编辑日志正常情况 HDFS 文件系统有更新操作时，就会滚动编辑日志。也可以用命令强制滚动编辑日志。- 滚动编辑日志（前提必须启动集群） 1hdfs dfsadmin -rollEdits - 镜像文件什么时候产生 Namenode 启动时加载镜像文件和编辑日志5.4 Namenode 版本号: 查看 namenode 版本号在/opt/apps/Hadoop/hadoop-2.7.6/hdfs/name/current 这个目录下查看 VERSIONnamespaceID=1483942760clusterID=CID-da97158f-34cc-4f8f-ad08-421c5fa542f7cTime=0storageType=NAME_NODEblockpoolID=BP-1314907513-192.168.100.100-1532659630649layoutVersion=-63 namenode 版本号具体解释 namespaceID 在 HDFS 上，会有多个 Namenode，所以不同 Namenode 的namespaceID 是不同的，分别管理一组 blockpoolID。 clusterID 集群 id，全局唯一 cTime 属性标记了 namenode 存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为 0；但是在文件系统升级之后，该值会更新到新的时间戳。 storageType 属性说明该存储目录包含的是 namenode 的数据结构。 blockpoolID：一个 block pool id 标识一个 block pool，并且是跨集群的全局唯一。当一个新的 Namespace 被创建的时候(format 过程的一部分)会创建并持久化一个唯一 ID。在创建过程构建全局唯一的 BlockPoolID 比人为的配置更可靠一些。NN 将 BlockPoolID 持久化到磁盘中，在后续的启动过程中，会再次 load 并使用。 layoutVersion 是一个负整数。通常只有 HDFS 增加新特性时才会更新这个版本号。 5.5 web 端访问 SecondaryNameNode 端口号: 启动集群。 浏览器中输入：http://master:50090/status.html 查看 SecondaryNameNode 信息。 5.6 chkpoint 检查时间参数设置: 通常情况下，SecondaryNameNode 每隔一小时执行一次。 12345&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt; 一分钟检查一次操作次数，当操作次数达到 1 百万时，SecondaryNameNode 执行一次。 12345678910 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt; 1 分钟检查一次操作次数&lt;/description&gt;&lt;/property&gt; 5.7 SecondaryNameNode 目录结构 : Secondary NameNode 用来监控 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS元数据的快照。在 /opt/apps/Hadoop/hadoop-2.7.6/tmp/dfs/namesecondary/current 这个目录中查看 SecondaryNameNode 目录结构。 SecondaryNameNode 的 namesecondary/current 目录和主 namenode 的 current 目录的布局相同。好 处 ： 在主 namenode 发生故障时（ 假 设 没 有 及 时 备 份 数 据 ），可以从SecondaryNameNode 恢复数据。 5.8 Namenode 故障处理方法Namenode 故障后，可以采用如下两种方法恢复数据。方法一：将 SecondaryNameNode 中数据拷贝到 namenode 存储数据的目录；方法二：使用 -importCheckpoint 选项启动 namenode 守护进程 ，从而将 SecondaryNameNode 中数据拷贝到 namenode 目录中。5.9 集群安全模式操作: 概述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Namenode 启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的 fsimage 文件和一个空的编辑日志。此时，namenode 开始监听 datanode 请求。但是此刻，namenode 运行在安全模式，即 namenode 的文件系统对于客户端来说是只读的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;系统中的数据块的位置并不是由 namenode 维护的，而是以块列表的形式存储在 datanode 中。在系统的正常操作期间，namenode 会在内存中保留所有块位置的映射信息。在安全模式下，各个 datanode 会向 namenode 发送最新的块列表信息，namenode 了解到足够多的块位置信息之后，即可高效运行文件系统。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果满足“最小副本条件”，namenode 会在 30 秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（ 默认值 ： dfs.replication.min=1）。在启动一个刚刚格式化的 HDFS 集群时，因为系统中还没有任何块，所以 namenode 不会进入安全模式。 基本语法集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。- bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） - bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） - bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） - bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态） 5.10 Namenode 多目录配置: namenode 的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。 具体配置如下：1234 &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 6. DataNode 工作机制6.1 DataNode 工作机制: 一个数据块在 datanode 上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 DataNode 启动后向 namenode 注册，通过后，周期性（1 小时）的向 namenode 上报所有的块信息。 心跳是每 3 秒一次，心跳返回结果带有 namenode 给该 datanode 的命令如复制块数据到另一台机器，或删除某个数据块。如果超过 10 分钟没有收到某个 datanode 的心跳，则认为该节点不可用。 集群运行中可以安全加入和退出一些机器 6.2 数据完整性: 当 DataNode 读取 block 的时候，它会计算 checksum 如果计算后的 checksum，与 block 创建时值不一样，说明 block 已经损坏 client 读取其他 DataNode 上的 block。 datanode 在其文件创建后周期验证 checksum 6.3 掉线时限参数设置&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;datanode 进程死亡或者网络故障造成 datanode 无法与 namenode 通信，namenode 不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS 默认的超时时长为 10 分钟+30 秒。如果定义超时时间为 timeout，则超时时长的计算公式为：timeout = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;而默认的 dfs.namenode.heartbeat.recheck-interval 大小为 5 分钟，dfs.heartbeat.interval 默认为 3 秒。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;需要注意的是 hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval 的单位为秒。 12345678&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;6.4 DataNode 的目录结构和 namenode 不同的是，datanode 的存储目录是初始阶段自动创建的，不需要额外格式化 在/opt/apps/Hadoop/hadoop-2.7.6/hdfs/data/current 这个目录下查看版本号 #Mon Jul 30 18:09:29 EDT 2018 storageID=DS-2967e493-97cf-4f11-88df-dee6ed3ca8bd clusterID=CID-da97158f-34cc-4f8f-ad08-421c5fa542f7 cTime=0 datanodeUuid=624d7aa2-522b-48d0-bfb9-f5c3b80b77e0 storageType=DATA_NODE layoutVersion=-56 具体解释- storageID：存储 id 号 - clusterID 集群 id，全局唯一 - cTime 属性标记了 datanode 存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为 0；但是在文件系统升级之后，该值会更新到新的时间戳。 - datanodeUuid：datanode 的唯一识别码 - storageType：存储类型 - layoutVersion 是一个负整数。通常只有 HDFS 增加新特性时才会更新这个版本号。 查看该数据块的版本号 /opt/apps/Hadoop/hadoop-2.7.6/hdfs/data/current/BP-1314907513-192.168.100.100-1532659630649/currentnamespaceID=1483942760cTime=0blockpoolID=BP-1314907513-192.168.100.100-1532659630649layoutVersion=-56 具体解释 namespaceID：是 datanode 首次访问 namenode 的时候从 namenode 处获取的 storageID 对每个 datanode 来说是唯一的（但对于单个 datanode 中所有存储目录来说则是相同的），namenode 可用这个属性来区分不同 datanode。 cTime 属性标记了 datanode 存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为 0；但是在文件系统升级之后，该值会更新到新的时间戳。 blockpoolID：一个 block pool id 标识一个 block pool，并且是跨集群的全局唯一。当一个新的 Namespace 被创建的时候(format 过程的一部分)会创建并持久化一个唯一 ID。在创建过程构建全局唯一的 BlockPoolID 比人为的配置更可靠一些。NN 将 BlockPoolID 持久化到磁盘中，在后续的启动过程中，会再次 load 并使用。 layoutVersion 是一个负整数。通常只有 HDFS 增加新特性时才会更新这个版本号。 6.4 Datanode 多目录配置: datanode 也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本。 具体配置如下1234 &lt;property&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt; 6.5 节点的动态上下线 配置相应的ssh以及环境变量和Hadoop的配置文件，直接在新节点上启动就可以实现节点的动态上线（新节点的服役） 在namenode节点上，的hdfs-site.xml文件中增加一个属性 7. 文件定时上传7.1 定时任务脚本: crontab命令格式 - 作用：用于生成cron进程所需要的crontab文件 - crontab -e 使用编辑器编辑当前的crontab文件 具体的命令格式 12345678minute hour day-of-month month-of-year day-of-week commands分 时 日 月 星期 要运行的命令minute：表示一个小时中的哪一分钟 [0,59]hour：表示一天中的哪个小时 [0,23]day-of-month：一月中的哪一天 [1,31]month-of-year：一年中的哪一月 [1,12]day-of-week：一周中的哪一天[0,6] 0表示星期日commands：要执行的命令 注意事项 全都不能为空，必须填入，不知道的值使用通配符*表示任何时间 每个时间字段都可以指定多个值，不连续的值用“,”间隔，连续的值用-间隔。 命令应该给出绝对路径 用户必须具有运行所对应的命令或程序的权限 小案例 12345678910111213案例1:每天4点备份 0 4 * * * 案例2:每周二，周五，下午6点 的计划任务 0 18 * * 2,5 案例3:1到3月份，每周二周五，下午6点的计划任务 0 18 * 1-3 2,5 案例4:周一到周五下午，5点45提醒员工15分钟后下班电脑自动关机 45 17 * * 1-5 /usr/bin/wall &lt; /opt/message 0 18 * * 1-5 /sbin/shutdown -h now 案例5:公司的计划任务， 12点14点，检查apache服务是否启动 */2 12-14 * 3-6,9-12 1-5 每隔10秒执行一次 12345* * * * * sleep 10; /bin/date &gt;&gt;/opt/test/date.log* * * * * sleep 20; /bin/date &gt;&gt;/opt/test/date.log* * * * * sleep 30; /bin/date &gt;&gt;/opt/test/date.log* * * * * sleep 40; /bin/date &gt;&gt;/opt/test/date.log* * * * * sleep 50; /bin/date &gt;&gt;/opt/test/date.log 每隔1分钟产生一个日志文件，然后将文件上传至Hadoop集群 1234#!/bin/bashfilename=test-$(date +%Y%m%d%H%M%S).log/bin/date &gt; /opt/test/$filename/opt/apps/Hadoop/hadoop-2.7.6/bin/hadoop fs -put /opt/test/$filename /logs/ 8. Hadoop 归档8.1 理论概述每个文件均按块存储，每个块的元数据存储在 namenode 的内存中，因此 hadoop 存储小文件会非常低效。因为大量的小文件会耗尽 namenode 中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个 1MB 的文件以大小为 128MB 的块存储，使用的是 1MB 的磁盘空间，而不是 128MB。Hadoop 存档文件或 HAR 文件，是一个更高效的文件存档工具，它将文件存入 HDFS 块，在减少 namenode 内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop 存档文件可以用作 MapReduce 的输入。8.2 解决存储小文件方法之一Hadoop归档文件或har文件，是一个更高效的文件归档工具，它将文件存入HDFS块，在减少namenode内存使用的同时，允许对文件进行访问。具体来说，Hadoop归档文件对内还是一个个独立的文件，对namenode而言则是一个整体，减少了namenode的内存占用。8.3 案例: 启动 yarn 进程 将/logs/目录中的所有文件归档为一个叫testhar.har文件，并把归档后的文件存储在/目录下 1hadoop archive -archiveName testhar.har -p /logs / 查看归档文件 1234567 [root@master test]# hadoop fs -lsr har:///testhar.har lsr: DEPRECATED: Please use 'ls -R' instead. -rw-r--r-- 2 root supergroup 43 2018-07-31 22:32 har:///testhar.har/test-20180731223201.log -rw-r--r-- 2 root supergroup 43 2018-07-31 22:36 har:///testhar.har/test-20180731223602.log -rw-r--r-- 2 root supergroup 43 2018-07-31 22:37 har:///testhar.har/test-20180731223701.log ##解归档[root@master data]# hadoop fs -cp har:///test/test.har /my 9. 回收站配置修改 core-site.xml，配置垃圾回收时间为 10 分钟。 1234&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;10&lt;/value&gt;&lt;/property&gt; 修改访问垃圾回收站用户名称进入垃圾回收站用户名称，默认是 dr.who，修改为 root 用户 1234&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt; 通过程序删除的文件不会经过回收站，需要调用 moveToTrash()才进入回收站 12Trash trash = New Trash(conf);trash.moveToTrash(path); 恢复回收站数据 1hadoop fs -mv /user/root/.Trash/Current/aaa.log / 清空回收站 1hdfs dfs -expunge]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-Flume]]></title>
    <url>%2F2019%2F04%2F25%2F1_Hadoop-Flume%2F</url>
    <content type="text"><![CDATA[Hadoop-FlumeFlume简介 Flume提供了一个分布式的，可靠的，对大数据量的日志进行高效的收集，聚集，转移的服务，只能在Linux下使用。 基于流式框架，容错性腔，比较灵活简单 可以用对数据进行实时采集，Kafka（卡弗卡）实时采集。Spark，Storm对数据进行实时的处理。impala对数据进行实时的查询。Hive做离线数据处理，MR离线数据处理。 Flume的架构![image_1cl070n9v11gkg9h13fh1tbs1k619.png-22.8kB][1]Source用于采集数据，简单来说就是和数据的来源进行对接，然后将采集的数据流传输到Channel。Channel简单来说就是将Source和Sinks连接起来，类似于一个队列Sink从Channel采集数据，然后将数据写到目标（可以使Source，也可以是HDFS，HBase等等）Event数据的传输单元。也就是事件，类似于Java中的bean类。数据传输source 可以监控某个文件或数据流，或端口等等，一旦数据源产生新的数据，拿到数据后，将数据分装成一个事件（Event），然后放入（put）到 Channel 中然后（Submit）提交，Channel 队列先进先出，Sink就去Channel 队列拉去数据，然后写入到目标中（HDFS或者其他的目标）Flume安装下载[下载地址][2] http://flume.apache.org/download.html解压12[root@master Flume]# tar -zxvf apache-flume-1.8.0-bin.tar.gz[root@master Flume]# mv apache-flume-1.8.0-bin flume-1.8.0修改配置1234567891011[root@master conf]# cp flume-env.sh.template flume-env.sh[root@master conf]# vim flume-env.sh## 添加以下内容export JAVA_HOME=/opt/apps/Java/jdk1.8.0_172export FLUME_HOME=/opt/apps/Flume/flume-1.8.0export FLUME_CLASSPATH=$FLUME_HOME/lib[root@master conf]# vim /etc/profile#Flume环境变量export FLUME_HOME=/opt/apps/Flume/flume-1.8.0export PATH=$PATH:$FLUME_HOME/binFlume实操案例1：监控端口的数据流安装 telnet 工具1[root@master conf]# yum install telnet创建Flume的agent配置文件1234567891011121314151617181920212223[root@master flume]# vim flume-telnet.conf# 配置agent中的名字a1.sources = r1a1.sinks = k1a1.channels = c1# 配置sourcea1.sources.r1.type = netcata1.sources.r1.bind = mastera1.sources.r1.port = 6666# 配置sinka1.sinks.k1.type = logger#配置channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100#建立联系a1.sources.r1.channels = c1a1.sinks.k1.channel = c1运行端口和Flume监控12345678## 启动Flume[root@master flume]# flume-ng agent \--conf conf \--conf-file ./flume-telnet.conf \--name a1 \-Dflume.root.logger==INFO,console## 往端口中写数据[root@master conf]# telnet master 6666案例2：实时读取本地文件到HDFS 监控Hive的日志文件，将日志文件的内容上传到HDFS 创建相应的配置文件123456789101112131415161718192021222324252627282930313233343536373839[root@master flume]# vim flume-hdfs.confa2.sources = r2a2.sinks = k2a2.channels = c2## sourcea2.sources.r2.type = execa2.sources.r2.command = tail -F /tmp/root/hive.log## sinka2.sinks.k2.type = hdfs#文件上传的HDFS路径a2.sinks.k2.hdfs.path = hdfs://master:9000/flume/%y-%m-%d/%H-%M#文件的前缀a2.sinks.k2.hdfs.filePrefix = hive-log-#是否按照时间滚动产生新的文件夹a2.sinks.k2.hdfs.round = true#按照多长时间滚动一次a2.sinks.k2.hdfs.roundValue = 1#时间的单位a2.sinks.k2.hdfs.roundUnit = minute#滚动产生新的文件a2.sinks.k2.hdfs.rollInterval = 30a2.sinks.k2.hdfs.rollSize = 125829120#设置事件多少个之后产生新的文件a2.sinks.k2.hdfs.rollCount = 30a2.sinks.k2.hdfs.fileType = DataStream#hdfs.minBlockReplicas这个就是block块的数目a2.sinks.k2.hdfs.useLocalTimeStamp = true##Channela2.channels.c2.type = memorya2.channels.c2.capacity = 1000a2.channels.c2.transactionCapacity = 100##关联a2.sources.r2.channels = c2a2.sinks.k2.channel = c2案例3：实时目录文件到HDFS配置文件123456789101112131415161718192021222324252627282930313233343536373839404142[root@master flume]# vi flume-dir.conf a3.sources = r3a3.sinks = k3a3.channels = c3## sourcea3.sources.r3.type = spooldira3.sources.r3.spoolDir = /opt/test/flume/dir#a3.sources.r3.includePattern 包含哪些文件，后面跟的 .log是正则表达式#a3.sources.r3.ignorePattern 忽略哪些文件 .tmp## sinka3.sinks.k3.type = hdfs#文件上传的HDFS路径a3.sinks.k3.hdfs.path = hdfs://master:9000/flume/dir/%y-%m-%d/%H-%M#文件的前缀a3.sinks.k3.hdfs.filePrefix = dir-#是否按照时间滚动产生新的文件夹a3.sinks.k3.hdfs.round = true#按照多长时间滚动一次a3.sinks.k3.hdfs.roundValue = 1#时间的单位a3.sinks.k3.hdfs.roundUnit = hour#滚动产生新的文件a3.sinks.k3.hdfs.rollInterval = 300000a3.sinks.k3.hdfs.rollSize = 125829120#设置事件多少个之后产生新的文件a3.sinks.k3.hdfs.rollCount = 0a3.sinks.k3.hdfs.fileType = DataStream#hdfs.minBlockReplicas这个就是block块的数目a3.sinks.k3.hdfs.useLocalTimeStamp = true##Channela3.channels.c3.type = memorya3.channels.c3.capacity = 1000a3.channels.c3.transactionCapacity = 100##关联a3.sources.r3.channels = c3a3.sinks.k3.channel = c3注意： 上传完成的文件是以.COMPLETED结尾，这个是可以修改的 上传文件的目录会默认每隔500毫秒扫描一次。pollDelay 上传文件的时候经常使用重命名操作 案例4：一个source多个sink1234567891011121314151617181920212223242526272829303132[root@master flume]# vi flume-more.conf a1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2##sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /opt/test/flume/date.log##sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://master:9000/flume/morea1.sinks.k1.hdfs.roundUnit = houra1.sinks.k1.hdfs.rollInterval = 0a1.sinks.k1.hdfs.rollCount = 0a1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k2.type = file_rolla1.sinks.k2.sink.directory = /opt/test/flume/newa1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100a1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2 案例5：一个agent到多个agent输出 Flume1监控文件的变动，Flume1讲述传递给Flume2，Flume2将数据写到HDFS，Flume1还会将数据传递给另一个Flume3，然后Flume3将数据的变化输出到本地。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172## Flume1[root@master flume]# vim flume-more-flume1.conf a1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2a1.sources.r1.type = execa1.sources.r1.command = tail -F /opt/test/flume/date.loga1.sinks.k1.type = avroa1.sinks.k1.hostname = mastera1.sinks.k1.port = 5555a1.sinks.k2.type = avroa1.sinks.k2.hostname = mastera1.sinks.k2.port = 5556a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100a1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2## Flume2[root@master flume]# vim flume-more-flume2.conf a2.sources = r1a2.sinks = k1a2.channels = c1a2.sources.r1.type = avroa2.sources.r1.bind = mastera2.sources.r1.port = 5555a2.sinks.k1.type = hdfsa2.sinks.k1.hdfs.path = hdfs://master:9000/flume/more-flumea2.sinks.k1.hdfs.roundUnit = houra2.sinks.k1.hdfs.rollInterval = 0a2.sinks.k1.hdfs.rollCount = 0a2.sinks.k1.hdfs.fileType = DataStreama2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100a2.sources.r1.channels = c1a2.sinks.k1.channel = c1## Flume3[root@master flume]# vim flume-more-flume3.conf a3.sources = r1a3.sinks = k2a3.channels = c2a3.sources.r1.type = avroa3.sources.r1.bind = mastera3.sources.r1.port = 5556a3.sinks.k2.type = file_rolla3.sinks.k2.sink.directory = /opt/test/flume/new1a3.channels.c2.type = memorya3.channels.c2.capacity = 1000a3.channels.c2.transactionCapacity = 100a3.sources.r1.channels = c2a3.sinks.k2.channel = c2 作业多对1![image_1crpb15pe4vt1ks76fe1e731k8p9.png-60.3kB][3] 总结Flume中的参数 12345Cannot stream to table that has not been bucketed## 导出的Hive表没有分桶HiveIgnoreKeyTextOutputFormat cannot be cast to org.apache.hadoop.hive.ql.io.AcidOutputFormat## Hive表的数据格式必须为orc [下载地址]http://flume.apache.org/download.html]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-Eclipse插件编译]]></title>
    <url>%2F2019%2F04%2F25%2F1_Hadoop-Eclipse%E6%8F%92%E4%BB%B6%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[Hadoop-Eclipse插件编译1. 安装ant: 下载 下载地址 配置环境变量 12ANT_HOME=D:\SoftWare\Tools\apache-ant-1.9.13Path=原路径;%ANT_HOME%\bin 测试是否安装成功 1ant -version 2. 下载Hadoop: 下载 下载地址 解压 注： hadoop文件存放目录不要带有空格，不然编译不成功 3. 下载Eclipse: 下载下载地址 解压 4. 下载hadoop2x-eclipse-plugin源码包: 下载下载地址 5. 修改配置文件: 修改bulid.xml 123456789101112&lt;target name="compile" depends="init, ivy-retrieve-common" unless="skip.contrib"&gt; &lt;echo message="contrib: $&#123;name&#125;"/&gt; &lt;javac encoding="$&#123;build.encoding&#125;" srcdir="$&#123;src.dir&#125;" includes="**/*.java" destdir="$&#123;build.classes&#125;" debug="$&#123;javac.debug&#125;" deprecation="$&#123;javac.deprecation&#125;"&gt; &lt;classpath refid="classpath"/&gt; &lt;/javac&gt; &lt;/target&gt; 123456789101112 &lt;target name="compile" unless="skip.contrib"&gt; &lt;echo message="contrib: $&#123;name&#125;"/&gt;&lt;javac encoding="$&#123;build.encoding&#125;" srcdir="$&#123;src.dir&#125;" includes="**/*.java" destdir="$&#123;build.classes&#125;" debug="$&#123;javac.debug&#125;" deprecation="$&#123;javac.deprecation&#125;"&gt; &lt;classpath refid="classpath"/&gt; &lt;/javac&gt; &lt;/target&gt; 修改12345##以及下面的内容slf4j-api.version=1.7.25slf4j-log4j12.version=1.7.25##下面这个文件还得从其他lib目录复制过来htrace.version=3.1.0-incubating 6. 开始执行编译1D:\SoftWare\Tools\hadoop2x-eclipse-plugin-master\src\contrib\eclipse-plugin&gt;ant jar -Dhadoop.version=2.7.6 -Declipse.home=D:\SoftWare\Eclipse\work-eclipse -Dhadoop.home=F:\hadoop-2.7.67. 编译结果在一下目录]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ETL之Kettle]]></title>
    <url>%2F2019%2F04%2F25%2F1_ETL%E4%B9%8BKettle%2F</url>
    <content type="text"><![CDATA[ETL之Kettle1. 简介ETL（Extract-Transform-Load的缩写，即数据抽取、转换、装载的过程），对于企业或行业应用来说，我们经常会遇到各种数据的处理，转换，迁移，所以了解并掌握一种etl工具的使用，必不可少，这里我介绍一个我在工作中使用了3年左右的ETL工具Kettle,本着好东西不独享的想法，跟大家分享碰撞交流一下！在使用中我感觉这个工具真的很强大，支持图形化的GUI设计界面，然后可以以工作流的形式流转，在做一些简单或复杂的数据抽取、质量检测、数据清洗、数据转换、数据过滤等方面有着比较稳定的表现，其中最主要的我们通过熟练的应用它，减少了非常多的研发工作量，提高了我们的工作效率，不过对于我这个.net研发者来说唯一的遗憾就是这个工具是Java编写的。 1.1 Kettle概念Kettle是一款国外开源的ETL工具，纯java编写，可以在Window、Linux、Unix上运行，绿色无需安装，数据抽取高效稳定。 Kettle 中文名称叫水壶，该项目的主程序员MATT 希望把各种数据放到一个壶里，然后以一种指定的格式流出。 Kettle这个ETL工具集，它允许你管理来自不同数据库的数据，通过提供一个图形化的用户环境来描述你想做什么，而不是你想怎么做。 Kettle中有两种脚本文件，transformation和job，transformation完成针对数据的基础转换，job则完成整个工作流的控制。 1.2 下载和部署下载https://community.hitachivantara.com/docs/DOC-1009855安装JDk: 运行Kettle:]]></content>
      <categories>
        <category>ETL工具Kettle</category>
      </categories>
      <tags>
        <tag>ETL</tag>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cookie与Session]]></title>
    <url>%2F2019%2F04%2F25%2F1_Cookie%E4%B8%8ESessio_n%2F</url>
    <content type="text"><![CDATA[Cookie与Session1. Servlet后续详解1.1 一个Servlet可以有多个 &lt;url-pattern&gt;1.2 配置Servlet可以使用通配符（*）*：表示任意字符 A. /*：可以使用任意的字符访问当前的Servlet B. /xxx/*：xxx表示某一个模块，一般用于权限验证 C. *.action,*.do等等1.3 自定义的&lt;servlet-name&gt;注意这个内容不能为default1.4 自定义启动Servlet启动时刻在执行Servlet的初始化操作的时候，如果初始化的处理工作比较复杂，那么第一个请求的用户的体验会非常差，那么可以设置Servlet执行初始化的时刻为服务器启动的时候123456789&lt;servlet&gt; &lt;servlet-name&gt;register&lt;/servlet-name&gt; &lt;servlet-class&gt;com.zhiyou100.um.servlet.RegisterServlet&lt;/servlet-class&gt; &lt;!-- 配置Servlet启动时刻和顺序 内容：数字，数字越小初始化越早 --&gt; &lt;load-on-startup&gt;0&lt;/load-on-startup&gt; &lt;/servlet&gt; 2. HTTP协议无状态问题2.1 什么是会话会话就是在打开浏览器后和服务器可以进行多次交流，即请求与响应 注意：在一次会话中可以有多次请求和多次接受响应2.2 无状态HTTP是无状态协议，也就是没有记忆力，每次请求之间无法共享数据。这就无法知道会话什么时候结束，什么时候开始，也无法确定发出请求的用户身份。 出现的问题：请求之间无法实现数据的共享2.3 会话跟踪技术一个会话中共享数据就是会话跟踪技术。2.4 无状态问题解决方案A. 使用参数拼接在请求的URL后面，实现数据的传递 问题：可以解决数据共享的问题，但是这种方式会将参数暴露在地址栏中，不安全 B. Cookie C. Session3. Cookie3.1 Cookie是什么？Cookie：小甜点 特点：客户端的技术，将共享数据保存在客户端（即浏览器）中 在第一次请求的时候，将共享数据发送到浏览器中进行保存，以后每次请求都将之前保存的共享数据发送到到服务器。3.2 Cookie的解释![image_1cb61m8u313dp1ipq172mbqb1vpsp.png-29.9kB][1] 1234//创建一个Cookie，类似于办卡Cookie c = new Cookie("usernumber", u.getNumber());//将Cookie返回给response，即将卡交给客户resp.addCookie(c); 第一次请求 ![无标题.png-109.2kB][2] 非第一次请求 ![无标题.png-104.4kB][3] ### 3.3 Cookie的操作细节 ![image_1cfn8nm3e13e1d8ubf47kl1rl59.png-90.4kB][4] 3.3.1 创建Cookie对象，设置共享数据12Cookie c = new Cookie(String name,String value);//一个Cookie只能存储一个字符串类型的数据，不能存储其他类型的数据 3.3.2 将Cookie响应给刘篮球1response对象.addCookie(c); 3.3.3 获取请求中的Cookie信息123456Cookie[] cs = request对象.getCookies();for(Cookie c:cs)&#123; if("usernumber".equals(c.getName()))&#123; String value = c.getValue(); &#125;&#125; 3.3.4 修改Cookie中的共享数据A. 重新创建一个新的cookie，名称和要修改的数据的Cookie的名称一样 B. 先获取到要修改的cookie对象，再调用setValue(String newValue)重新设置值 注意：修改Cookie中的数据，需要再次发送给浏览器3.3.5 操作Cookie的生命周期12//默认是在关闭浏览器的时候销毁c.setMaxAge(int expiry) expiry&gt;0：设置Cookie能够存活expiry秒，即使关闭浏览器，不影响Cookie中的共享数据。比如：十天内免登录：setMaxAge(60*60*24*10) expiry=0：立即删除当前的Cookie信息 expiry&lt;0：缺省值，关闭浏览器的时候销毁![image_1cb63umlp1rap1arv1sib4umj4t2b.png-40.8kB][5] 3.3.6 删除Cookie中的共享数据1setMaxAge(0); 3.3.7 Cookie中的key和value不支持中文1234//设置编码Cookie c = new Cookie("usernumber",URLEncoder.encode(number,"UTF-8"));//获取的时候设置解码usernumber = URLDecoder.decode(value,"UTF-8"); 3.3.7 Cookie的作用范围Cookie在创建的时候会根据当前的Servlet的相对路径设置自己的路径，比如当前Servlet的路径为/login/loginServlet,则相对路径为/login/,那么只有在访问/login/下面的资源的时候，才能够将该Cookie发送到服务器。 123//设置Cookie的路径setPath(String uri);//Cookie对象.setPath("/")，表示当前项目中所有的资源都能够共享该Cookie信息 多个项目之间共享数据，则需要设置域范围，比如：wenku.baidu.compan.baidu.comCookie对象.setDomain(“baidu.com”); 3.4 Cookie的作用与缺陷3.4.1 作用是实现会话跟踪3.4.2 Cookie的缺陷1. 获取Cookie信息很麻烦 2. Cookie不支持中文 3. 一个Cookie只能存储一个字符串类型的数据 4. Cookie在浏览器中有数量的限制 一个浏览器对一个站点最多存储20条Cookie信息 一个浏览器最多只能存储300个Cookie 5. 共享数据是保存在浏览器中的，很容易造成数据的泄露（不安全）4. Session4.1 Session是什么Session是服务端的会话技术，是将数据保存在服务端。其实底层就是Cookie。4.2 Session的操作4.2.1 获取Session对象123request对象.getSession()：和参数为true的一样request对象.getSession(true)：获取Session对象，如果没有session对象，直接创建一个新的返回，缺省值request对象.getSession(false)：获取Session对象，如果没有返回null 4.2.2 设置共享数据Session对象.setAttribute(String name,Object value) Session可以存储任何类型的数据，比如登录用户的信息，可以封装到User对象中。4.2.3 修改Session中的共享数据重新设置一个同名的共享数据即可4.2.4 获取共享数据Object value = Session对象.getAttribute(String name);4.2.5 删除Session中的共享数据Session对象.removeAttribute(String name);4.2.6 销毁SessionSession对象.invalidate()4.2.7 Session的命名规范Session中共享数据的属性名的命名规范：XXX_IN_SESSION Session对象.setAttribute(&quot;USER_IN_SESSION&quot;,user)4.2.8 Session中的序列化与反序列化Session中存储的对象通常需要实现序列化接口 在网络之间传输的数据格式为二进制数据 序列化：将对象转换成二进制数据 反序列化：将二进制数据转换成对象4.2.9 Session的超时管理超时：在访问当前的资源的过程中，不和网页进行任何交互，超过设定的时间就是超时 1setMaxInactiveInterval(int interval) 在服务器中默认的配置为30分钟，通常不需要修改。 5. Cookie与Session的区别 Cookie中只能保存ASCII字符串，Session中可以保存任意类型的数据，甚至Java Bean乃至任何Java类、对象等 隐私策略不同。Cookie存储在客户端，对客户端是可见的，可被客户端窥探、复制、修改。而Session存储在服务器上，不存在敏感信息泄露的风险 有效期不同。Cookie的过期时间可以被设置很长。Session依赖于名为JSESSIONI的Cookie，其过期时间默认为-1，只要关闭了浏览器窗口，该Session就会过期，因此Session不能完成信息永久有效。如果Session的超时时间过长，服务器累计的Session就会越多，越容易导致内存溢出。 服务器压力不同。每个用户都会产生一个session，如果并发访问的用户过多，就会产生非常多的session，耗费大量的内存。因此，诸如Google、Baidu这样的网站，不太可能运用Session来追踪客户会话。 浏览器支持不同。Cookie运行在浏览器端，若浏览器不支持Cookie，需要运用Session和URL地址重写。 跨域支持不同。Cookie支持跨域访问（设置domain属性实现跨子域），Session不支持跨域访问]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Javase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH安装文档-Cloudera Manager]]></title>
    <url>%2F2019%2F04%2F25%2F1_CDH%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[CDH安装文档前言Cloudera Manager 架构![image_1cr7ui3hq12jp1qtmcic19c1sa39.png-67.2kB][1] Cloudera Manager 的架构如上图所示（cs结构），主要由如下几部分组成: 服务端/Server:Cloudera Manager 的核心。主要用于管理 web server 和应 用逻辑。它用于安装软件，配置，开始和停止服务，以及管理服务运行的集群。 代理/agent:安装在每台主机上。它负责启动和停止的进程，部署配置，触发安装和监控主机。 数据库/Database:存储配置和监控信息。通常可以在一个或多个数据库服务器上运行的多个逻辑数据库。例如，所述的 Cloudera 管理器服务和监视，后台程序使用不同的逻辑数据库。 Cloudera Repository: ：由cloudera manager 提供的软件分发库 客户端/Clients:提供了一个与 Server 交互的接口: 管理平台/Admin Console:提供一个管理员管理集群和 Cloudera Manage 的基于网页的交互界面。 API:为开发者提供了创造自定义 Cloudera Manager 程序的 API。 1. 集群规划集群节点分配|主机名|主机IP|主机内存|硬盘存储||—|—|—|—||node01|192.168.100.101|7G|70G||node02|192.168.100.102|4G|50G||node03|192.168.100.103|4G|50G||node04|192.168.100.104|4G|50G||node05|192.168.100.105|4G|50G|软件版本|软件名称|版本号||—|—||Java|1.8.0_162||MySQL|5.7.24||CentOS|CentOS-7-x86_64-DVD-1804||CDH|5.15.1|各软件安装路径|软件名|路径地址||—|—||CM|/opt/apps/cm-5.15.1||Java|/opt/apps/Java|2. 软件下载2.1 JDK 下载jdk-8u162-linux-x64.rpm 下载[下载地址][2] 注意: 1. JDK必须是64位。不要使用32位JDK。 2. 已安装的JDK必须是受支持的版本，如[CDH和Cloudera Manager支持的JDK版本][3]中所述。 3. 在相同版本的JDK必须安装在每个集群主机上。 4. 这里建议下载rpm版本，自动安装会更好一些2.2 MySQL 下载以下组件都需要数据库：Cloudera Manager Server，Oozie Server，Sqoop Server，活动监视器，Reports Manager，Hive Metastore Server，Hue Server，Sentry Server，Cloudera Navigator Audit Server和Cloudera Navigator Metadata Server。数据库中包含的数据类型及其相对大小如下： Cloudera Manager Server - 包含有关已配置的服务及其角色分配，所有配置历史记录，命令，用户和正在运行的进程的所有信息。这个是相对较小的数据库（&lt;100 MB），非常重要，必须备份。注意：重新启动进程时，将使用Cloudera Manager数据库中保存的信息重新部署每个服务的配置。如果此信息不可用，则集群无法启动或正常运行。您必须安排并维护Cloudera Manager数据库的定期备份，以便在丢失此数据库时恢复集群。 Oozie Server - 包含Oozie工作流，协调器和捆绑数据。可以长得很大。 Sqoop Server - 包含连接器，驱动程序，链接和作业等实体。相对较小。 Activity Monitor - 包含有关过去活动的信息。在大型集群中，此数据库可能会变大。只有部署了MapReduce服务，才需要配置活动监视器数据库。 Reports Manager - 跟踪磁盘利用率和处理活动。中型。 Hive Metastore Server - 包含Hive元数据。相对较小。 Hue服务器 - 包含用户帐户信息，作业提交和Hive查询。相对较小。 Sentry Server - 包含授权元数据。相对较小。 Cloudera Navigator Audit Server - 包含审计信息。在大型集群中，此数据库可能会变大。 Cloudera Navigator Metadata Server - 包含授权，策略和审计报告元数据。相对较小。 mysql-5.7.24-1.el7.x86_64.rpm-bundle.tar.gz[下载地址][4] 2.3 CDH 相关下载[manifest.json][5][CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel][6][CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel.sha1][7][cloudera-manager-centos7-cm5.15.1_x86_64.tar.gz][8] 3. 基本配置3.0 网路配置此配置在这里不再过多赘述，在centos 7安装过程中就可以将网络配置完成 3.1 修改主机名并配置hosts（所有节点）12345678910111213## 修改主机名（这里使用的是centos 7的命令，建议在安装的时候就配置好）[root@localhost ~]# hostnamectl set-hostname node01## 修改hosts文件[root@node01 ~]# vim /etc/hosts## 注释前两行，添加以下内容#127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4#::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.100.101 node01192.168.100.102 node02192.168.100.103 node03192.168.100.104 node04192.168.100.105 node05 3.2 小工具安装（所有节点）1234567891011121314## 安装vim编辑器[root@node01 ~]# yum -y install vim## 安装上传下载工具[root@node01 ~]# yum -y install lrzsz## 安装pstree[root@node01 ~]# yum -y install psmisc[root@node01 init.d]# yum -y install httpd[root@node01 init.d]# yum -y install mod_ssl[root@node01 init.d]# yum -y install rpcbind[root@node01 ~]# systemctl start rpcbind[root@node01 java]# yum install -y python-lxml 3.3 NTP服务器配置（所有节点，用于几个节点的时间同步）集群中所有主机必须保持时间同步，如果时间相差较大会引起各种问题。 1234567891011[root@node01 ~]# vim /etc/ntp.conf## 修改内容为server 0.pool.ntp.orgserver 1.pool.ntp.orgserver 2.pool.ntp.org[root@node01 ~]# systemctl start ntpd[root@node01 ~]# systemctl restart ntpd[root@node01 ~]# systemctl enable ntpd[root@node01 ~]# ntpdate -u time.nist.gov 2 Nov 13:12:53 ntpdate[31560]: step time server 132.163.96.1 offset 1812.928750 sec[root@node01 ~]# hwclock --systohc 3.4 关闭防火墙（所有节点）12345678## 关闭防火墙[root@node01 ~]# systemctl stop firewalld## 禁止开机启动防火墙[root@node01 ~]# systemctl disable firewalld## 关闭SELinux（重启生效）## SELinux主要作用就是最大限度地减小系统中服务进程可访问的资源（最小权限原则）。如果开启，可能会导致文件权限修改不了等问题[root@node01 ~]# vim /etc/selinux/configSELINUX=disabled 3.5 SSH免密码（所有节点）12345678910111213141516171819202122232425262728293031323334353637383940414243## 先ssh一下，产生相应的隐藏目录[root@node01 Java]# ssh localhost## 进入到用户主目录的.ssh目录[root@node01 Java]# cd ~/.ssh## 产生公钥和私钥[root@node01 .ssh]# ssh-keygen -t rsa -P ''## 将每台机器上的id_rsa.pub公钥内容复制到authorized_keys文件中[root@node01 .ssh]# cp id_rsa.pub authorized_keys## 将所有的authorized_keys文件进行合并（最简单的方法是将其余节点的文件内容追加到node01主机上）[root@node02 .ssh]# cat ~/.ssh/authorized_keys | ssh root@node01 'cat &gt;&gt; ~/.ssh/authorized_keys'[root@node03 .ssh]# cat ~/.ssh/authorized_keys | ssh root@node01 'cat &gt;&gt; ~/.ssh/authorized_keys'[root@node04 .ssh]# cat ~/.ssh/authorized_keys | ssh root@node01 'cat &gt;&gt; ~/.ssh/authorized_keys'[root@node05 .ssh]# cat ~/.ssh/authorized_keys | ssh root@node01 'cat &gt;&gt; ~/.ssh/authorized_keys'## 查看node01上的authorized_keys文件内容，类似如下即可eys[root@node01 .ssh]# more authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD4AUA+aUYpSYEWnOhD8apy7nNGZvwYGQhn3aA8Q6cn2fLYN2NVipOeAqXmFD4ao0gPFoqzt+ylxb3xGZ5QHKLQpDGSHyspXaJV+ow13IwDKpGuWjN9NgrldCw2eCYDBffEVnWVSRTSGw7KdJYdmE8cBxziDDQPBBIiDz8uXLeEf6RafvgJXm+NmGVIxKs+dj/Bve7dIEVBtCxhEWcUQZXGEGl0H0JxwRWRACgYXgJg6oVnaRA8T0b1CUdvekiH7hnoWAgSSHgFnTdhzcicAZeGESCCjJDqYmm8Njbpr/X98uyBOtojMS1d9kYqtO85QN7/VG/n1Hc5aLvZLYFDvwN9 root@node01ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC8ZiYNqOkm3HfheBehnA/D+1qk1T/7/DmNjfCTgEHG4HhrhghC7RUH2Nbrar29Ad0LmH48jbtgHeftIadoXvEqojc9wOKuIQkJmyu/7ab0t35/W4fbA8x+dH23Gr1ZV05OiqaT/QrCbaOxx/oC2QSyh5mukP5PIyMNlskO4ywgr2QINST5qur4F7xfR+yzZH/j8Bo9aUKlKpSgXacfkhnWBV5L8BJEnEgAUIutIN8ZOIF08HW7YO8sYsyAy4Ram4M81SuCpe01DfbWInnX37j5lthcw3I0cpZ4txhSZsM1BtRUpNsU24TB8ryCEIiGHvFJd6ljgfG2kKoFqgUUSqZ9 root@node04ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCSZxq0qWHdUIDOYypw2qzrt9rNwUOyABUxyVjuxpDEOD3td7X5D9h+fQjPYwhwUcVHNtEApsvvV2Rbp6gn8eGUIBlkX2rJCfbsJIJSJTRq9E8LiRcDSNdbqqjzIpf/qbIcYRl/2MBxS60k2wQE6/cCBwWpil+YyIplttI58y/CfUt6wygy2LEeMox4VrljgdTj27hhrIIzL7K8BLJcpPS9hgK2oMd3/YNeM8RhStR6SdlSNd06s9GD0xMDfY1IzAqd+7DzGp3WZOuQAOjgD2Ej1bMjta5B4pg84HH+qLdwU2i+Yt/ycDAZBREadVWBWS+DTQdPof5wYgDXXOET4SU1 root@node02ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDN177q22IpskSzCzLQBRGTguJWBKR+EJ/qgE0f4vNOJgHvZyWOObabHQOnktRonKEW4ShKyK1w9xf406YK7FTeyOHrYb+j3Zi+A0YbWgjScd6XxmMLQDVmVgJhUMSgAVL+4s9rrQ1g5baVU8W5O2xr0dVSXXWF5khtV5aCfxU/O1Mkmbt9WiCfp5I6biwXQj41Sm77l8EDSm1rijmXi0zc3uxtEqvqDJXV0A6s7vpQLJ9sw4kkwFdki1hwU9irG+E9ci70egZr3wuBYEdGzmJ9UNiCUvNjS5S0Q4FvjptyOcfad/7Oqbch2JNCWRSLu5Vyi9bcK+ulKOtmS+IX6Ivl root@node05ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC4nV01w7sPiOBO8Be/8OfTFYZxC3xSoR4Tm1wgXbUa3nJpsjqw/blIeEUWwb18MRZ7V1EEvJw+/HXh4ZiEALHAb/KxeH/BNifJJ4BZO/SMXOmQKupX4LzSzBoBAxDvY8c9+E2EImP2v1Bd4Hqf7Wfs/UJlW0BAgNbDJTpje+CFQnlOQPQC2zxuhWUm2yuRe2kuxAki5m7j1N2JeQFyUtTArXeWRRfRpTCK56MGfGYET65Wb1uFYANklo1LmcFHgw41haWj3suRjm3uqwSLfn3LCsY5lOGn3L6iuNBoVkIuEcllET4dIUiX8Yaajs1/m/Lqcx2GrhOJHQ35HKl+AUFf root@node03## 将node01上的authorized_keys文件分发到其他主机上[root@node01 .ssh]# scp ~/.ssh/authorized_keys root@node02:~/.ssh/[root@node01 .ssh]# scp ~/.ssh/authorized_keys root@node03:~/.ssh/[root@node01 .ssh]# scp ~/.ssh/authorized_keys root@node04:~/.ssh/[root@node01 .ssh]# scp ~/.ssh/authorized_keys root@node05:~/.ssh/## 每台机器之间进行ssh免密码登录操作，包括自己与自己 3.6 JDK安装（所有节点）1234567891011121314151617181920212223242526272829## 创建Java相应目录[root@node01 ~]# mkdir -p /opt/apps/Java## 通过rz命令上传文件[root@node01 ~]# cd /opt/apps/Java[root@node01 Java]# rz## 建议使用以下方式安装——省事而且不会出错[root@node01 Java]# rpm -ivh jdk-8u162-linux-x64.rpm 准备中... ################################# [100%]正在升级/安装... 1:jdk1.8-2000:1.8.0_162-fcs ################################# [100%]Unpacking JAR files... tools.jar... plugin.jar... javaws.jar... deploy.jar... rt.jar... jsse.jar... charsets.jar... localedata.jar...## 测试安装是否生效[root@node01 Java]# java -versionjava version "1.8.0_162"Java(TM) SE Runtime Environment (build 1.8.0_162-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.162-b12, mixed mode)## 在环境变量中配置JAVA_HOME[root@node01 Java]# vim /etc/profile#添加以下内容export JAVA_HOME=/usr/java/jdk1.8.0_162 { name:”zhangsan”, } 3.7 安装MySQL数据库（主节点安装即可）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111## 注意：MySQL只需要在CM server主机上安装即可## 首先要写在数据库mariadb[root@node01 MySQL]# rpm -qa | grep mariadbmariadb-libs-5.5.56-2.el7.x86_64[root@node01 MySQL]# rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64## 否则会报一下错误error: Failed dependencies: mysql-community-common(x86-64) &gt;= 5.7.9 is needed by mysql-community-libs-5.7.24-1.el7.x86_64 mariadb-libs is obsoleted by mysql-community-libs-5.7.24-1.el7.x86_64## 创建相应的MySQL目录（并不会安装在这个目录）[root@node01 ~]# mkdir /opt/apps/MySQL[root@node01 ~]# cd /opt/apps/MySQL## 使用rz命令上传[root@node01 MySQL]# rz## 解包（注意：不是解压）[root@node01 MySQL]# tar -xvf mysql-5.7.24-1.el7.x86_64.rpm-bundle.tar## 开始安装：注意安装顺序[root@node01 MySQL]# rpm -ivh mysql-community-common-5.7.24-1.el7.x86_64.rpm[root@node01 MySQL]# rpm -ivh mysql-community-libs-5.7.24-1.el7.x86_64.rpm[root@node01 MySQL]# rpm -ivh mysql-community-libs-compat-5.7.24-1.el7.x86_64.rpm## 不安装的话会出现错误，缺少这个依赖[root@node01 MySQL]# yum -y install net-tools[root@node01 MySQL]# rpm -ivh mysql-community-client-5.7.24-1.el7.x86_64.rpm[root@node01 MySQL]# rpm -ivh mysql-community-server-5.7.24-1.el7.x86_64.rpm## 启动mysql服务[root@node01 MySQL]# systemctl start mysqld## 修改配置文件：详细看原来的Hadoop安装文档上的MySQL安装[root@node01 MySQL]# vim /etc/my.cnf# CDH官方推荐以下配置transaction-isolation = READ-COMMITTEDkey_buffer_size = 32Mmax_allowed_packet = 32Mthread_stack = 256Kthread_cache_size = 64query_cache_limit = 8Mquery_cache_size = 64Mquery_cache_type = 1max_connections = 550log_bin=/var/lib/mysql/mysql_binary_logserver_id=1binlog_format = mixedread_buffer_size = 2Mread_rnd_buffer_size = 16Msort_buffer_size = 8Mjoin_buffer_size = 8M# InnoDB settingsinnodb_file_per_table = 1innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 64Minnodb_buffer_pool_size = 4Ginnodb_thread_concurrency = 8innodb_flush_method = O_DIRECTinnodb_log_file_size = 512Msql_mode=STRICT_ALL_TABLES# 取消密码安全策略validate_password=offcharacter_set_server=utf8init_connect='SET NAMES utf8'#重新启动mysql服务[root@master opt]# systemctl restart mysqld## 查看初始化密码[root@node01 MySQL]# cat /var/log/mysqld.log |grep password2018-10-30T17:06:31.899394Z 1 [Note] A temporary password is generated for root@localhost: 54epwShQg9)B2018-10-30T17:14:39.775316Z 0 [Note] Shutting down plugin 'validate_password'2018-10-30T17:14:40.796592Z 0 [Note] Shutting down plugin 'sha256_password'2018-10-30T17:14:40.796596Z 0 [Note] Shutting down plugin 'mysql_native_password'2018-10-30T17:14:42.674505Z 0 [Note] Plugin 'validate_password' is disabled.## 进入mysql客户端[root@node01 MySQL]# mysql -uroot -p## 修改密码mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY '123456';## 修改root的远程访问权限## root代表用户名 , %代表任何主机都可以访问 , 123456为root访问的密码mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION; ## flush privileges刷新MySQL的系统权限,使其即时生效，否则就重启服务器mysql&gt; FLUSH PRIVILEGES;## 创建数据库，以备后用mysql&gt; CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;Query OK, 1 row affected (0.01 sec)mysql&gt; CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;Query OK, 1 row affected (0.00 sec)## 退出mysql&gt; exit; 3.7 安装Cloudera Manager Server （主节点操作）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354## 上传cloudera-manager-centos7-cm5.15.1_x86_64.tar.gz到/opt/apps目录[root@node01 apps]# rz## 解压[root@node01 apps]# tar -zxvf cloudera-manager-centos7-cm5.15.1_x86_64.tar.gz## 删除这个压缩包[root@node01 apps]# rm -rf cloudera-manager-centos7-cm5.15.1_x86_64.tar.gz## ls查看如下[root@node01 apps]# lscloudera cm-5.15.1 Java MySQL## 为Cloudera Manager建立数据库## 上传mysql驱动jar包到/usr/share/java[root@node01 apps]# mkdir -p /usr/share/java[root@node01 apps]# cd /usr/share/java/[root@node01 java]# rz[root@node01 java]# mv mysql-connector-java-5.1.47-bin.jar mysql-connector-java.jar [root@node01 java]# lsmysql-connector-java.jar## 启动脚本：进入到/opt/apps/cm-5.15.1/share/cmf/schema目录[root@node01 schema]# ./scm_prepare_database.sh mysql scm rootEnter SCM password: JAVA_HOME=/usr/java/jdk1.8.0_162Verifying that we can write to /opt/apps/cm-5.15.1/etc/cloudera-scm-serverCreating SCM configuration file in /opt/apps/cm-5.15.1/etc/cloudera-scm-servergroups: cloudera-scm: no such userExecuting: /usr/java/jdk1.8.0_162/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/apps/cm-5.15.1/share/cmf/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /opt/apps/cm-5.15.1/etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.Wed Oct 31 01:25:14 CST 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.2018-10-31 01:25:15,925 [main] INFO com.cloudera.enterprise.dbutil.DbCommandExecutor - Successfully connected to database.All done, your SCM database is configured correctly!## 配置Cloudera Manager Agent(每台节点)## 修改CM主机地址：目录/opt/apps/cm-5.15.1/etc/cloudera-scm-agent[root@node01 cloudera-scm-agent]# vim config.ini## 修改如下# Hostname of the CM server.server_host=node01## 拷贝cm目录到其他节点[root@node01 apps]# scp -r cm-5.15.1 root@node02:/opt/apps/[root@node01 apps]# scp -r cm-5.15.1 root@node03:/opt/apps/[root@node01 apps]# scp -r cm-5.15.1 root@node04:/opt/apps/[root@node01 apps]# scp -r cm-5.15.1 root@node05:/opt/apps/## 准备Parcels，用以安装CDH## 上传下面几个文件到指定目录：/opt/apps/cloudera/parcel-repo# CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel# CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel.sha# manifest.json## 创建cloudera-scm用户（每台节点均执行）[root@node01 ~]# useradd --system --home=/opt/apps/cm-5.15.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment "Cloudera SCM User" cloudera-scm 4. 创建快照4. 启动4.1 启动cloudera-scm-server123[root@node01 ~]# cd /opt/apps/cm-5.15.1/etc/init.d/[root@node01 init.d]# ./cloudera-scm-server start 4.2 启动cloudera-scm-agent12[root@node01 ~]# cd /opt/apps/cm-5.15.1/etc/init.d/[root@node01 init.d]# ./cloudera-scm-agent start 4.3 进入web界面开始安装http://192.168.100.101:7180/cmf/login![image_1cr86pb1v1sls5tg1lml1mjn1ql9m.png-48.4kB][9]![image_1cr86sad41aje1lcn11hj1llc1ca013.png-202kB][10]![image_1cr8704k3q8skga69n1thj2a71g.png-130.4kB][11]![image_1cr871dst164r6iq65713rfi781t.png-152.3kB][12]![image_1cr875jm9da7pvo1kd515fd1efg2a.png-127.8kB][13]![image_1cr877kuc1rv54qi1upro8n13be2n.png-127.5kB][14]![image_1cr87bf9h1hiq4gt1r883jb1d9f34.png-132.6kB][15] 4.3.1 接下来重启server123[root@node01 init.d]# ./cloudera-scm-server restartStopping cloudera-scm-server: [ 确定 ]Starting cloudera-scm-server: [ 确定 ] 4.3.2 重启之后，仍然正常登录到这一步![image_1cr87ja8e1u98t3seek1s9311b03h.png-143.8kB][16]![image_1cr87loa2t011ik1ts41ngp1jq33u.png-72.4kB][17]![image_1cr87vrvi1l9vhtl1jmv1scbioh4b.png-75.2kB][18]![image_1cr882f9jd3r1bs31498vrpvd4o.png-153kB][19] 123456## 出现以上警告，则在所有主机上执行下面的命令[root@node01 init.d]# echo 10 &gt; /proc/sys/vm/swappiness[root@node01 init.d]# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag[root@node01 init.d]# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled[root@node01 init.d]# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag &gt;&gt; /etc/rc.local[root@node01 init.d]# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled &gt;&gt; /etc/rc.local ![image_1cr887909t6q1rfggtjiivhv055.png-130.7kB][20]![image_1cr888o2t10sdm601hlg1njn17ho5i.png-149.8kB][21]![image_1cr88g66aomtgone8llrqi4e5v.png-117.1kB][22]![image_1cr88k1fc4gdvi6qob8rn1p4k6s.png-155.8kB][23] ![image_1cr88lvu8e891vjovde1mm5ht779.png-151kB][24] ![image_1cr4i1pr2189r166rn3ucn1k0q1p.png-120.5kB][25]![image_1cr4i2ij41mfqv4qa4iv841mn026.png-108.8kB][26]![image_1cr4i3odi1ndsh1qiehvms1a3j.png-100.5kB][27]![image_1cr4i48vm1ih1p6s1hlhirh1jd540.png-114.4kB][28]后面一步一步走即可 4.4 设置开机自启1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162## 1. 复制脚本 cloudera-scm-server 到 server 主机上/etc/init.d## 2. 复制脚本 cloudera-scm-agent 到所有的 agent 主机上## 3. 设置agent开机自启# 3.1 每台主机均执行添加到开机自启项[root@node01 init.d]# chkconfig --add cloudera-scm-agent# 3.2 设置自启[root@node01 init.d]# chkconfig cloudera-scm-agent on# 3.3 展示自启内容，2-5开启说明配置成功[root@node01 init.d]# chkconfig --list cloudera-scm-agent 注：该输出结果只显示 SysV 服务，并不包含原生 systemd 服务。SysV 配置数据可能被原生 systemd 配置覆盖。 要列出 systemd 服务，请执行 'systemctl list-unit-files'。 查看在具体 target 启用的服务请执行 'systemctl list-dependencies [target]'。cloudera-scm-agent 0:关 1:关 2:开 3:开 4:开 5:开 6:关## 4. 设置server开机自启[root@node01 init.d]# chkconfig --add cloudera-scm-server [root@node01 init.d]# chkconfig cloudera-scm-server on[root@node01 init.d]# chkconfig --list cloudera-scm-server 注：该输出结果只显示 SysV 服务，并不包含原生 systemd 服务。SysV 配置数据可能被原生 systemd 配置覆盖。 要列出 systemd 服务，请执行 'systemctl list-unit-files'。 查看在具体 target 启用的服务请执行 'systemctl list-dependencies [target]'。cloudera-scm-server 0:关 1:关 2:开 3:开 4:开 5:开 6:关## 5. 修改配置信息，否则失败# 5.1 修改cloudera-scm-server与agent脚本中的CMF_DEFAULTS[root@node01 init.d]# vim /etc/init.d/cloudera-scm-agentCMF_DEFAULTS=$&#123;CMF_DEFAULTS:-/opt/apps/cm-5.15.1/etc/default&#125;## 6. 错误信息集锦## 6.1 错误1[root@node01 init.d]# systemctl status cloudera-scm-server ● cloudera-scm-server.service - LSB: Cloudera SCM Server Loaded: loaded (/etc/rc.d/init.d/cloudera-scm-server; bad; vendor preset: disabled) Active: failed (Result: exit-code) since 日 2018-11-04 09:24:49 CST; 6min ago Docs: man:systemd-sysv-generator(8) Process: 943 ExecStart=/etc/rc.d/init.d/cloudera-scm-server start (code=exited, status=1/FAILURE)11月 04 09:24:49 node01 systemd[1]: Starting LSB: Cloudera SCM Server...11月 04 09:24:49 node01 cloudera-scm-server[943]: File not found: /usr/sbin/cmf-server11月 04 09:24:49 node01 systemd[1]: cloudera-scm-server.service: control process exite...s=111月 04 09:24:49 node01 systemd[1]: Failed to start LSB: Cloudera SCM Server.11月 04 09:24:49 node01 systemd[1]: Unit cloudera-scm-server.service entered failed state.11月 04 09:24:49 node01 systemd[1]: cloudera-scm-server.service failed.Hint: Some lines were ellipsized, use -l to show in full.## 6.1 解决方案修改cloudera-scm-server与agent脚本中的CMF_DEFAULTS 错误总结123456789## Unable to verify database connection.[root@node01 java]# yum install -y python-lxml## Hue启动出错[root@node01 init.d]# yum -y install httpd[root@node01 init.d]# yum -y install mod_ssl## No portmap or rpcbind service is running on this host. Please start portmap or rpcbind service before attempting to start the NFS Gateway role on this host.[root@node01 init.d]# yum -y install rpcbind [2](https://download.oracle.com/otn/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-8u181-linux-x64.tar.gz?AuthParam=1540826002_cc60927e5e66e3999064c255b8e675da [3](https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_jdk [4](https://dev.mysql.com/downloads/file/?id=481064 [5](http://archive.cloudera.com/cdh5/parcels/latest/manifest.json [6](http://archive.cloudera.com/cdh5/parcels/5.15.1/ [7](http://archive.cloudera.com/cdh5/parcels/5.15.1/ [8](http://archive.cloudera.com/cm5/cm/5/]]></content>
      <categories>
        <category>大数据应用</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux第一天]]></title>
    <url>%2F2019%2F04%2F25%2F1_Linux%E7%AC%AC%E4%B8%80%E5%A4%A9__%2F</url>
    <content type="text"><![CDATA[Linux第一天为什么要学习Linux因为后期的Hadoop，Spark等都要运行在Linux上 Linux简介Unix 1969年Linux 1991年 Linux是一个自由和开放源代码的操作系统，有很多不同的发行版本，使用的都是Linux内核 开发版和发行版：就是基于Linux的内核，增加一些应用程序，然后增加一些桌面，就是发行版Linux的发行版主要有两大阵营Redhat：Redhat，CentOS，suse等 Redhat与CentOS的区别在于一个提供后期的服务支持，一个不提供 Debian：Ubuntu，Debian等等Linux的应用领域 企业服务器 嵌入式系统 大型电影的特效处理 Linux特点 开源的 多用户，多任务 速度性能较高 一般情况下，不使用图形化界面 CentOS社区版主流：Redhat和centos区别：Redhat和centos相差不大，CentOS基于Redhat的一个开发源代码的企业级Linux发行版，CentOS是不提供后期维护服务的，要想提供就使用Redhat 下载https://www.centos.org/Linux与windows windows:图形化界面/鼠标,linux命令/键盘 windows不区分大小写，Linux严格区分大小写 在Linux中一切皆文件，没有所谓的扩展名，扩展名只是为了让管理员能够看到这个文件是什么文件而已 常见标识 ~ 表示当前用户的主目录/ 表示根目录 Linux的目录结构 /bin: (binaries) 存放系统命令的目录，所有用户都可以执行。/sbin : (super user binaries) 保存和系统环境设置相关的命令，只有超级用户可以使用这些命令，有些命令可以允许普通用户查看。/usr/bin：存放系统命令的目录，所有用户可以执行。这些命令和系统启动无关，单用户模式下不能执行/usr/sbin：存放根文件系统不必要的系统管理命令，超级用户可执行/root: 存放root用户的相关文件,root用户的家目录。宿主目录 超级用户/home：用户缺省宿主目录 eg:/home/spark/tmp：(temporary)存放临时文件/etc：(etcetera)系统配置文件/usr：（unix software resource）系统软件共享资源目录，存放所有命令、库、手册页等/proc：虚拟文件系统，数据保存在内存中，存放当前进程信息/boot：系统启动目录 /dev：(devices)存放设备文件/sys :虚拟文件系统，数据保存在内存中，主要保存于内存相关信息/lib：存放系统程序运行所需的共享库/lost+found：存放一些系统出错的检查结果。/var：(variable) 动态数据保存位置，包含经常发生变动的文件，如邮件、日志文件、计划任务等/mnt：(mount)挂载目录。临时文件系统的安装点，默认挂载光驱和软驱的目录/media:挂载目录。 挂载媒体设备，如软盘和光盘/misc:挂载目录。 挂载NFS服务/opt: 第三方安装的软件保存位置。 习惯放在/usr/local/目录下/srv : 服务数据目录 Linux常用命令命令的格式: 命令名 [-选项] [参数]说明中括号表示可有可无，大部分命令均会遵从这个格式cdchange directory 切换目录 语法：cd [目录] 1234567891011[zhang@bogon ~]$ cd /[zhang@bogon /]$ cd ..[zhang@bogon /]$ cd /etc/[zhang@bogon etc]$ cd ..[zhang@bogon /]$ cd /etc[zhang@bogon etc]$ cd .#显示并打开到上一次操作的目录[zhang@bogon etc]$ cd -/etc#进入到当前用户的主目录/home/zhang[zhang@bogon etc]$ cd ~lslist 作用: 显示目录和文件 语法：ls [-alrRd] [目录]- a: all,显示所有的文件 - l: long,显示详细信息 - r: reverse,逆序排序 - R: 递归显示当前目录下的所有的目录 - h: 友好显示，大小可以按照K显示 - t: 按照修改时间排序（降序） - ll: 等价于ls -l pwdprint working directory 作用: 在控制台显示当前工作的目录 语法: pwd [-LP]- L 显示的链接的路径，当前路径，默认 - P 物理路径 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179[zhang@bogon ~]$ ls公共 模板 视频 图片 文档 下载 音乐 桌面[zhang@bogon ~]$ ls -a. .bash_history .bash_profile .cache .esd_auth .local 公共 视频 文档 音乐.. .bash_logout .bashrc .config .ICEauthority .mozilla 模板 图片 下载 桌面[zhang@bogon ~]$ ls -l总用量 0drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面[zhang@bogon ~]$ ls -l -a总用量 36drwx------. 14 zhang zhang 4096 7月 23 2018 .drwxr-xr-x. 3 root root 19 7月 23 2018 ..-rw-------. 1 zhang zhang 89 7月 23 14:46 .bash_history-rw-r--r--. 1 zhang zhang 18 8月 3 2017 .bash_logout-rw-r--r--. 1 zhang zhang 193 8月 3 2017 .bash_profile-rw-r--r--. 1 zhang zhang 231 8月 3 2017 .bashrcdrwx------. 15 zhang zhang 4096 7月 23 13:09 .cachedrwxr-xr-x. 16 zhang zhang 4096 7月 23 14:26 .config-rw-------. 1 zhang zhang 16 7月 23 2018 .esd_auth-rw-------. 1 zhang zhang 314 7月 23 2018 .ICEauthoritydrwx------. 3 zhang zhang 19 7月 23 2018 .localdrwxr-xr-x. 5 zhang zhang 54 7月 23 13:09 .mozilladrwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面[zhang@bogon ~]$ ls -la总用量 36drwx------. 14 zhang zhang 4096 7月 23 2018 .drwxr-xr-x. 3 root root 19 7月 23 2018 ..drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面[zhang@bogon ~]$ ls -lr总用量 0drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共[zhang@bogon ~]$ ls -lR.:总用量 0drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面./公共:总用量 0[zhang@bogon ~]$ ls -l总用量 0drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面[zhang@bogon ~]$ ls -lh总用量 0drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面[zhang@bogon ~]$ cd /usr[zhang@bogon usr]$ lsbin etc games include lib lib64 libexec local sbin share src tmp[zhang@bogon usr]$ ls -lh总用量 252Kdr-xr-xr-x. 2 root root 44K 7月 23 14:42 bindrwxr-xr-x. 2 root root 6 11月 5 2016 etcdrwxr-xr-x. 2 root root 6 11月 5 2016 gamesdrwxr-xr-x. 35 root root 4.0K 7月 23 14:42 includedr-xr-xr-x. 44 root root 4.0K 7月 23 14:42 libdr-xr-xr-x. 138 root root 72K 7月 23 14:42 lib64drwxr-xr-x. 43 root root 12K 7月 23 14:42 libexecdrwxr-xr-x. 12 root root 131 7月 23 2018 localdr-xr-xr-x. 2 root root 20K 7月 23 14:42 sbindrwxr-xr-x. 226 root root 8.0K 7月 23 2018 sharedrwxr-xr-x. 4 root root 34 7月 23 2018 srclrwxrwxrwx. 1 root root 10 7月 23 2018 tmp -&gt; ../var/tmp[zhang@bogon usr]$ ls -l总用量 252dr-xr-xr-x. 2 root root 45056 7月 23 14:42 bindrwxr-xr-x. 2 root root 6 11月 5 2016 etcdrwxr-xr-x. 2 root root 6 11月 5 2016 gamesdrwxr-xr-x. 35 root root 4096 7月 23 14:42 includedr-xr-xr-x. 44 root root 4096 7月 23 14:42 libdr-xr-xr-x. 138 root root 73728 7月 23 14:42 lib64drwxr-xr-x. 43 root root 12288 7月 23 14:42 libexecdrwxr-xr-x. 12 root root 131 7月 23 2018 localdr-xr-xr-x. 2 root root 20480 7月 23 14:42 sbindrwxr-xr-x. 226 root root 8192 7月 23 2018 sharedrwxr-xr-x. 4 root root 34 7月 23 2018 srclrwxrwxrwx. 1 root root 10 7月 23 2018 tmp -&gt; ../var/tmp[zhang@bogon usr]$ cd ~[zhang@bogon ~]$ ls -t公共 模板 视频 图片 下载 音乐 桌面 文档[zhang@bogon ~]$ ls公共 模板 视频 图片 文档 下载 音乐 桌面[zhang@bogon ~]$ ls -lt总用量 0drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档[zhang@bogon ~]$ ll总用量 0drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面[zhang@bogon ~]$ ls -l总用量 0drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面[zhang@bogon ~]$ ll总用量 0drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 公共drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 模板drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 视频drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 图片drwxr-xr-x. 3 zhang zhang 34 7月 23 14:26 文档drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 下载drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 音乐drwxr-xr-x. 2 zhang zhang 6 7月 23 2018 桌面[zhang@bogon ~]$ labash: la: 未找到命令...[zhang@bogon ~]$ pwd/home/zhang[zhang@bogon ~]$ cd /etc/init.d[zhang@bogon init.d]$ pwd/etc/init.d[zhang@bogon init.d]$ pwd -P/etc/rc.d/init.d[zhang@bogon init.d]$ mkdirmake directory 作用：创建新目录 语法：mkdir [-p] 目录名- p 父目录不存在则先创建父目录 1234567891011121314151617[zhang@bogon ~]$ ls公共 模板 视频 图片 文档 下载 音乐 桌面[zhang@bogon ~]$ mkdir test/zhangsan/hahahahamkdir: 无法创建目录"test/zhangsan/hahahaha": 没有那个文件或目录[zhang@bogon ~]$ mkdir -p test/zhangsan/hahahaha[zhang@bogon ~]$ lstest 公共 模板 视频 图片 文档 下载 音乐 桌面[zhang@bogon ~]$ mkdir -p test1 test2 test3[zhang@bogon ~]$ lstest test1 test2 test3 公共 模板 视频 图片 文档 下载 音乐 桌面[zhang@bogon ~]$ mkdir -p test4/&#123;ha,ha2,ha3&#125;[zhang@bogon ~]$ lstest test1 test2 test3 test4 公共 模板 视频 图片 文档 下载 音乐 桌面[zhang@bogon ~]$ cd test4[zhang@bogon test4]$ lsha ha2 ha3[zhang@bogon test4]$ mkdir -p /ha/haha touch: 作用: 创建空文件或者更新文件的时间 语法: touch 文件名 cpcopy 作用: 复制文件或目录 语法：cp [-rp] 源文件或目录 目的地目录- r: 递归 - p: 保留文件的属性 1[root@bogon etc]# cp -r /etc/* ./etc/ mvmove 作用：移动文件或目录，更改文件或目录的名字 语法: mv 源文件或目录 目的地目录12[root@bogon mv]# mv /home/zhang/etc .[root@bogon mv]# mv etc xiaozhou rmremove 作用: 删除文件 语法：rm [-rf] 文件或目录- r: 递归 - f: 强制删除文件或目录，即使原文件的属性是只读模式，也无需确认 123456# 只能用于删除文件，但是有提示[root@bogon zhang]# rm pinforc # 递归但是有提示[root@bogon zhang]# rm -r kernel##常用下面的方式[root@bogon zhang]# rm -rf kernel cat : 作用: 显示文件的内容 语法：cat [-n] [文件名]- A 显示所有的文件内容，包含隐藏字符 - n 显示行号123[root@bogon mv]# cat hahha/services[root@bogon mv]# cat -A hahha/services[root@bogon mv]# cat -n hahha/services more: 作用: 分页显示文件内容 语法：more [文件名] 空格或者f：显示下一页 回车：显示下一行 q或Q 退出 head: - 作用: 显示文件的前几行（默认10行） - 语法：head [-n] [文件名] 12[root@bogon mv]# head hahha/services[root@bogon mv]# head -20 hahha/servicestail: 作用: 显示文件的后几行（动态显示） 语法: tail [-nf] [文件名] - n: 指定显示的行数 - f: 动态显示文件的内容（follow） 123[root@bogon mv]# tail hahha/services[root@bogon mv]# tail -20 hahha/services [root@bogon mv]# tail -f hahha/services lnlink 作用: 产生链接文件 语法：ln -s [源文件] [目标文件] 创建的是软连接 ln [源文件] [目标文件] 创建的是硬链接 1234#创建软链接：类似于快捷方式，但是不占用很大的空间，修改或编辑的仍然是源文件[root@bogon mv]# ln -s hahha/services ./service.soft#创建硬链接：类似于拷贝，但是不一样。会占用源文件的空间大小，修改或编辑仍然是源文件[root@bogon mv]# ln hahha/services ./service.hard manmanual 作用: 获取命令或配置文件的帮助信息 语法: man [命令/配置文件]说明: 调用more命令来查看帮助文档 help: 作用: 查看shell内置命令的帮助信息 语法: help 命令1234[root@bogon mv]# help cd[root@bogon mv]# type cd[root@bogon mv]# type ls[root@bogon mv]# ls --help find: 作用: 查找文件或目录 语法: find [搜索路径] [匹配条件] - -name 按照名称查找 - -iname 按照名称查找 - *: 匹配所有 - ?: 匹配单个字符 - -size 按照文件大小查找 以block为单位，一个block是512B，1K=2block，+表示大于，-表示小于，不写表示等于 - -type f 二进制文件，l表示软连接文件，d表示目录，c表示字符文件 规则: 查询范围要缩小，查询条件要精准 12345678910111213141516171819202122232425262728293031323334353637383940414243 [root@bogon mv]# find /etc -name "init"/etc/sysconfig/init/etc/selinux/targeted/active/modules/100/init/etc/selinux/targeted/tmp/modules/100/init[root@bogon mv]# find /etc -iname "init"/etc/sysconfig/init/etc/selinux/targeted/active/modules/100/init/etc/selinux/targeted/tmp/modules/100/init/etc/gdm/Init[root@bogon mv]# find /etc -name "init*"/etc/sysconfig/network-scripts/init.ipv6-global/etc/sysconfig/init/etc/init.d/etc/rc.d/init.d/etc/selinux/targeted/active/modules/100/init/etc/selinux/targeted/contexts/initrc_context/etc/selinux/targeted/tmp/modules/100/init/etc/iscsi/initiatorname.iscsi/etc/inittab[root@bogon mv]# find /etc -name "init???"/etc/inittab[root@bogon mv]# find /etc -name init???/etc/inittab[root@bogon mv]# find /etc -name init*/etc/sysconfig/network-scripts/init.ipv6-global/etc/sysconfig/init/etc/init.d/etc/rc.d/init.d/etc/selinux/targeted/active/modules/100/init/etc/selinux/targeted/contexts/initrc_context/etc/selinux/targeted/tmp/modules/100/init/etc/iscsi/initiatorname.iscsi/etc/inittab[root@bogon mv]# find /etc -name *init/etc/security/namespace.init/etc/gdbinit/etc/X11/xinit/etc/sysconfig/init/etc/selinux/targeted/active/modules/100/init/etc/selinux/targeted/tmp/modules/100/init[root@bogon mv]# find /etc -size 2048[root@bogon mv]# find /etc -size -2048[root@bogon mv]# find /etc -size +2048 grep: 作用: 在文件中搜寻匹配的字符所在的行并输出 语法: grep [-cinv] ‘要搜索的字符串’ 文件名 - -c: 表示要输出的匹配的行的次数（输出多少行） - -i: 忽略大小写 - -n: 显示匹配的行及行号 - -v: 反向选择，显示不包含匹配的文本的行 12345 [root@bogon mv]# grep udp /etc/services [root@bogon mv]# grep -v ^# /etc/inittab [root@bogon mv]# grep -c udp /etc/services 5389[root@bogon mv]# grep -n udp /etc/services which: 作用: 显示系统命令所在的目录 123456789 [root@bogon mv]# which lsalias ls='ls --color=auto' /usr/bin/ls[root@bogon mv]# which cd/usr/bin/cd[root@bogon mv]# which cc/usr/bin/cc[root@bogon mv]# which cf/usr/bin/which: no cf in (/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin:/sbin:/home/zhang/.local/bin:/home/zhang/bin) whereis: 作用: 搜索命令所在的目录，配置文件所在的目录，以及帮助文档的路径 12345[root@bogon mv]# whereis passwdpasswd: /usr/bin/passwd /etc/passwd /usr/share/man/man1/passwd.1.gz /usr/share/man/man5/passwd.5.gz[root@bogon mv]# which passwd/usr/bin/passwd[root@bogon mv]# man 5 passwd]]></content>
      <categories>
        <category>Linux学习</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK安装文档]]></title>
    <url>%2F2019%2F04%2F25%2F1_ELK%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3__%2F</url>
    <content type="text"><![CDATA[ELK安装文档1. 安装准备 1.1 操作环境操作系统环境：Centos7 1708版本 JDK版本：jdk-8u144-linux-x64.tar.gz Elasticsearch版本：5.6.3 Logstash版本：5.6.3 Kibana版本：5.6.32. 安装上传文件和下载文件的插件（可以不安装） 鉴于Elasticsearch用户只能是非root用户，因此后面的所有操作都在es账户下执行 1[root@master ~]# yum -y install lrzsz 3. 安装sudo命令并配置 由于这里采用的是最小化安装，因此sudo命令不可用。 3.1 安装sudo插件1[root@master ~]# yum -y install sudo 3.2 修改/etc/sudoers文件的权限1[root@master ~]# chmod u+w /etc/sudoers 123456789101112package com.zhiyou100.abstrac;public class Test &#123; final static int width = 100; public static void main(String[] args) &#123; USB.read(); &#125; public static void eat() &#123; &#125; &#125; 3.3 开始编辑/etc/sudoers文件1[root@master ~]# vi /etc/sudoers 3.4 找到root ALL=(ALL) ALL行，在下面添加如下内容1es ALL=(ALL) NOPASSWD: ALL 3.5 去掉#%wheel ALL=(ALL) NOPASSWD: ALL的#号3.6 保存并退出3.7将es用户调整至wheel用户组中，这样以后使用sudo命令的时候就不用输入密码了。1[root@master ~]# gpasswd -a es wheel 3.8 恢复/etc/sudoers文件的权限1[root@master ~]# chmod u-w /etc/sudoers 然后你就可以肆无忌惮的使用es用户了 #4. 切换至es用户1[root@slave3 ~]# su es #5. 创建相应目录1234[es@master ~]$ sudo mkdir -p /opt/SoftWare/Java[es@master ~]$ sudo mkdir -p /opt/SoftWare/ES[es@master ~]$ sudo mkdir -p /opt/SoftWare/Kibana[es@master ~]$ sudo mkdir -p /opt/SoftWare/Logstash #6. 关闭防火墙12[es@master Java]$ sudo systemctl stop firewalld[es@master Java]$ sudo systemctl disable firewalld.service #7. 安装jdk环境7.1 上传tar包1[es@master Java]$ sudo rz 7.1 卸载JDK7.2 解压jdk1[es@master Java]$ sudo tar -zxvf jdk-8u144-linux-x64.tar.gz 7.3 配置环境变量1[es@master Java]$ sudo vi /etc/profile 添加一下内容： 12345#JDK1.8export JAVA_HOME=/opt/SoftWare/Java/jdk1.8.0_144export JRE_HOME=/opt/SoftWare/Java/jdk1.8.0_144/jreexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin 使修改生效 1[es@master Java]$ source /etc/profile 7.4 将master节点上的jdk远程拷贝到其他节点上123[es@master Java]$ sudo scp -r jdk1.8.0_144 root@192.168.100.101:/opt/SoftWare/Java/[es@master Java]$ sudo scp -r jdk1.8.0_144 root@192.168.100.102:/opt/SoftWare/Java/[es@master Java]$ sudo scp -r jdk1.8.0_144 root@192.168.100.103:/opt/SoftWare/Java/ #8. Elasticsearch的安装Elasticsearch的安装比较简单，但是有一些细节要注意，在这里咱们采用tar包进行安装 8.1 将Elasticsearch上传至VM可以在命令行中输入rz调出上传窗口即可：如[root@master ES]# rz。 8.2 解压Elasticsearch压缩包1[root@master ES]# tar zxvf elasticsearch-5.6.3.tar.gz 8.3 创建相关目录并赋权限给es1234567[root@master ~]# mkdir -p /var/elasticsearch/log[root@master ~]# mkdir -p /var/elasticsearch/data[root@master ~]# mkdir -p /var/elasticsearch/plugins[root@master ~]# chown -R es:es /opt/SoftWare/ES/elasticsearch-5.6.3[root@master ~]# chown -R es:es /var/elasticsearch/log[root@master ~]# chown -R es:es /var/elasticsearch/data[root@master ~]# chown -R es:es /var/elasticsearch/plugins 8.4 配置核心文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200################################### Cluster ################################### # 代表一个集群,集群中有多个节点,其中有一个为主节点,这个主节点是可以通过选举产生的,主从节点是对于集群内部来说的. # es的一个概念就是去中心化,字面上理解就是无中心节点,这是对于集群外部来说的,因为从外部来看es集群,在逻辑上是个整体,你与任何一个节点的通信和与整个es集群通信是等价的。 # cluster.name可以确定集群名称,当elasticsearch集群在同一个网段中,elasticsearch会自动的找到具有相同cluster.name的elasticsearch服务,这个时候需要开启多播服务才行（不建议）# 所以当同一个网段具有多个elasticsearch集群时cluster.name就成为同一个集群的标识. #此为必须修改项cluster.name: zhiyou #################################### Node ##################################### # 节点名称同理,可自动生成也可手动配置. （建议手动配置）node.name: node0# 允许此节点是否可以成为一个master节点,es是默认集群中的第一台机器为master,如果这台机器停止就会重新选举master. 在新版本中默认没有此参数，但是可以直接写上，不写默认为truenode.master: true # 允许该节点存储数据(默认开启，同上) node.data: true # 配置文件中给出了三种配置高性能集群拓扑结构的模式,如下： # 1. 如果你想让节点从不选举为主节点,只用来存储数据,可作为负载器 # node.master: false # node.data: true # node.ingest: true #默认true# 2. 如果想让节点成为主节点,且不存储任何数据,并保有空闲资源,可作为协调器 # node.master: true # node.data: false# node.ingest: true# 3. 如果想让节点既不称为主节点,又不成为数据节点,那么可将他作为搜索器,从节点中获取数据,生成搜索结果等 # node.master: false # node.data: false # node.ingest: true## 4. 仅作为协调器 # node.master: false # node.data: false# node.ingest: false# 监控集群状态有一下插件和API可以使用: # 每个节点都可以定义一些与之关联的通用属性，用于后期集群进行碎片分配时的过滤# node.rack: rack314 # 设置一台服务器能运行的节点数,一般为1就好,因为一般情况下一台机器只跑一个节点node.max_local_storage_nodes: 1 #################################### Index #################################### # 设置索引的分片数,默认为5 index.number_of_shards: 5 # 设置索引的副本数,默认为1: index.number_of_replicas: 2 # 配置文件中提到的最佳实践是,如果服务器够多,可以将分片提高,尽量将数据平均分布到大集群中去# 同时,如果增加副本数量可以有效的提高搜索性能 # 需要注意的是,"number_of_shards" 是索引创建后一次生成的,后续不可更改设置 # "number_of_replicas" 是可以通过API去实时修改设置的 #################################### Paths #################################### # 配置文件存储位置 # path.conf: /path/to/conf # 数据存储位置(单个目录设置) path.data: /var/elasticsearch/data # 多个数据存储位置,有利于性能提升 # path.data: /path/to/data1,/path/to/data2 # 临时文件的路径 # path.work: /path/to/work # 日志文件的路径 path.logs: /var/elasticsearch/log # 插件安装路径 # path.plugins: /var/elasticsearch/plugins ################################### Memory #################################### # 当JVM开始写入交换空间时（swapping）ElasticSearch性能会低下,你应该保证它不会写入交换空间 # 设置这个属性为true来锁定内存,同时也要允许elasticsearch的进程可以锁住内存,linux下可以通过 `ulimit -l unlimited` 命令 bootstrap.mlockall: true # 确保 ES_MIN_MEM 和 ES_MAX_MEM 环境变量设置为相同的值,以及机器有足够的内存分配给Elasticsearch # 注意:内存也不是越大越好,一般64位机器,最大分配内存别才超过32G ############################## Network And HTTP ############################### # 设置绑定的ip地址,可以是ipv4或ipv6的,默认为0.0.0.0 network.bind_host: 192.168.100.100 #只有本机可以访问http接口# 设置其它节点和该节点交互的ip地址,如果不设置它会自动设置,值必须是个真实的ip地址 network.publish_host: 192.168.100.100 # 同时设置bind_host和publish_host上面两个参数 network.host: 192.168.100.100 #绑定监听IP# 设置节点间交互的tcp端口,默认是9300 transport.tcp.port: 9300 # 设置是否压缩tcp传输时的数据，默认为false,不压缩transport.tcp.compress: true # 设置对外服务的http端口,默认为9200 http.port: 9200 # 设置请求内容的最大容量,默认100mb # http.max_content_length: 100mb # 使用http协议对外提供服务,默认为true,开启 # http.enabled: false ###################### 使用head等插件监控集群信息，需要打开以下配置项 ###########http.cors.enabled: truehttp.cors.allow-origin: "*"http.cors.allow-credentials: true################################### Gateway ################################### # gateway的类型,默认为local即为本地文件系统,可以设置为本地文件系统 # gateway.type: local # 下面的配置控制怎样以及何时启动一整个集群重启的初始化恢复过程 # (当使用shard gateway时,是为了尽可能的重用local data(本地数据)) # 一个集群中的N个节点启动后,才允许进行恢复处理 # gateway.recover_after_nodes: 1 # 设置初始化恢复过程的超时时间,超时时间从上一个配置中配置的N个节点启动后算起 # gateway.recover_after_time: 5m # 设置这个集群中期望有多少个节点.一旦这N个节点启动(并且recover_after_nodes也符合), # 立即开始恢复过程(不等待recover_after_time超时) # gateway.expected_nodes: 2 ############################# Recovery Throttling ############################# # 下面这些配置允许在初始化恢复,副本分配,再平衡,或者添加和删除节点时控制节点间的分片分配 # 设置一个节点的并行恢复数 # 1.初始化数据恢复时,并发恢复线程的个数,默认为4 # cluster.routing.allocation.node_initial_primaries_recoveries: 4 # 2.添加删除节点或负载均衡时并发恢复线程的个数,默认为2 # cluster.routing.allocation.node_concurrent_recoveries: 2 # 设置恢复时的吞吐量(例如:100mb,默认为0无限制.如果机器还有其他业务在跑的话还是限制一下的好) # indices.recovery.max_bytes_per_sec: 20mb # 设置来限制从其它分片恢复数据时最大同时打开并发流的个数,默认为5 # indices.recovery.concurrent_streams: 5 # 注意: 合理的设置以上参数能有效的提高集群节点的数据恢复以及初始化速度 ################################## Discovery ################################## # 设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点.默认为1,对于大的集群来说,可以设置大一点的值(2-4) # discovery.zen.minimum_master_nodes: 1 # 探查的超时时间,默认3秒,提高一点以应对网络不好的时候,防止脑裂 # discovery.zen.ping.timeout: 3s # 设置是否打开多播发现节点.默认是true. (尽可能使用单播模式)# 当多播不可用或者集群跨网段的时候集群通信还是用单播吧 # discovery.zen.ping.multicast.enabled: false # 这是一个集群中的主节点的初始列表,当节点(主节点或者数据节点)启动时使用这个列表进行探测 discovery.zen.ping.unicast.hosts: ["192.168.100.100:9300", "192.168.100.101:9300", "192.168.100.102:9300", "192.168.100.103:9300"] # Slow Log部分与GC log部分略,不过可以通过相关日志优化搜索查询速度 ################ X-Pack ############################################ 官方插件 相关设置请查看此处############## Memory(重点需要调优的部分) ################ # Cache部分: # es有很多种方式来缓存其内部与索引有关的数据.其中包括filter cache # filter cache部分: # filter cache是用来缓存filters的结果的.默认的cache type是node type.node type的机制是所有的索引内部的分片共享filter cache.node type采用的方式是LRU方式.即:当缓存达到了某个临界值之后，es会将最近没有使用的数据清除出filter cache.使让新的数据进入es. # 这个临界值的设置方法如下：indices.cache.filter.size 值类型：eg.:512mb 20%。默认的值是10%。 # out of memory错误避免过于频繁的查询时集群假死 # 1.设置es的缓存类型为Soft Reference,它的主要特点是据有较强的引用功能.只有当内存不够的时候,才进行回收这类内存,因此在内存足够的时候,它们通常不被回收.另外,这些引用对象还能保证在Java抛出OutOfMemory异常之前,被设置为null.它可以用于实现一些常用图片的缓存,实现Cache的功能,保证最大限度的使用内存而不引起OutOfMemory.在es的配置文件加上index.cache.field.type: soft即可. # 2.设置es最大缓存数据条数和缓存失效时间,通过设置index.cache.field.max_size: 50000来把缓存field的最大值设置为50000,设置index.cache.field.expire: 10m把过期时间设置成10分钟. # index.cache.field.max_size: 50000 # index.cache.field.expire: 10m # index.cache.field.type: soft # field data部分&amp;&amp;circuit breaker部分： # 用于fielddata缓存的内存数量,主要用于当使用排序,faceting操作时,elasticsearch会将一些热点数据加载到内存中来提供给客户端访问,但是这种缓存是比较珍贵的,所以对它进行合理的设置. # 可以使用值：eg:50mb 或者 30％(节点 node heap内存量),默认是：unbounded #indices.fielddata.cache.size： unbounded # field的超时时间.默认是-1,可以设置的值类型: 5m #indices.fielddata.cache.expire: -1 # circuit breaker部分: # 断路器是elasticsearch为了防止内存溢出的一种操作,每一种circuit breaker都可以指定一个内存界限触发此操作,这种circuit breaker的设定有一个最高级别的设定:indices.breaker.total.limit 默认值是JVM heap的70%.当内存达到这个数量的时候会触发内存回收# 另外还有两组子设置： #indices.breaker.fielddata.limit:当系统发现fielddata的数量达到一定数量时会触发内存回收.默认值是JVM heap的70% #indices.breaker.fielddata.overhead:在系统要加载fielddata时会进行预先估计,当系统发现要加载进内存的值超过limit * overhead时会进行进行内存回收.默认是1.03 #indices.breaker.request.limit:这种断路器是elasticsearch为了防止OOM(内存溢出),在每次请求数据时设定了一个固定的内存数量.默认值是40% #indices.breaker.request.overhead:同上,也是elasticsearch在发送请求时设定的一个预估系数,用来防止内存溢出.默认值是1 # Translog部分: # 每一个分片(shard)都有一个transaction log或者是与它有关的预写日志,(write log),在es进行索引(index)或者删除(delete)操作时会将没有提交的数据记录在translog之中,当进行flush 操作的时候会将tranlog中的数据发送给Lucene进行相关的操作.一次flush操作的发生基于如下的几个配置 #index.translog.flush_threshold_ops:当发生多少次操作时进行一次flush.默认是 unlimited #index.translog.flush_threshold_size:当translog的大小达到此值时会进行一次flush操作.默认是512mb #index.translog.flush_threshold_period:在指定的时间间隔内如果没有进行flush操作,会进行一次强制flush操作.默认是30m #index.translog.interval:多少时间间隔内会检查一次translog,来进行一次flush操作.es会随机的在这个值到这个值的2倍大小之间进行一次操作,默认是5s #index.gateway.local.sync:多少时间进行一次的写磁盘操作,默认是5s 一般情况下，配置以下内容即可：编辑elasticsearch.yml文件 123456789101112131415161718192021222324252627282930[root@master config]# vi elasticsearch.yml# 添加一下内容：# 集群名称cluster.name: zhiyou# 节点名称node.name: node0# 数据文件的路径path.data: /var/elasticsearch/data# 日志文件的路径path.logs: /var/elasticsearch/log# 锁定内存bootstrap.memory_lock: false# 本节点的IP即本机的IPnetwork.host: 192.168.100.100# 访问端口号，API交互的端口http.port: 9200# 设置自动创建索引action.auto_create_index: true# 设置transport交互的IP和端口号transport.host: 192.168.100.100transport.tcp.port: 9300transport.tcp.compress: true# 单播模式发现其他节点，注意为9300端口discovery.zen.ping.unicast.hosts: ["192.168.100.100:9300", "192.168.100.101:9300", "192.168.100.102:9300", "192.168.100.103:9300"]#discovery.zen.minimum_master_nodes: 3# 设置下面的参数，这样head插件才能使用http.cors.enabled: truehttp.cors.allow-origin: "*"http.cors.allow-credentials: true 8.5 配置JVM1234[root@master config]# vi jvm.options# 修改为以下设置：因为我的虚拟机给的内存为2G-Xms1g-Xmx1g 8.6 配置数据节点很明显，前面那一台节点咱们设置的是不存储数据的，一般来说咱们都会固定master节点，如果条件允许，可以多设置几台都行。在这里，我只配置了一台master节点，其余的三个节点设置为数据节点在master节点上使用远程拷贝将elasticsearch主目录拷贝到其他节点上 123[root@master ES]# scp -r elasticsearch-5.6.3 root@192.168.100.101:/opt/SoftWare/ES/[root@master ES]# scp -r elasticsearch-5.6.3 root@192.168.100.102:/opt/SoftWare/ES/[root@master ES]# scp -r elasticsearch-5.6.3 root@192.168.100.103:/opt/SoftWare/ES/ 在数据节点上修改相关配置,其余的可以不修改了 123456# 节点名称node.name: node1# 本节点的IP即本机的IPnetwork.host: 192.168.100.101# 设置transport交互的IP和端口号transport.host: 192.168.100.101 8.7 给数据节点创建相关目录并赋权限给es12345[root@master ~]# mkdir -p /var/elasticsearch/log[root@master ~]# mkdir -p /var/elasticsearch/data[root@master ~]# chown -R es:es /opt/SoftWare/ES/elasticsearch-5.6.3[root@master ~]# chown -R es:es /var/elasticsearch/log[root@master ~]# chown -R es:es /var/elasticsearch/data 8.8 修改系统配置文件（每台节点均需要配置并重启生效）1234567891011#编辑文件1：/etc/security/limits.conf[root@slave3 ~]# vi /etc/security/limits.conf 添加以下内容* hard nofile 131072* soft nofile 65536* hard nproc 4096* soft nproc 2048es soft memlock unlimitedes hard memlock unlimited#编辑文件1：/etc/sysctl.conf 添加以下内容[root@slave3 ~]# vi /etc/sysctl.confvm.max_map_count=262144 8.9 可能的错误集锦：8.9.1 can not run elasticsearch as root12345#错误详情：[2017-11-01T10:32:44,124][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [node2] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root#解决方案：不要使用root用户启动，使用非root用户启动方可解决问题 8.9.2 Unable to lock JVM Memory: error=12, reason=无法分配内存12345678910111213#错误详情：[2017-11-01T10:32:04,993][WARN ][o.e.b.JNANatives ] Unable to lock JVM Memory: error=12, reason=无法分配内存[2017-11-01T10:32:04,998][WARN ][o.e.b.JNANatives ] This can result in part of the JVM being swapped out.[2017-11-01T10:32:04,998][WARN ][o.e.b.JNANatives ] Increase RLIMIT_MEMLOCK, soft limit: 65536, hard limit: 65536[2017-11-01T10:32:04,998][WARN ][o.e.b.JNANatives ] These can be adjusted by modifying /etc/security/limits.conf, for example: # allow user 'es' mlockall es soft memlock unlimited es hard memlock unlimited#解决方案：编辑文件/etc/security/limits.conf，添加一下内容（重启生效）[root@slave3 ~]# vi /etc/security/limits.confes soft memlock unlimitedes hard memlock unlimited 8.9.3 ERROR: [1] bootstrap checks failed1234567891011#错误详情：[2017-11-01T11:18:54,289][INFO ][o.e.b.BootstrapChecks ] [node3] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checksERROR: [1] bootstrap checks failed[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]#解决方案：编辑文件/etc/security/limits.conf，添加以下内容（重启生效）[root@slave3 ~]# vi /etc/security/limits.conf* hard nofile 131072* soft nofile 65536* hard nproc 4096* soft nproc 2048 8.9.4 ERROR: [1] bootstrap checks failed12345678#错误详情：[2017-11-01T11:18:54,289][INFO ][o.e.b.BootstrapChecks ] [node3] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checksERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]#解决方案：编辑文件/etc/sysctl.conf，添加以下内容（重启生效）[root@slave3 ~]# vi /etc/sysctl.confvm.max_map_count=262144 8.9.5 access denied (“javax.management.MBeanTrustPermission” “register”)12345678#错误详情：2017-11-01 10:46:27,754 main ERROR Could not register mbeans java.security.AccessControlException: access denied ("javax.management.MBeanTrustPermission" "register") at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) at java.lang.SecurityManager.checkPermission(SecurityManager.java:585) at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.checkMBeanTrustPermission(DefaultMBeanServerInterceptor.java:1848)#解决方案：给elasticsearch-5.6.3的主目录分配权限[es@master elasticsearch-5.6.3]$ sudo chown -R es:es elasticsearch-5.6.3 8.10 测试集群环境是否搭建成功12345678910111213141516171819202122#每台节点都进入到es主目录，使用es用户[es@master elasticsearch-5.6.3]# bin/elasticsearch#在浏览器的地址栏中输入http://192.168.100.103:9200/#出现以下内容，说明当前es节点已启动&#123; "name" : "node3", "cluster_name" : "zhiyou", "cluster_uuid" : "7kO5OPi8RoWUuXfEBYPITw", "version" : &#123; "number" : "5.6.3", "build_hash" : "1a2f265", "build_date" : "2017-10-06T20:33:39.012Z", "build_snapshot" : false, "lucene_version" : "6.6.1" &#125;, "tagline" : "You Know, for Search"&#125;#在浏览器中输入http://192.168.100.100:9200/_cluster/health#出现以下内容，说明es集群已搭建成功（看status和number_of_nodes是否准确）&#123;"cluster_name":"zhiyou","status":"green","timed_out":false,"number_of_nodes":4,"number_of_data_nodes":4,"active_primary_shards":0,"active_shards":0,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":0,"delayed_unassigned_shards":0,"number_of_pending_tasks":0,"number_of_in_flight_fetch":0,"task_max_waiting_in_queue_millis":0,"active_shards_percent_as_number":100.0&#125; #9. 安装Head插件9.1 下载点击下载 9.2 解压由于最小化安装，故没有解压zip文件的命令，首先安装unzip解压软件，然后再进行解压 12[es@master plugins]$ sudo yum install zip unzip[es@master plugins]$ unzip elasticsearch-head-master.zip 9.3 安装HTTP由于这个插件就是HTML5写的，就是一个前端的一个东西，只需要set到HTTP服务中即可 12#安装HTTP[es@master plugins]$ sudo yum install httpd -y 9.4 复制head到HTTP的指定目录12[es@master ~]$ cd /var/www/html[es@master html]$ sudo cp -r /opt/SoftWare/ES/plugins/head /var/www/html/ 9.5 启动ES集群和HTTPD服务12[es@master ~]# sudo systemctl restart httpd 访问IP地址如下说明已经启动，可以设置开机自启![image_1btt80jm0qspmmrk121ksqp559.png-555.7kB][1] 123# 启动ES集群[es@master ~]# cd /opt/SoftWare/ES/elasticsearch-5.6.3[es@master elasticsearch-5.6.3]# bin/elasticsearch 出现下面的类似界面![image_1btt9a90v8qaqfo1a5i130htnm.png-111.2kB][2] 10. 安装Kibana 10.1 下载单击下载 10.2 解压123[root@master Kibana]# tar kibana-5.6.3-linux-x86_64.tar.gz# 名字太长了也不好，就mv一下了[root@master Kibana]# mv kibana-5.6.3-linux-x86_64 kibana-5.6.3 10.3 配置1234567891011[root@master Kibana]# cd kibana-5.6.3#修改一下内容即可# Kibana服务端口server.port: 5601# kibana访问的IPserver.host: "192.168.100.100"#elasticsearch的master地址elasticsearch.url: "http://192.168.100.100:9200"#kibana默认的元数据索引kibana.index: ".kibana" 10.4 启动Kibana1[root@master kibana-5.6.3]# bin/kibana 出现下面的界面，可能会因为数据的问题有点不一样![image_1btta457a3d01e371c3fn3c1dav13.png-45.8kB][3]![image_1btta50r5vikqra135r1dv7lfp20.png-199.8kB][4]第一次启动会提示配置一个索引，其实就是默认的一个搜索位置 11. 安装Logstash 11.1 下载Logstash点击下载 11.2 解压缩1[root@master Logstash]# tar -zxvf logstash-5.6.3.tar.gz 11.3 运行Logstash12# 在这里可以使用标准输入线测试一下logstash[root@master logstash-5.6.3]# bin/logstash -e 'input &#123; stdin&#123;&#125; &#125; output &#123;stdout&#123;&#125; &#125;' 出现下面的内容说明启动成功![image_1bttcndg518kq1enu8tg3v14f32d.png-16.1kB][5]然后练习以下输入输出 11.3.1 codec1[root@master logstash-5.6.3]# bin/logstash -e 'input &#123; stdin&#123;&#125; &#125; output &#123;stdout&#123;codec =&gt; rubydebug&#125; &#125;' ![image_1bttcvo60152g36i5oa19ei1u7v2q.png-45.7kB][6] 11.3.2 输出es集群中注意：老版本和新版本不同，注意看官方文档 1[root@master logstash-5.6.3]# bin/logstash -e 'input &#123; stdin&#123;&#125; &#125; output &#123;elasticsearch &#123; hosts =&gt; ["192.168.100.100:9200"] &#125; stdout&#123;&#125; &#125;' 12. 使用ELK进行整合操作12.1 编写数据读取的配置文件1[root@master logstash-5.6.3]# vi /var/logstash/config/std.conf 添加如下内容 123456789101112input &#123; stdin &#123;&#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["192.168.100.100:9200"] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 应用 1[root@master logstash-5.6.3]# bin/logstash -f /var/logstash/config/std.conf 12.2 file输入详解1234567891011121314151617181920#监听文件的路径path =&gt; ["/var/logstash/data/*","/var/logstash/test.log"]#排除不想监听的文件exclude =&gt; "1.log"#添加自定义的字段add_field =&gt; &#123;"test"=&gt;"test"&#125;#增加标签tags =&gt; "tag1"#设置新事件的标志delimiter =&gt; "\n"#设置多长时间扫描目录，发现新文件discover_interval =&gt; 15#设置多长时间检测文件是否修改stat_interval =&gt; 1#监听文件的起始位置，默认是endstart_position =&gt; beginning#监听文件读取信息记录的位置sincedb_path =&gt; "/var/logstash/test.log"#设置多长时间会写入读取的位置信息sincedb_write_interval =&gt; 15]]></content>
      <categories>
        <category>ELK日志分析平台</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据-数据仓库]]></title>
    <url>%2F2019%2F04%2F25%2F1_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[数据仓库1. 什么是数据仓库？数据仓库（Data Warehouse）,可简写为DW或DWH,数据仓库，是为了企业所有级别的决策制定计划过程，提供所有类型数据类型的战略集合。它出于分析性报告和决策支持的目的而创建。为需要业务智能的企业 ,为需要指导业务流程改进、监视时间，成本，质量以及控制等.2. 数据仓库能干什么？（举几个栗子）: 年度销售目标的制定，需要根据以往的历史报表进行决策，不能随便制定。 优化业务流程例如：某电商平台某品牌的手机，在过去5年主要的的购买人群的年龄在什么年龄段，在那个季节购买量人多，这样就可以根据这个特点为目标人群设定他们主要的需求和动态分配产生的生产量，和仓库的库存。 3. 数据仓库的特点: 数据仓库是面向主题的。与传统的数据库不一样，数据仓库是面向主题的，那什么是主题呢？首页主题是一个较高乘次的概念，是较高层次上企业信息系统中的数据综合，归类并进行分析的对象。在逻辑意义上，他是对企业中某一个宏观分析领域所涉及的分析对象。（说人话：就是用户用数据仓库进行决策所关心的重点方面，一个主题通常与多个操作信息型系统有关，而操作型数据库的数据组织面向事务处理任务，各个任务之间是相互隔离的）； 数据仓库是集成的。数据仓库的数据是从原来的分散的数据库数据（mysql等关系型数据库）抽取出来的。操作型数据库与DSS（决策支持系统）分析型数据库差别甚大。第一，数据仓库的每一个主题所对应的源数据在所有的各个分散的数据库中，有许多重复和不一样的地方，且来源于不同的联机系统的数据都和不同的应用逻辑捆绑在一起；第二，数据仓库中的综合数据不能从原来有的数据库系统直接得到。因此子在数据进入数据仓库之前，必然要经过统一与综合，这一步是数据仓库建设中最关键，最复杂的一步，所要挖成的工作有： 要统计源数据中所有矛盾之处，如字段的同名异议、异名同义、单位不统一，字长不统一等。 进行数据的综合和计算。数据仓库中的数据综合工作可以在原有数据库抽取数据时生成，但许多是在数据仓库内部生成的，即进入数据仓库以后进行综合生成的。 数据仓库的数据是随着时间的变化而变化的。 数据仓库中的数据不可更新是针对应用来说的，也就是说，数据仓库的用户进行分析处理是不进行数据更新操作的。但并不是说，在从数据集成输入数据仓库开始到最后被删除的整个生存周期中，所有的数据仓库数据都是永远不变的。 数据仓库的数据是随着时间变化而变化的，这是数据仓库的特征之一。这一特征主要有以下三个表现： 数据仓库随着时间变化不断增加新的数据内容。数据仓库系统必须不断捕捉OLTP数据库中变化的数据，追加到数据仓库当中去，也就是要不断的生成OLTP数据库的快照，经统一集成增加到数据仓库中去；但对于确实不在变化的数据库快照，如果捕捉到新的变化数据，则只生成一个新的数据库快照增加进去，而不会对原有的数据库快照进行修改。 数据库随着时间变化不断删去旧的数据内容 。数据仓库内的数据也有存储期限，一旦过了这一期限，过期数据就要被删除。只是数据库内的数据时限要远远的长于操作型环境中的数据时限。在操作型环境中一般只保存有6090天的数据，而在数据仓库中则要需要保存较长时限的数据（例如：510年），以适应DSS进行趋势分析的要求。 数据仓库中包含有大量的综合数据，这些综合数据中很多跟时间有关，如数据经常按照时间段进行综合，或隔一定的时间片进行抽样等等。这些数据要随着时间的变化不断地进行从新综合。因此数据仓库的数据特征都包含时间项，以标明数据的历史时期。 数据仓库的数据是不可修改的。数据仓库的数据主要提供企业决策分析之用，所涉及的数据操作主要是数据查询，一般情况下并不进行修改操作。数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库快照的集合， 以及基于这些快照进行统计、综合和重组的导出数据，而不是联机处理的数据。数据库中进行联机处理的书库进过集成输入到数据仓库中，一旦数据仓库存放的数据已经超过数据仓库的数据存储期限，这些数据将从当前的数据仓库中删去。因为数据仓库只进行数据查询操作，所以数据仓库当中的系统要比数据库中的系统要简单的多。数据库管理系统中许多技术难点，如完整性保护、并发控制等等，在数据仓库的管理中几乎可以省去。但是由于数据仓库的查询数据量往往很大，所以就对数据查询提出了更高的要求，他要求采用各种复杂的索引技术；同时数据仓库面向的是商业企业的高层管理层，他们会对数据查询的界面友好性和数据表示提出更高的要求； 数据仓库和数据库的区别想了解区别之前，我们需要了解三个概念，数据库软件、数据库和数据仓库是什么？ 数据库软件是一种软件（并不是链接数据库的图形化客户端）。用来实现数据库逻辑过程，属于物理层。数据库是一种逻辑概念，用来存放数据的仓库，通过数据库软件来实现。数据库由很多表组成，表是二维的，一张表里面有很多字段。字段一字排开，对数据就一行一行的写入表中。数据库的表，在于能够用二维表现多维的关系。如：oracle、DB2、MySQL、Sybase、MSSQL Server等。数据仓库是数据库概念的升级。从逻辑上理解，数据库和数据仓库没有区别，都是通过数据库软件实现存放数据的地方，只不过从数据量来说，数据仓库要比数据库更庞大德多。数据仓库主要用于数据挖掘和数据分析，辅助领导做决策；在IT的架构体系中，数据库是必须存在的，必须要有地方存数据。比如现在的网购等电商。物品的存货多少，货品的价格，用户的账户余额之类的。这些数据都是存放在后台数据库中。或者最简单的理解，我们现在的微信、微博和QQ等账户和密码。在后台数据库必须是一个user表，字段起码有两个吗，即用户名和密码，然后我们的数据就一行一行的存在表上面。当我们登录的时候，我们填写了用户名和密码，这些数据就会回传到回台去，去跟表上面的数据匹配，匹配成功了，就能登录。匹配不成功就会报错，这就是数据库，数据库在生产环境就是用来干活的。凡是跟业务有关应用挂钩的，我们都使用数据库。数据仓库是BI下的其中一种技术。由于数据库跟业务应用挂钩的，所以一个数据库不可能装下一家公司的所有数据。数据库的表设计往往是针对某一个应用进行设计的。比如刚刚的登录功能，这张user表上就只有这两个字段，没有别的字段了。到那时这张表符合应该，没有问题，但是这张表不符合分析。比如我想知道在哪个时间段，用户的量最多？哪个用户一年购物最多？诸如此类的指标。那就要从新设计数据库的表结构了。对于数据分析和数据挖掘，我们引入了数据仓科概念。数据仓库的表结构是依照分析需求，分析维度，分析指标进行设计的。 数据库与数据仓库的区别实际讲的是OLTP与OLAP的区别。 操作型处理叫联机事务处理OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发的支持用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。分析型处理叫联机分析处理OLAP（On-Line Analytical Processing）一般针对某些主题历史数据进行分析，支持管理决策。 操作型处理 分析型处理 细节的 综合或者提炼的 实体-关系（E-R）模型 星型模型或雪花模型 存储瞬间数据 存储历史数据，不包含最近的数据 可更新的 只读、只追加 一次操作一个单元 一次操作一个集合 性能要求高，响应时间短 性能要求宽松 面向事务 面向分析 一次操作数据量小 支持决策需求 数据量小 数据量大 客户订单、库存水平和银行账户查询 客户收益分析、市场细分]]></content>
      <categories>
        <category>大数据介绍使用</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper详解]]></title>
    <url>%2F2019%2F04%2F25%2F1_Zookeeper%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Zookeeperzookeeper概述Zookeeper 是一个开源的分布式的，为分布式应用提供协调服务的 Apache 项目。它包含一个简单的原语集，分布式应用程序可以基于它实现同步服务，配置维护和命名服务等 为什么要使用zookeeper大部分分布式应用需要一个主控、协调器或控制器来管理物理分布的子进程（如资源、任务分配等）.目前，大部分应用需要开发私有的协调程序，缺乏一个通用的机制协调程序的反复编写浪费，且难以形成通用、伸缩性好的协调器ZooKeeper：提供通用的分布式锁服务，用以协调分布式应用zookeepr可以做什么: Hadoop2.X使用Zookeeper的事件处理确保整个集群只有一个活跃的NameNode,存储配置信息等. HBase使用Zookeeper的事件处理确保整个集群只有一个HMaster,察觉HRegionServer联机和宕机,存储访问控制列表等.zookeeper原理 工作机制看PPTzookeeper服务Zookeeper的角色: 领导者（leader），负责进行投票的发起和决议，更新系统状态 学习者（learner），包括跟随者（follower）和观察者（observer），follower用于接受客户端请求并给客户端返回结果，在选主过程中参与投票Observer可以接受客户端连接，将写请求转发给leader，但observer不参加投票过程，只同步leader的状态，observer的目的是为了扩展系统，提高读取速度 客户端（client），请求发起方 zookeeper的数据模型 层次化的目录结构，命名符合常规文件系统规范，类似于Linux 每个节点在zookeeper中叫做znode,并且其有一个唯一的路径标识 节点Znode可以包含数据和子节点，但是EPHEMERAL类型的节点不能有子节点 Znode中的数据可以有多个版本，比如某一个路径下存有多个数据版本，那么查询这个路径下的数据就需要带上版本 客户端应用可以在节点上设置监视器 节点不支持部分读写，而是一次性完整读写 zookeeper的应用场景统一命名服务、统一的配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等 统一命名服务分布式应用中，通常需要有一套完整的命名规则，既能够产生唯一的名称又便于人识别和记住，通常情况下用树形的名称结构是一个理想的选择，树形的名称结构是一个有层次的目录结构，既对人友好又不会重复。 类似于域名与ip之间对应关系，ip 不容易记住，而域名容易记住 通过名称来获取资源或服务的地址，提供者等信息。 统一的配置管理配置的管理在分布式应用环境中很常见，例如同一个应用系统需要多台 PC Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 PC Server，这样非常麻烦而且容易出错。将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中。 举例分布式环境下，配置文件管理和同步是一个常见问题。 一个集群中，所有节点的配置信息是一致的，比如 Hadoop 集群。 对配置文件修改后，希望能够快速同步到各个节点上。 配置管理可交由ZooKeeper实现。 可将配置信息写入ZooKeeper上的一个znode。 各个节点监听这个znode。 一旦znode中的数据被修改，ZooKeeper将通知各个节点 统一的集群管理分布式环境中，实时掌握每个节点的状态是必要的。可根据节点实时状态做出一些调整。可交由ZooKeeper实现。 可将节点信息写入ZooKeeper上的一个Znode。 监听这个Znode可获取它的实时状态变化。 典型应用: HBase中Master状态监控与选举。 服务器动态上下线：看PPT 软负载均衡 zookeeper安装 下载下载地址 配置安装 1234567[root@master Zookeeper]# tar -zxvf zookeeper-3.4.12.tar.gz[root@master conf]# mv zoo_sample.cfg zoo.cfg[root@master conf]# vim zoo.cfg## 修改以下内容dataDir=/opt/apps/Zookeeper/data## 创建相应目录[root@master Zookeeper]# mkdir data 配置参数: tickTime=2000：通信心跳数，Zookeeper服务器心跳时间，单位毫秒Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。 initLimit=10：Leader和Follower初始通信时限集群中的follower跟随者服务器与leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。投票选举新leader的初始化时间Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。Leader允许Follower在initLimit时间内完成这个工作。 syncLimit=5：Leader 和 Follower 同步通信时限集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。 dataDir：数据文件目录+数据持久化路径保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。 clientPort=2181：客户端连接端口监听客户端连接的端口 简单操作1234567891011121314151617181920212223## 启动zookeeper服务[root@master Zookeeper]# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /opt/apps/Zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED## 查看进程[root@master Zookeeper]# jps1777 ResourceManager9987 QuorumPeerMain1413 NameNode10006 Jps1614 SecondaryNameNode## 查看服务状态[root@master Zookeeper]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/apps/Zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: standalone## 启动客户端[root@master Zookeeper]# zkCli.sh## 退出客户端[zk: localhost:2181(CONNECTED) 1] quit## 停止zookeeper服务器端[root@master Zookeeper]# zkServer.sh stop选举机制 半数机制（Paxos 协议）：集群中半数以上机器存活，集群可用。所以 zookeeper 适合装在奇数台机器上。 Zookeeper 虽然在配置文件中并没有指定 master 和 slave。但是，zookeeper 工作时，是有一个节点为 leader，其他则为 follower，Leader是通过内部的选举机制临时产生的。 以一个简单的例子来说明整个选举的过程。假设有五台服务器组成的 zookeeper 集群，它们的 id 从 1-5，同时它们都是最新启动的， 服务器 1 启动，此时只有它一台服务器启动了，它发出去的包没有任何响应，所以它的选举状态一直是 LOOKING 状态。 服务器 2 启动，它与最开始启动的服务器 1 进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以 id 值较大的服务器 2 胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是 3)，所以服务器 1、2 还是继续保持LOOKING 状态。 服务器 3 启动，根据前面的理论分析，服务器 3 成为服务器 1、2、3 中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的 leader。 服务器 4 启动，根据前面的分析，理论上服务器 4 应该是服务器 1、2、3、4 中最大的，但是由于前面已经有半数以上的服务器选举了服务器 3，所以它只能接收当小弟的命了。 服务器 5 启动，同 4 一样当小弟。 节点类型Znode有两种类型：: 短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除 Znode有四种形式的目录节点（默认是persistent ）: 持久化目录节点（PERSISTENT） 客户端与zookeeper断开连接后，该节点依旧存在 持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 临时目录节点（EPHEMERAL）客户端与zookeeper断开连接后，该节点被删除 临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL）客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 完全分布式安装在单机的基础上添加以下操作 123456789101112131415## 在zoo.cfg中添加以下内容## 配置集群server.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888##配置参数详情:## server.A=B:C:D## A :表示一个数字，这个数字表示第几号服务器，配置在myid的文件中## B :服务的地址，也就是IP地址，但是IP不好记，所以配置了Hosts文件，让这个IP映射为域名## C :是本台服务器与集群中的Leader服务器交换信息的端口## D :万一leader挂了，就需要重新来选举Leader，用这个端口进行选举投票## 创建一个新的配置文件myid[root@master data]# vim myid## 添加一个数字作为本机的ID 命令行操作1234567891011121314151617181920212223[zk: localhost:2181(CONNECTED) 0] help stat path [watch] #查看节点的状态 set path data [version] #设置节点的数据 ls path [watch] #查看当前节点中所包含的内容 ls2 path [watch] #查看当前节点的数据并且能够看到更新次数等数据 delete path [version] #删除节点 sync path #同步节点 rmr path #递归删除节点 get path [watch] #获取节点的数据 create [-s] [-e] path data acl #创建节点并添加数据，-s带序号，-e临时节点 quit #退出 [zk: localhost:2181(CONNECTED) 14] stat /yangxiucZxid = 0x900000015 #创建节点的事务ID，是Zookeeper中所有修改的总的次序，每一个修改都会有一个这样的IDctime = Mon Aug 20 02:52:21 EDT 2018 #创建的时间mZxid = 0x900000015 #最后更新的zxidmtime = Mon Aug 20 02:52:21 EDT 2018 #最后修改的时候pZxid = 0x900000015 #最后更新的子节点的zxidcversion = 0 #更新子节点的次数（不包含对子节点数据的修改）dataVersion = 0 #数据的变化版本号aclVersion = 0 #访问控制列表的变化号ephemeralOwner = 0x0 #如果不是临时节点则为0，是临时节点则为拥有这的session iddataLength = 2 #数据长度numChildren = 0 #子节点的个数 API操作]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类加载器+反射+动态代理]]></title>
    <url>%2F2019%2F04%2F25%2F1_%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%2B%E5%8F%8D%E5%B0%84%2B%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[类加载器+反射+动态代理一. JVM和类 运行Java: java 带有main方法的类名 启动JVM,并加载字节码. 当调用java命令来运行某个Java程序时,该命令将会启动一个JVM进程.同一个JVM中的所有线程,变量都处于同一个进程中,共享该JVM的内存区域.当出现以下情况是,JVM会退出: 程序正常执行结束 使用System.exit(0)方法 出现异常时,没有捕获异常 平台强制结束JVM进程. JVM进程一旦结束,该进程中内存中的数据将会丢失. 二. 类的加载机制当程序主动使用到某个类时,如果该类还未被加载进内存中,则系统会通过加载,连接,初始化三个步骤来对该类进行初始化操作. 类的加载: 1. 类加载时指将类的class文件(字节码文件)载入内存中,并为之创建一个java.lang.Class对象,我们称之为字节码对象. 2. 类的加载过程由类加载器(ClassLoader)完成,类加载器通常由JVM提供,我们称之为系统类加载器,我们也可以继承ClassLoader类来提供自定义类加载器. 3. 不同的类加载器可以实现加载本地字节码文件,jar包中的字节码,通过网络加载字节码等.类的连接当类被加载进内存之后,系统为之生产一个对应的Class对象,接着把类的二进制数据合并到JRE（运行环境）中。 验证：检测被加载的类是否有正确的内部结构 准备：负责为类的static变量分配内存,并设置默认值 解析：把类的二进制数据中的符号引用替换为直接引用(书籍：深入分析JVM) 类的初始化在此阶段,JVM负责对类进行初始化,主要就是对static变量进行初始化。只有主动使用类时才会执行初始化类的初始化包含以下几个步骤: 如果该类还未被加载和连接,则程序先加载并连接该类. 如果该类的直接父类还未被初始化,则先初始化其父类,一直初始化类继承结构到Object 如果类中有初始化语句(静态代码块),则系统依次执行这些初始化语句. 三. 什么是反射问题1在面向对象中提到,一切事物都可以看成是对象,那么问题来了,类这种事物是啥对象呢?又使用什么类来表示类这种对象呢?概念: 元数据：数据的数据 反射：得到类的元数据的过程。在运行时期，动态去获取某一个类中的成员信息（构造器，方法，字段，内部类，接口，父类等等）。 我们把类中的每一种成员,都描述成一个新的类：|类名|所在包|描述||—|—|—||Class| java.lang |表示所有的类||Constructor| java.lang.reflect |表示所有的构造器||Method| java.lang.reflect |表示所有的方法||Field| java.lang.reflect |表示所有的字段| 四. Class类Class类用来描述类或者接口的类型,描述类的类.Class类的实例在JVM中的一份份字节码,Class实例表示在JVM中的类或者接口,枚举是一种特殊的类,注解是一种特殊的接口.当程序第一次使用某一个java.util.Date类的时候,就会把该类的字节码对象加载进JVM,并创建出一个Class对象.此时的Class对象就表示java.util.Date的字节码.如何创建Class对象1234567891011121314 //方式一：使用class属性获取Class c1 = Date.class;//方式二：通过对象的getClass()方法获取，getClass()属于Object的方法Date d = new Date();Class c2 = d.getClass();//方式三：通过Class中的forName方法获取【用的较多】Class c3 = Class.forName("java.util.Date");//打印输出System.out.println(c1);System.out.println(c2);System.out.println(c3);//同一个类在JVM中只存在一份字节码对象，下面的结果均为trueSystem.out.println(c1==c2);System.out.println(c2==c3);九大内置Class对象在上述讲了三种获取Class对象的方式,基本数据类型不能表示为对象,也就不能使用getClass的方式,基本类型没有类名的概念,也不能使用Class.forName的方式,如何表示基本类型的字节码对象呢?所有的数据类型都有class属性。Class clz = 数据类型.class;JVM中预先提供好的Class实例,分别:byte,short,int,long,float,double,boolea,char,void.byte.class,short.class,int.class,….void.class.数组的Class对象方式1: 数组类型.class; 方式2: 数组对象.getClass();注意:所有的具有相同的维数和相同元素类型的数组共享同一份字节码对象,和元素没有关系获取构造器public Constructor[] getConstructors() 该方法只能获取当前Class所表示类的public修饰的构造器 public Constructor[] getDeclaredConstructors()获取当前Class所表示类的所有的构造器,和访问权限无关]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>反射</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据01-集群搭建]]></title>
    <url>%2F2019%2F04%2F25%2F1_%E5%A4%A7%E6%95%B0%E6%8D%AE01-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[大数据01-集群搭建1. 集群规划集群节点分配|主机名|主机IP||—|—||master|192.168.100.100||slave1|192.168.100.101||slave2|192.168.100.102||slave3|192.168.100.103|软件版本|软件名称|版本号||—|—||Java|1.8.0_172||CentOS|CentOS-7-x86_64-DVD-1708||Hadoop|2.7.6|各软件安装路径|软件名|路径地址||—|—||Hadoop|/opt/apps/Hadoop||Java|/opt/apps/Java|2. 基础软件服务安装2.1 ssh安装安装完整的ssh服务，文件上传以及远程操作都是该服务 1yum -y install openssh-clients 单独启动脚本12hdfs相关的，就使用hadoop-daemons.shyarn相关的（MR），就使用yarn-daemons.sh]]></content>
      <categories>
        <category>大数据介绍使用</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据01-大数据概述]]></title>
    <url>%2F2019%2F04%2F25%2F1_%E5%A4%A7%E6%95%B0%E6%8D%AE01-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[大数据01-大数据概述1. 什么是大数据基本概念总体来说就是数据处理,在互联网技术发展到现今阶段，大量日常、工作等事务产生的数据都已经信息化，人类产生的数据量相比以前有了爆炸式的增长，以前的传统的数据处理技术已经无法胜任，需求催生技术，一套用来处理海量数据的软件工具应运而生，这就是大数据！处理海量数据的核心技术海量数据存储：分布式海量数据计算：分布式存储框架HDFS——分布式文件存储系统（Hadoop中的存储框架）HBase——分布式数据库系统Kafka——分布式消息缓存系统(实时流式数据处理场景中应用广泛)计算框架要解决的核心问题就是帮用户将处理逻辑在很多机器上并行 MapReduce—— 离线批处理/Hadoop中的运算框架 Spark —— 离线批处理/实时流式计算 Storm —— 实时流式计算 辅助类的工具: Hive —— 数据仓库工具：可以接收sql，翻译成MapReduce或者Spark程序运行 Flume——用于数据采集 Sqoop——数据迁移，ETL工具 ElasticSearch—— 分布式的搜索引擎 大数据在现实生活中的具体应用: 数据处理的最典型应用：公司的产品运营情况分析 电商推荐系统：基于海量的浏览行为、购物行为数据，进行大量的算法模型的运算，得出各类推荐结论，以供电商网站页面来为用户进行商品推荐 精准广告推送系统：基于海量的互联网用户的各类数据，统计分析，进行用户画像（得到用户的各种属性标签），然后可以为广告主进行有针对性的精准的广告投放 2. 什么是Hadoop Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构 主要解决，海量数据的存储和海量数据的分析计算问题。 广义上来说，HADOOP 通常是指一个更广泛的概念——HADOOP 生态圈 Hadoop 发展历史: Lucene–Doug Cutting 开创的开源软件，用 java 书写代码，实现与 Google 类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎 2001 年年底成为 apache 基金会的一个子项目 对于大数量的场景，Lucene 面对与 Google 同样的困难 学习和模仿 Google 解决这些问题的办法 ：微型版 Nutch 可以说 Google 是 hadoop 的思想之源(Google 在大数据方面的三篇论文) - GFS ---&gt;HDFS - Map-Reduce ---&gt;MR - BigTable ---&gt;Hbase 2003-2004 年，Google 公开了部分 GFS 和 Mapreduce 思想的细节，以此为基础 Doug Cutting 等人用了 2 年业余时间实现了 DFS 和 Mapreduce 机制，使 Nutch 性能飙升 2005 年 Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入 Apache基金会。2006 年 3 月份，Map-Reduce 和 Nutch Distributed File System (NDFS) 分别被纳入称为 Hadoop 的项目中 名字来源于 Doug Cutting 儿子的玩具大象 Hadoop 就此诞生并迅速发展，标志这云计算时代来临 Hadoop 三大发行版本Hadoop 三大发行版本: Apache、Cloudera、Hortonworks。Apache 版本最原始（最基础）的版本，对于入门学习最好。Cloudera 在大型互联网企业中用的较多。Hortonworks 文档较好。 Apache Hadoop 官网地址：http://hadoop.apache.org/releases.html下载地址：https://archive.apache.org/dist/hadoop/common/ Cloudera Hadoop 官网地址：https://www.cloudera.com/downloads/cdh/5-10-0.html 下载地址：http://archive-primary.cloudera.com/cdh5/cdh/5/ 2008 年成立的 Cloudera 是最早将 Hadoop 商用的公司，为合作伙伴提供 Hadoop 的商用解决方案，主要是包括支持、咨询服务、培训。 2009 年 Hadoop 的创始人 Doug Cutting 也加盟 Cloudera 公司。Cloudera 产品主要为 CDH，Cloudera Manager，Cloudera Support CDH 是 Cloudera 的 Hadoop 发行版，完全开源，比 Apache Hadoop 在兼容性，安全性，稳定性上有所增强。 Cloudera Manager 是集群的软件分发及管理监控平台，可以在几个小时内部署好一个 Hadoop 集群，并对集群的节点及服务进行实时监控。Cloudera Support 即是对 Hadoop 的技术支持。 Cloudera 的标价为每年每个节点 4000 美元。Cloudera 开发并贡献了可实时处理大数据的 Impala 项目。 Hortonworks Hadoop官网地址：https://hortonworks.com/products/data-center/hdp/ 下载地址：https://hortonworks.com/downloads/#data-platform 2011 年成立的 Hortonworks 是雅虎与硅谷风投公司 Benchmark Capital 合资组建. 公司成立之初就吸纳了大约 25 名至 30 名专门研究 Hadoop 的雅虎工程师，上述工程师均在 2005 年开始协助雅虎开发 Hadoop，贡献了 Hadoop80%的代码。 雅虎工程副总裁、雅虎 Hadoop 开发团队负责人 Eric Baldeschwieler 出任 Hortonworks 的首席执行官。 Hortonworks 的主打产品是 Hortonworks Data Platform（HDP），也同样是 100%开源的产品，HDP 除常见的项目外还包括了 Ambari，一款开源的安装和管理系统. HCatalog，一个元数据管理系统，HCatalog 现已集成到 Facebook 开源的 Hive 中。 Hortonworks 的 Stinger 开创性的极大的优化了 Hive 项目。Hortonworks 为入门提供了一个非常好的，易于使用的沙盒。 Hortonworks 开发了很多增强特性并提交至核心主干，这使得 Apache Hadoop 能够在包括Window Server和Windows Azure在内的 microsoft Windows平台上本地运行。定价以集群为基础，每 10 个节点每年为 12500 美元。 Hadoop 的优势: 高可靠性 因为 Hadoop 假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处理。 高扩展性在集群间分配任务数据，可方便的扩展数以千计的节点。 高效性在 MapReduce 的思想下，Hadoop 是并行工作的，以加快任务处理速度。 高容错性自动保存多份副本数据，并且能够自动将失败的任务重新分配。 Hadoop 组成: Hadoop HDFS一个高可靠、高吞吐量的分布式文件系统。 Hadoop MapReduce一个分布式的离线并行计算框架。 Hadoop YARN作业调度与集群资源管理的框架。 Hadoop Common支持其他模块的工具模块（Configuration、RPC、序列化机制、日志操作）。]]></content>
      <categories>
        <category>大数据介绍使用</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xml & Tomcat]]></title>
    <url>%2F2019%2F04%2F24%2F1_Xml%20%26%20Tomcat%2F</url>
    <content type="text"><![CDATA[#Xml &amp; Tomcat ##Xml eXtendsible markup language 可扩展的标记语言 ###XML 有什么用? 可以用来保存数据 可以用来做配置文件 数据传输载体 ![document.png-27.5kB][1] ##定义xml 其实就是一个文件，文件的后缀为 .xml ###. 文档声明 简单声明， version : 解析这个xml的时候，使用什么版本的解析器解析 &lt;?xml version=&quot;1.0&quot; ?&gt; encoding : 解析xml中的文字的时候，使用什么编码来翻译 &lt;?xml version=&quot;1.0&quot; encoding=&quot;gbk&quot; ?&gt; standalone : no - 该文档会依赖关联其他文档 ， yes-- 这是一个独立的文档 &lt;?xml version=&quot;1.0&quot; encoding=&quot;gbk&quot; standalone=&quot;no&quot; ?&gt;###encoding详解 在解析这个xml的时候，使用什么编码去解析。 —解码。 文字， 而是存储这些文字对应的二进制 。 那么这些文字对应的二进制到底是多少呢？ 根据文件使用的编码 来得到。 默认文件保存的时候，使用的是GBK的编码保存。 所以要想让我们的xml能够正常的显示中文，有两种解决办法 让encoding也是GBK 或者 gb2312 . gbk=gb2312+生僻字+繁体字 如果encoding是 utf-8 ， 那么保存文件的时候也必须使用utf-8 保存的时候见到的ANSI 对应的其实是我们的本地编码 GBK。 为了通用，建议使用UTF-8编码保存，以及encoding 都是 utf-8 ###元素定义（标签） 其实就是里面的标签， &lt;&gt; 括起来的都叫元素 。 成对出现。 如下： &lt;stu&gt; &lt;/stu&gt; 文档声明下来的第一个元素叫做根元素 (根标签) 标签里面可以嵌套标签 空标签 既是开始也是结束。 一般配合属性来用。 &lt;age/&gt; &lt;stu&gt; &lt;name&gt;张三&lt;/name&gt; &lt;age/&gt; &lt;/stu&gt; 标签可以自定义。 XML 命名规则 XML 元素必须遵循以下命名规则： 名称可以含字母、数字以及其他的字符 名称不能以数字或者标点符号开始 名称不能以字符 “xml”（或者 XML、Xml）开始 名称不能包含空格 命名尽量简单，做到见名知义###简单元素 &amp; 复杂元素 简单元素 元素里面包含了普通的文字 复杂元素 元素里面还可以嵌套其他的元素 ###属性的定义 定义在元素里面， &lt;元素名称 属性名称=”属性的值”&gt;&lt;/元素名称&gt; &lt;stus&gt; &lt;stu id=&quot;10086&quot;&gt; &lt;name&gt;张三&lt;/name&gt; &lt;age&gt;18&lt;/age&gt; &lt;/stu&gt; &lt;stu id=&quot;10087&quot;&gt; &lt;name&gt;李四&lt;/name&gt; &lt;age&gt;28&lt;/age&gt; &lt;/stu&gt; &lt;/stus&gt;##xml注释： 与html的注释一样。 &lt;!-- --&gt; 如： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!-- //这里有两个学生 //一个学生，名字叫张三， 年龄18岁， 学号：10086 //另外一个学生叫李四 。。。 --&gt; xml的注释，不允许放置在文档的第一行。 必须在文档声明的下面。 ##CDATA区 非法字符 严格地讲，在 XML 中仅有字符 “&lt;”和”&amp;” 是非法的。省略号、引号和大于号是合法的，但是把它们替换为实体引用是个好的习惯。 &lt; &amp;lt; &amp; &amp;amp; 如果某段字符串里面有过多的字符， 并且里面包含了类似标签或者关键字的这种文字，不想让xml的解析器去解析。 那么可以使用CDATA来包装。 不过这个CDATA 一般比较少看到。 通常在服务器给客户端返回数据的时候。 &lt;des&gt;&lt;![CDATA[&lt;a href=&quot;http://www.baidu.com&quot;&gt;我爱黑马训练营&lt;/a&gt;]]&gt;&lt;/des&gt;##XML 解析 其实就是获取元素里面的字符数据或者属性数据。 ###XML解析方式(面试常问) 有很多种，但是常用的有两种。 DOM SAX ![parse_type.png-45.9kB][2] ###针对这两种解析方式的API 一些组织或者公司， 针对以上两种解析方式， 给出的解决方案有哪些？ jaxp sun公司。 比较繁琐 jdom dom4j 使用比较广泛###Dom4j 基本用法 element.element(&quot;stu&quot;) : 返回该元素下的第一个stu元素 element.elements(); 返回该元素下的所有子元素。 创建SaxReader对象 指定解析的xml 获取根元素。 根据根元素获取子元素或者下面的子孙元素 try { //1. 创建sax读取对象 SAXReader reader = new SAXReader(); //jdbc -- classloader //2. 指定解析的xml源 Document document = reader.read(new File(&quot;src/xml/stus.xml&quot;)); //3. 得到元素、 //得到根元素 Element rootElement= document.getRootElement(); //获取根元素下面的子元素 age //rootElement.element(&quot;age&quot;) //System.out.println(rootElement.element(&quot;stu&quot;).element(&quot;age&quot;).getText()); //获取根元素下面的所有子元素 。 stu元素 List&lt;Element&gt; elements = rootElement.elements(); //遍历所有的stu元素 for (Element element : elements) { //获取stu元素下面的name元素 String name = element.element(&quot;name&quot;).getText(); String age = element.element(&quot;age&quot;).getText(); String address = element.element(&quot;address&quot;).getText(); System.out.println(&quot;name=&quot;+name+&quot;==age+&quot;+age+&quot;==address=&quot;+address); } } catch (Exception e) { e.printStackTrace(); }SaxReader 创建好对象 。 DocumentElement 看文档 记住关键字 。 有对象先点一下。 看一下方法的返回值。 根据平时的积累。 getXXX setXXX ###Dom4j 的 Xpath使用 dom4j里面支持Xpath的写法。 xpath其实是xml的路径语言，支持我们在解析xml的时候，能够快速的定位到具体的某一个元素。 添加jar包依赖 jaxen-1.1-beta-6.jar 在查找指定节点的时候，根据XPath语法规则来查找 后续的代码与以前的解析代码一样。 //要想使用Xpath， 还得添加支持的jar 获取的是第一个 只返回一个。 Element nameElement = (Element) rootElement.selectSingleNode(&quot;//name&quot;); System.out.println(nameElement.getText()); System.out.println(&quot;----------------&quot;); //获取文档里面的所有name元素 List&lt;Element&gt; list = rootElement.selectNodes(&quot;//name&quot;); for (Element element : list) { System.out.println(element.getText()); }##XML 约束【了解】 如下的文档， 属性的ID值是一样的。 这在生活中是不可能出现的。 并且第二个学生的姓名有好几个。 一般也很少。那么怎么规定ID的值唯一， 或者是元素只能出现一次，不能出现多次？ 甚至是规定里面只能出现具体的元素名字。 &lt;stus&gt; &lt;stu id=&quot;10086&quot;&gt; &lt;name&gt;张三&lt;/name&gt; &lt;age&gt;18&lt;/age&gt; &lt;address&gt;深圳&lt;/address&gt; &lt;/stu&gt; &lt;stu id=&quot;10086&quot;&gt; &lt;name&gt;李四&lt;/name&gt; &lt;name&gt;李五&lt;/name&gt; &lt;name&gt;李六&lt;/name&gt; &lt;age&gt;28&lt;/age&gt; &lt;address&gt;北京&lt;/address&gt; &lt;/stu&gt; &lt;/stus&gt;###DTD 语法自成一派， 早起就出现的。 可读性比较差。 ![image_1cegiq80lq0717e62ti1nf31cpe11.png-46.2kB][3] 引入网络上的DTD 1234&lt;!-- 引入dtd 来约束这个xml --&gt;&lt;!-- 文档类型 根标签名字 网络上的dtd dtd的名称 dtd的路径&lt;!DOCTYPE stus PUBLIC "//UNKNOWN/" "unknown.dtd"&gt; --&gt; 引入本地的DTD 12&lt;!-- 引入本地的DTD ： 根标签名字 引入本地的DTD dtd的位置 --&gt;&lt;!-- &lt;!DOCTYPE stus SYSTEM "stus.dtd"&gt; --&gt; 直接在XML里面嵌入DTD的约束规则12345678910111213&lt;!-- xml文档里面直接嵌入DTD的约束法则 --&gt;&lt;!DOCTYPE stus [ &lt;!ELEMENT stus (stu)&gt; &lt;!ELEMENT stu (name,age)&gt; &lt;!ELEMENT name (#PCDATA)&gt; &lt;!ELEMENT age (#PCDATA)&gt;]&gt;&lt;stus&gt; &lt;stu&gt; &lt;name&gt;张三&lt;/name&gt; &lt;age&gt;18&lt;/age&gt; &lt;/stu&gt;&lt;/stus&gt; 123456&lt;!ELEMENT stus (stu)&gt; : stus 下面有一个元素 stu ， 但是只有一个&lt;!ELEMENT stu (name , age)&gt; stu下面有两个元素 name ,age 顺序必须name-age&lt;!ELEMENT name (#PCDATA)&gt; &lt;!ELEMENT age (#PCDATA)&gt;&lt;!---&lt;!ATTLIST 元素名称 属性名称 属性类型 默认值&gt;--&gt;&lt;!ATTLIST stu id CDATA #IMPLIED&gt; stu有一个属性 文本类型， 该属性可有可无 元素的个数： ＋ 一个或多个 * 零个或多个 ? 零个或一个 属性的类型定义 CDATA : 属性是普通文字 ID : 属性的值必须唯一,&lt;!-- ID 类型的属性必须为 #REQUIRED --&gt; &lt;!ELEMENT stu (name , age)&gt; 按照顺序来 &lt;!ELEMENT stu (name | age)&gt; 两个中只能包含一个子元素###Schema 其实就是一个xml ， 使用xml的语法规则， xml解析器解析起来比较方便 ， 是为了替代DTD 。 但是Schema 约束文本内容比DTD的内容还要多。 所以目前也没有真正意义上的替代DTD 约束文档： &lt;!-- xmlns : xml namespace : 名称空间 / 命名空间 targetNamespace : 目标名称空间 。 下面定义的那些元素都与这个名称空间绑定上。 elementFormDefault ： 元素的格式化情况。 --&gt; &lt;schema xmlns=&quot;http://www.w3.org/2001/XMLSchema&quot; targetNamespace=&quot;http://www.zhiyou100.com/teacher&quot; elementFormDefault=&quot;qualified&quot;&gt; &lt;element name=&quot;teachers&quot;&gt; &lt;complexType&gt; &lt;sequence maxOccurs=&quot;unbounded&quot;&gt; &lt;!-- 这是一个复杂元素 --&gt; &lt;element name=&quot;teacher&quot;&gt; &lt;complexType&gt; &lt;sequence&gt; &lt;!-- 以下两个是简单元素 --&gt; &lt;element name=&quot;name&quot; type=&quot;string&quot;&gt;&lt;/element&gt; &lt;element name=&quot;age&quot; type=&quot;int&quot;&gt;&lt;/element&gt; &lt;/sequence&gt; &lt;/comp lexType&gt; &lt;/element&gt; &lt;/sequence&gt; &lt;/complexType&gt; &lt;/element&gt; &lt;/schema&gt; 实例文档： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!-- xmlns:xsi : 这里必须是这样的写法，也就是这个值已经固定了。 xmlns : 这里是名称空间，也固定了，写的是schema里面的顶部目标名称空间 xsi:schemaLocation : 有两段： 前半段是名称空间，也是目标空间的值 ， 后面是约束文档的路径。 --&gt; &lt;teachers xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.zhiyou100.com/teacher&quot; xsi:schemaLocation=&quot;http://www.zhiyou100.com/teacher teacher.xsd&quot; &gt; &lt;teacher&gt; &lt;name&gt;zhangsan&lt;/name&gt; &lt;age&gt;19&lt;/age&gt; &lt;/teacher&gt; &lt;teacher&gt; &lt;name&gt;lisi&lt;/name&gt; &lt;age&gt;29&lt;/age&gt; &lt;/teacher&gt; &lt;teacher&gt; &lt;name&gt;lisi&lt;/name&gt; &lt;age&gt;29&lt;/age&gt; &lt;/teacher&gt; &lt;/teachers&gt;##名称空间的作用 一个xml如果想指定它的约束规则， 假设使用的是DTD ，那么这个xml只能指定一个DTD ， 不能指定多个DTD 。 但是如果一个xml的约束是定义在schema里面，并且是多个schema，那么是可以的。简单的说： 一个xml 可以引用多个schema约束。 但是只能引用一个DTD约束。 名称空间的作用就是在 写元素的时候，可以指定该元素使用的是哪一套约束规则。 默认情况下 ，如果只有一套规则，那么都可以这么写 &lt;name&gt;张三&lt;/name&gt; &lt;aa:name&gt;&lt;/aa:name&gt; &lt;bb:name&gt;&lt;/bb:name&gt;###程序架构 网页游戏 C/S(client/server) QQ 微信 LOL 优点： 有一部分代码写在客户端， 用户体验比较好。 缺点： 服务器更新，客户端也要随着更新。 占用资源大。 B/S(browser/server) 网页游戏 ， WebQQ … 优点： 客户端只要有浏览器就可以了。 占用资源小， 不用更新。 缺点： 用户体验不佳。 ###服务器 其实服务器就是一台电脑。 配置比一般的要好。 ###Web服务器软件 客户端在浏览器的地址栏上输入地址 ，然后web服务器软件，接收请求，然后响应消息。处理客户端的请求， 返回资源 | 信息 Web应用 需要服务器支撑。 index.html Tomcat apache WebLogic BEA Websphere IBM IIS 微软###Tomcat安装 直接解压 ，然后找到bin/startup.bat 可以安装 启动之后，如果能够正常看到黑窗口，表明已经成功安装。 为了确保万无一失， 最好在浏览器的地址栏上输入 ： http://localhost:8080 , 如果有看到内容 就表明成功了。 如果双击了startup.bat, 看到一闪而过的情形，一般都是 JDK的环境变量没有配置。 ###Tomcat目录介绍 bin## &gt; 包含了一些jar , bat文件 。 startup.batconf##​ tomcat的配置 server.xml web.xml lib tomcat运行所需的jar文件logs 运行的日志文件temp 临时文件webapps## 发布到tomcat服务器上的项目，就存放在这个目录。 work(目前不用管) jsp翻译成class文件存放地​ ##如何把一个项目发布到tomcat中 需求： 如何能让其他的电脑访问我这台电脑上的资源 。 stu.xml localhost : 本机地址###1. 拷贝这个文件到webapps/ROOT底下， 在浏览器里面访问： http://localhost:8080/stu.xml * 在webaps下面新建一个文件夹xml , 然后拷贝文件放置到这个文件夹中​ http://localhost:8080/xml/stu.xml http://localhost:8080 ： 其实对应的是到webapps/root http://localhost:8080/xml/ : 对应是 webapps/xml 使用IP地址访问： http://192.168.37.48:8080/xml/stu.xml###2. 配置虚拟路径 使用localhost：8080 打开tomcat首页， 在左侧找到tomcat的文档入口， 点击进去后， 在左侧接着找到 Context入口，点击进入。 http://localhost:8080/docs/config/context.html 在conf/server.xml 找到host元素节点。 加入以下内容。 &lt;!-- docBase ： 项目的路径地址 如： D:\xml02\person.xml path : 对应的虚拟路径 一定要以/打头。 对应的访问方式为： http://localhost:8080/a/person.xml --&gt; &lt;Context docBase=&quot;D:\xml02&quot; path=&quot;/a&quot;&gt;&lt;/Context&gt; 在浏览器地址栏上输入： http://localhost:8080/a/person.xml ###3. 配置虚拟路径 在tomcat/conf/catalina/localhost/ 文件夹下新建一个xml文件，名字可以自己定义。 person.xml 在这个文件里面写入以下内容 &lt;?xml version=&apos;1.0&apos; encoding=&apos;utf-8&apos;?&gt; &lt;Context docBase=&quot;D:\xml02&quot;&gt;&lt;/Context&gt; 在浏览器上面访问 http://localhost:8080/person/xml的名字即可 ###给Eclipse配置Tomcat 在server里面 右键新建一个服务器， 选择到apache分类， 找到对应的tomcat版本， 接着一步一步配置即可。 配置完毕后， 在server 里面， 右键刚才的服务器，然后open ， 找到上面的Server Location , 选择中间的 Use Tomcat installation… 创建web工程， 在WebContent下定义html文件， 右键工程， run as server ##总结： xml 1. 会定义xml 2. 会解析xml dom4j 基本解析 Xpath手法 tomcat 1. 会安装 ，会启动 ， 会访问。 2. 会设置虚拟路径 3. 给eclipse配置tomcat​ #Http协议 什么是协议 双方在交互、通讯的时候， 遵守的一种规范、规则。 http协议 针对网络上的客户端 与 服务器端在执行http请求的时候，遵守的一种规范。 其实就是规定了客户端在访问服务器端的时候，要带上哪些东西， 服务器端返回数据的时候，也要带上什么东西。 版本 1.0 请求数据，服务器返回后， 将会断开连接 1.1 请求数据，服务器返回后， 连接还会保持着。 除非服务器 | 客户端 关掉。 有一定的时间限制，如果都空着这个连接，那么后面会自己断掉。 ###演示客户端 如何 与服务器端通讯。 在地址栏中键入网络地址 回车 或者是平常注册的时候，点击了注册按钮 ， 浏览器都能显示出来一些东西。那么背地里到底浏览器和服务器是怎么通讯。 它们都传输了哪些数据。 安装抓包工具 HttpWatch (IE插件) 打开tomcat. 输入localhost:8080 打开首页 在首页上找到Example 字样 6.x 和 7.x 的文档页面有所不同，但是只要找到example就能够找到例子工程 选择 servlet 例子 —&gt; Request Parameter 接着点击Request Parameters 的 Execute超链接 执行tomcat的例子，然后查看浏览器和 tomcat服务器的对接细节 ###Http请求数据解释 请求的数据里面包含三个部分内容 ： 请求行 、 请求头 、请求体 请求行 POST /examples/servlets/servlet/RequestParamExample HTTP/1.1 POST ： 请求方式 ，以post去提交数据 /examples/servlets/servlet/RequestParamExample 请求的地址路径 ， 就是要访问哪个地方。 HTTP/1.1 协议版本 请求头 Accept: application/x-ms-application, image/jpeg, application/xaml+xml, image/gif, image/pjpeg, application/x-ms-xbap, */* Referer: http://localhost:8080/examples/servlets/servlet/RequestParamExample Accept-Language: zh-CN User-Agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E) Content-Type: application/x-www-form-urlencoded Accept-Encoding: gzip, deflate Host: localhost:8080 Content-Length: 31 Connection: Keep-Alive Cache-Control: no-cache Accept: 客户端向服务器端表示，我能支持什么类型的数据。 Referer ： 真正请求的地址路径，全路径 Accept-Language: 支持语言格式 User-Agent: 用户代理 向服务器表明，当前来访的客户端信息。 Content-Type： 提交的数据类型。经过urlencoding编码的form表单的数据 Accept-Encoding： gzip, deflate ： 压缩算法 。 Host ： 主机地址 Content-Length： 数据长度 Connection : Keep-Alive 保持连接 Cache-Control ： 对缓存的操作 请求体 浏览器真正发送给服务器的数据 发送的数据呈现的是key=value ,如果存在多个数据，那么使用 &amp; firstname=zhang&amp;lastname=sansan###Http响应数据解析 请求的数据里面包含三个部分内容 ： 响应行 、 响应头 、响应体 HTTP/1.1 200 OK Server: Apache-Coyote/1.1 Content-Type: text/html;charset=ISO-8859-1 Content-Length: 673 Date: Fri, 17 Feb 2017 02:53:02 GMT ...这里还有很多数据... 响应行 HTTP/1.1 200 OK 协议版本 状态码 咱们这次交互到底是什么样结果的一个code. 200 : 成功，正常处理，得到数据。 403 : for bidden 拒绝 404 ： Not Found 500 ： 服务器异常 OK 对应前面的状态码 响应头 Server: 服务器是哪一种类型。 Tomcat Content-Type ： 服务器返回给客户端你的内容类型 Content-Length ： 返回的数据长度 Date ： 通讯的日期，响应的时间 ##Get 和 Post请求区别 post 1. 数据是以流的方式写过去，不会在地址栏上面显示。 现在一般提交数据到服务器使用的都是POST 2. 以流的方式写数据，所以数据没有大小限制。 get 1. 会在地址栏后面拼接数据，所以有安全隐患。 一般从服务器获取数据，并且客户端也不用提交上面数据的时候，可以使用GET 2. 能够带的数据有限， 1kb大小 ###Web资源 在http协议当中，规定了请求和响应双方， 客户端和服务器端。与web相关的资源。 有两种分类 静态资源 html 、 js、 css 动态资源 servlet/jsp ##Servlet servlet是什么? 其实就是一个java程序，运行在我们的web服务器上，用于接收和响应 客户端的http请求。 更多的是配合动态资源来做。 当然静态资源也需要使用到servlet，只不过是Tomcat里面已经定义好了一个 DefaultServlet ###Hello Servlet 得写一个Web工程 ， 要有一个服务器。 测试运行Web工程 1. 新建一个类， 实现Servlet接口 2. 配置Servlet ， 用意： 告诉服务器，我们的应用有这么些个servlet。 在webContent/WEB-INF/web.xml里面写上以下内容。 &lt;!-- 向tomcat报告， 我这个应用里面有这个servlet， 名字叫做HelloServlet , 具体的路径是com.zhiyou100.servlet.HelloServlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.zhiyou100.servlet.HelloServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;!-- 注册servlet的映射。 servletName : 找到上面注册的具体servlet， url-pattern: 在地址栏上的path 一定要以/打头 --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/a&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 3. 在地址栏上输入 http://localhost:8080/项目名称/a###Servlet执行过程 ###Servlet的通用写法 Servlet (接口) | | GenericServlet | | HttpServlet （用于处理http的请求） 定义一个类，继承HttpServlet 复写doGet 和 doPost ##Servlet的生命周期 生命周期 从创建到销毁的一段时间 生命周期方法 从创建到销毁，所调用的那些方法。 init方法 在创建该servlet的实例时，就执行该方法。 一个servlet只会初始化一次， init方法只会执行一次 默认情况下是 ： 初次访问该servlet，才会创建实例。 service方法 只要客户端来了一个请求，那么就执行这个方法了。 该方法可以被执行很多次。 一次请求，对应一次service方法的调用 destroy方法 servlet销毁的时候，就会执行该方法 1. 该项目从tomcat的里面移除。 2. 正常关闭tomcat就会执行 shutdown.bat doGet 和 doPost不算生命周期方法，所谓的生命周期方法是指，从对象的创建到销毁一定会执行的方法， 但是这两个方法，不一定会执行。 ###让Servlet创建实例的时机 提前。 默认情况下，只有在初次访问servlet的时候，才会执行init方法。 有的时候，我们可能需要在这个方法里面执行一些初始化工作，甚至是做一些比较耗时的逻辑。 那么这个时候，初次访问，可能会在init方法中逗留太久的时间。 那么有没有方法可以让这个初始化的时机提前一点。 在配置的时候， 使用load-on-startup元素来指定， 给定的数字越小，启动的时机就越早。 一般不写负数， 从2开始即可。 &lt;servlet&gt; &lt;servlet-name&gt;HelloServlet04&lt;/servlet-name&gt; &lt;servlet-class&gt;com.zhiyou100.servlet.HelloServlet04&lt;/servlet-class&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt; &lt;/servlet&gt;##ServletConfig Servlet的配置，通过这个对象，可以获取servlet在配置的时候一些信息 先说 ， 在写怎么用， 最后说有什么用。 //1. 得到servlet配置对象 专门用于在配置servlet的信息 ServletConfig config = getServletConfig(); //获取到的是配置servlet里面servlet-name 的文本内容 String servletName = config.getServletName(); System.out.println(&quot;servletName=&quot;+servletName); //2、。 可以获取具体的某一个参数。 String address = config.getInitParameter(&quot;address&quot;); System.out.println(&quot;address=&quot;+address); //3.获取所有的参数名称 Enumeration&lt;String&gt; names = config.getInitParameterNames(); //遍历取出所有的参数名称 while (names.hasMoreElements()) { String key = (String) names.nextElement(); String value = config.getInitParameter(key); System.out.println(&quot;key===&quot;+key + &quot; value=&quot;+value); }###为什么需要有这个ServletConfig 未来我们自己开发的一些应用，使用到了一些技术，或者一些代码，我们不会。 但是有人写出来了。它的代码放置在了自己的servlet类里面。 刚好这个servlet 里面需要一个数字或者叫做变量值。 但是这个值不能是固定了。 所以要求使用到这个servlet的公司，在注册servlet的时候，必须要在web.xml里面，声明init-params 在开发当中比较少用。 刚才的这个实验， 希望基础好或者感兴趣的同学，回去自己做一下。 ##总结 Http协议 1. 使用HttpWacht 抓包看一看http请求背后的细节。 2. 基本了解 请求和响应的数据内容 请求行、 请求头 、请求体 响应行、响应头、响应体 3. Get和Post的区别 Servlet【重点】 1. 会使用简单的servlet 1.写一个类，实现接口Servlet 2. 配置Servlet 3. 会访问Setvlet 2. Servlet的生命周期 init 一次 创建对象 默认初次访问就会调用或者可以通过配置，让它提前 load-on-startup service 多次，一次请求对应一次service destory 一次 销毁的时候 从服务器移除 或者 正常关闭服务器 3. ServletConfig 获取配置的信息， params]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
        <tag>Xml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WEB组件数据交互]]></title>
    <url>%2F2019%2F04%2F23%2F1_WEB%E7%BB%84%E4%BB%B6%E6%95%B0%E6%8D%AE%E4%BA%A4%E4%BA%92%2F</url>
    <content type="text"><![CDATA[WEB组件数据交互 页面之间要实现跳转 页面之间要实现数据的共享 使用的工具：servlet和jsp 一. JSP的出现 JSP动态网页=Java代码+HTML代码（说白了，在HTML中可以随意的编写Java代码） 二. 重定向与转发的区别转发: 浏览器的地址不会发生改变，比如：地址栏中/login,最终依然为/login 共享同一个请求中的数据 最终地址栏中的地址由谁决定？由最后的servlet中是重定向还是转发决定，如果转发的是jsp页面，那么地址栏中不会有变化，如果是重定向到jsp页面，那么地址栏中有变化。如果最后转发的不是jsp页面，而是一个请求，那么继续循环。 可以访问WEB-INF目录中的资源。 请求转发不能跨域访问，只能跳转到当前应用中的资源 URL重定向: 浏览器的地址会发生改变 重定向相当于发送了两次请求 就决定了请求中的数据不能共享 最终响应给浏览器的地址由该请求中的地址决定 能够支持跨域访问，说白了可以访问其他应用中的资源 不能访问WEB-INF目录中的资源 什么时候用什么？: 如果请求中需要共享数据，那就来一个转发 如果需要跨域访问，那就来一个重定向 如果需要访问WEB-INF下的资源，那就来一个转发 其他情况下随便选择 三. Servlet的三大作用域对象 作用：共享数据 request每一次请求都是一个新的request对象，如果想共享request数据，那么就使用转发，因为request对象不会重新产生session每一次会话都是一个新的session对象，一个会话可以有多次请求，这中间是可以共享数据的application应用对象，服务器软甲启动到关闭，Tomcat启动到关闭，表示一个应用，在一个应用中有且只有一个application对象，作用于整个WEB应用，可以实现数据共享。 对象名称 对象的类型 如何获取 request HttpServletRequest 直接通过参数得到 session HttpSession getSession() application ServletContext getServletContext() 作用域如何共享数据？: 设置作用域中的共享数据（包含了修改）作用域对象.setAttribute(String arrtName,Object value); 获取作用域中的共享数据Object value = 作用域对象.getAttribute(String arrtName); 删除作用域中的共享数据作用域对象.removeAttribute(String arrtName) 四. Jsp的四大作用域对象page当前jsp页面request: session: application: 对象名称 对象的类型 描述 page PageContext 当前的JSP页面 request HTTPServletRequest 当前的请求作用域对象 session HttpSession 当前会话的作用域 application ServletContext 整个应用的作用域 五.JSP的九大内置对象 内置对象的名称 类型 说明 request HttpServletRequest 请求对象 session HttpSession 当前会话对象 application ServletContext 应用对象 page Object 当前的Servlet对象 pageContext PageContext 当前JSP作用域对象 config ServletConfig 当前Servlet的配置信息对象 out JSPWriter 字符输出流对象 response HttpServletResponse 响应对象 exception Throwable 异常类型 六. JSP指令page常用的有language,import,pageEncoding,errorPage 作用：定义JSP页面的各种属性 属性：language:指示JSP页面中使用脚本语言。默认值java，目前只支持java。 extends：指示JSP对应的Servlet类的父类。不要修改。 import：导入JSP中的Java脚本使用到的类或包。（如同Java中的import语句） JSP引擎自动导入以下包中的类： javax.servlet.* javax.servlet.http.* javax.servlet.jsp.* **注意：**一个import属性可以导入多个包，用逗号分隔。 sessioin:指示JSP页面是否创建HttpSession对象。默认值是true，创建 buffer：指示JSP用的输出流的缓存大小.默认值是8Kb。 autoFlush：自动刷新输出流的缓存。 isThreadSafe：指示页面是否是线程安全的（过时的）。默认是true。 true：不安全的。 false：安全的。指示JSP对应的Servlet实现SingleThreadModel接口。 errorPage:指示当前页面出错后转向（转发）的页面。目标页面如果以”/“（当前应用）就是绝对路径。 include包含指令 静态包含：翻译成servlet的时候就已经将jsp页面进行合并 动态包含：是将每一个jsp全部翻译成Servlet类之后，在运行阶段，动态的合并到一起 什么时候怎么用 如果包含的时候，需要传递新的数据，这个时候只能使用动态包含。taglibTaglib指令引入一个自定义标签集合的定义，包括库路径、自定义标签七. 标签库 清除JSP中的Java代码 在WEB项目引入两个jar包 将JAR放到/WEB-INF/lib目录 在JSP页面引入标签库 &lt;%@taglib uri=”http://java.sun.com/jsp/jstl/core&quot; prefix=”c” %&gt; 简单的使用123456789101112131415161718192021222324252627282930313233343536&lt;!--判断--&gt;&lt;c:if test="判断条件" var="变量名" scope="page|request|session|application"&gt;&lt;/c:if&gt;&lt;!--多重判断--&gt;&lt;c:choose&gt; &lt;c:when test="$&#123;fenshu&gt;90 &#125;"&gt; 优秀 &lt;/c:when&gt; &lt;c:when test="$&#123;fenshu&gt;70 &#125;"&gt; 良好 &lt;/c:when&gt; &lt;c:when test="$&#123;fenshu&gt;60 &#125;"&gt; 及格 &lt;/c:when&gt; &lt;c:otherwise&gt; 不及格 &lt;/c:otherwise&gt;&lt;/c:choose&gt;&lt;!--循环--&gt;&lt;c:forEach items="$&#123;userList &#125;" var="user"&gt; &lt;tr&gt; &lt;td&gt;$&#123;user.id &#125;&lt;/td&gt; &lt;td&gt;$&#123;user.getName() &#125;&lt;/td&gt; &lt;td&gt;$&#123;user.number &#125;&lt;/td&gt; &lt;td&gt;$&#123;user.sex &#125;&lt;/td&gt; &lt;td&gt;$&#123;user.age &#125;&lt;/td&gt; &lt;td&gt;$&#123;user.school &#125;&lt;/td&gt; &lt;td&gt;$&#123;user.major &#125;&lt;/td&gt; &lt;td&gt;&lt;a href="delete?id=$&#123;user.id &#125;"&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/c:forEach&gt;&lt;c:forEach begin="1" end="10" var="i" step="2"&gt; $&#123;i &#125;&lt;br&gt;&lt;/c:forEach&gt; 八. EL表达式语言]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>WEB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2019%2F04%2F23%2FkAFKA%2F</url>
    <content type="text"><![CDATA[kAFKAKafka概述Kafka是什么在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。 Apache Kafka是一个开源消息系统，由Scala和Java写成。是由Apache软件基金会开发的一个开源消息系统项目。 Kafka最初是由LinkedIn公司开发，并于 2011 年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。 Kafka是一个分布式消息队列。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。 消息队列内部实现原理 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。 发布/订阅模式（一对多，数据生产后，推送给所有订阅者）发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。 为什么需要消息队列: 解耦允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。 灵活性 &amp; 峰值处理能力在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 可恢复性系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性） 缓冲有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 异步通信很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 Kafka 架构: Producer ：消息生产者，就是向kafka broker发消息的客户端。 Consumer ：消息消费者，向kafka broker取消息的客户端 Topic ：可以理解为一个队列。 Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制-给consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。 Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。 Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。 Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka Kafka集群部署下载下载地址CDH下载地址安装123456789101112131415161718192021222324## 修改server.properties文件[root@master config]# vim server.properties# 每一台broker必须有唯一的一个idbroker.id=0# 处理网络请求的时候的线程数num.network.threads=3log.dirs=/opt/apps/Kafka/logszookeeper.connect=master:2181,slave1:2181,slave2:2181,slave3:2181,slave4:2181[root@master config]# vim producer.propertiesbootstrap.servers=master:9092,slave1:9092,slave2:9092,slave3:9092,slave4:9092[root@master config]# vim consumer.propertiesbootstrap.servers=master:9092,slave1:9092,slave2:9092,slave3:9092,slave4:9092## 环境变量[root@master config]# vim /etc/profile#Kafka环境变量export KAFKA_HOME=/opt/apps/Kafka/kafka_2.11_2.0.0export PATH=$PATH:$KAFKA_HOME/bin## 启动服务kafka-server-start.sh /opt/apps/Kafka/kafka_2.11_2.0.0/config/server.properties &amp;CDH安装: 简单操作12345678910111213141516171819202122232425## 创建主题[root@master ~]# kafka-topics.sh --zookeeper master:2181 --create --replication-factor 3 --partitions 1 --topic stormKafka## 生产数据（消息）kafka-console-producer.sh \ --broker-list slave1:9092 \ --topic bigdata## 消费数据（消息）kafka-console-consumer.sh \--bootstrap-server slave4:9092 \--consumer.config /opt/apps/_2.0.0/config/consumer.properties \--from-beginning \--topic bigdata## 删除主题[root@master logs]# kafka-topics.sh --zookeeper master:2181 --delete --topic bigdata## delete.topic.enable=true才可以删除主题## 查看主题详情[root@master ~]# kafka-topics.sh --zookeeper master:2181 --describe --topic bigdataKafka生产过程分析写入方式producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。分区（Partition）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：下图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区。 分区的原因 方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了 可以提高并发，因为可以以Partition为单位读写了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。 分区的原则 指定了patition，则直接使用 未指定patition但指定key，通过对key的value进行hash出一个patition patition和key都未指定，使用轮询选出一个patition。12345678910111213141516171819 //DefaultPartitioner类public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; &#125; 副本（Replication）同一个partition可能会有多个replication（对应 server.properties 配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。写入流程producer写入消息流程如下： producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader producer将消息发送给该leader leader将消息写入本地log followers从leader pull消息，写入本地log后向leader发送ACK leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK Broker 保存消息: 存储方式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件） 存储策略无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据： 基于时间：log.retention.hours=168 基于大小：log.retention.bytes=1073741824需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。 Zookeeper存储结构/controller_epoch就是一个数字，第一次为1，后续的时候只要中央控制器发生变更（选举出来的），这个值就会+1 /controller存储的是中央控制器所在的broker的信息{“version”:1,”brokerid”:60,”timestamp”:”1542495029190”} /consumers/console-consumer-95629/ids/console-consumer-95629_node03-1542495483815-afd89b49/主节点/消费者组的ID/ids/消费者的ID{“version”:1,”subscription”:{“hehe”:1},”pattern”:”white_list”,”timestamp”:”1542495483878”}{版本，订阅的topic列表{tiopic名称:消费者中topic消费者的线程数}} Kafka消费过程分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kafka提供了两套consumer API：高级Consumer API和低级API。 消费模型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费。高级API: 高级API优点 高级API 写起来简单 不需要自行去管理offset，系统通过zookeeper自行管理。 不需要管理分区，副本等情况，.系统自动管理。 消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset） 可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响） 高级API缺点 不能自行控制offset（对于某些特殊需求来说） 不能细化控制如分区、副本、zk等 低级API: 低级 API 优点 能够让开发者自己控制offset，想从哪里读取就从哪里读取。 自行控制连接分区，对分区自定义进行负载均衡 对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中） 低级API缺点太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等。 消费者组消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。消费方式push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。代码案例1 Kafka producer拦截器(interceptor)拦截器原理Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括： configure(configs)获取配置信息和初始化数据时调用。 onSend(ProducerRecord)：该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算 onAcknowledgement(RecordMetadata, Exception)：该方法会在消息被应答或消息发送失败时调用，并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率 close：关闭interceptor，主要用于执行一些资源清理工作如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。 kafka StreamsKafka Streams。Apache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序。 Kafka Streams特点: 功能强大高扩展性，弹性，容错 轻量级无需专门的集群一个库，而不是框架 完全集成100%的Kafka 0.10.0版本兼容易于集成到现有的应用程序 实时性毫秒级延迟并非微批处理窗口允许乱序数据允许迟到数据 为什么要有Kafka Stream当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易。既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？主要有如下原因。 Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。 虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求。 就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低。 使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。但是Kafka作为类库不占用系统资源。 由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力。 由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度。 总结Kafka是一个分布式消息系统，每个记录包含key+value+timestapproducer：消息的生产者consumer：消息的消费者consumer group：消费者组bootstrap-server：集群服务器topic：主题partition：分区replication-：副本数zookeeper：Hadoop高可用（HDFS ha，RM ha），Hbase，Kafka，Stormkafka数据丢失问题:]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据项目]]></title>
    <url>%2F2019%2F04%2F23%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[项目名称:paas平台组件运维监控/分布式平台监控预警系统开发时间：6个月左右项目架构：jmx+kafka+sparkstreaming+springboot+mysql+javamail+sms+echart项目简介：自动化监控预警paas平台。提高了管理集群的效率项目职责：1.通过jmx采集hadoop集群指标数据 2.数据以json形式缓存进kafka 3.sparkstreaming消费数据做预警，调用短信接口或者javamail通知管理员，结果写入数据库 4.springboot消费kafka做数据的实时展示 5.项目的部署，bug的修复项目难点或遇到的问题：1.kafka性能问题 解决方案：通过设置snappy压缩，增大批处理量 2.sparkstreaming的更新部署问题 解决方案：设置与老sparkstreaming同一消费者组，启动新程序，然后关闭老程序 3.sparkstreaming保存kafka偏移量 解决方案：默认设置自动保存偏移量，在程序出现错误时，可能会出现少部分的 数据丢失，这是可以容忍的。如果想要不丢失，需要手动将offset保存到数据库等 第三方存储中。项目名称：净网开发时间：6个月左右项目架构：httpclient+hadoop+spark+mysql+sparkmllib项目简介：自动识别赌博，暴力等非法网站，为政府提供网络管理的技术支持。项目职责：1.利用spark开发分布式爬虫爬取各域名的主页 2.将爬取的主页html代码存入hdfs，同时使用spark清洗去除js,css，html标签代码 3.利用lucene分词工具包，处理html文本。主要包括转码，去除标点，stopword,分词等。 4.在sparkstreaming中利用naive bayes算法实现文本的分类。判别网站是否非法项目难点或遇到的问题：1.html网页的转码问题 解决方案：实现自动识别html代码的编码功能。将所有网页统一转化成utf8 2.文本分类问题 解决方案：监督学习算法需要有训练数据，因此应该先手动标记一部分数据，然后通过sparkmllib识别]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据精选知识点]]></title>
    <url>%2F2019%2F04%2F23%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B2%BE%E9%80%89%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[简介本文介绍大数据入门的一些面试知识点，基础知识点，在此不再过多说明 hadoophadoop基础通常是集群的最主要瓶颈: 磁盘 IO 原因：磁盘io导致MapReduce 集群的管理hadoop生态圈的组件并做简要描述如何安装配置apache的一个开源HadoopHadoop中需要哪些配置文件，其作用是什么 core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml请列出正常工作的Hadoop集群中Hadoop都分别需要启动哪些进程，它们的作用分别是什么?Hadoop的几个默认端口及其含义 HDFS（☆☆☆）HDFS 默认 BlockSize 是多少Client 端上传文件流程JobTracker通常与 NameNode 在一个节点启动增加文件块大小，需要增加磁盘的传输速率。减少元数据在内存的占用HDFS的存储机制（☆☆☆☆☆）secondary namenode工作机制（☆☆☆☆☆）NameNode与SecondaryNameNode 的区别与联系？（☆☆☆☆☆）HAnamenode 是如何工作的? （☆☆☆☆☆）hadoop2.x Federation（联邦）MapReduce （☆☆☆☆☆）两个类TextInputFormat和KeyValueInputFormat的区别是什么？请描述mapReduce中shuffle阶段的工作流程，如何优化shuffle阶段（☆☆☆☆☆）请描述mapReduce中combiner的作用是什么，一般使用情景，哪些情况不需要，及和reduce的区别？使 Hadoop 任务输出到多个目录中,怎么做？什么样的计算不能用mr来提速ETL是什么MapReduce实际操作Yarn（☆☆☆☆）hadoop1与hadoop2 的架构异同yarn,它解决了什么问题，有什么优势HDFS的数据压缩算法? （☆☆☆☆☆）Hadoop的调度器总结（☆☆☆☆☆）MapReduce 2.0 容错性hadoop安全，及资源管理方案介绍（☆☆☆☆☆）优化mapreduce 跑的慢的原因（☆☆☆☆☆）Mapreduce 程序效率的瓶颈在于两点： HDFS小文件优化方法（☆☆☆☆☆）企业运维常出现的问题Hadoop会有哪些重大故障，如何应对？至少给出 5个如何测压hadoop集群你们公司业务数据量多大？有多少行数据？你们的数据是用什么导入到数据库的？导入到什么数据库？你们处理数据是直接读数据库的数据还是读文本数据你在项目中遇到了哪些难题，是怎么解决的列举你了解的海量数据的处理方法及适用范围，如果有相关使用经验，可简要说明。腾讯面试题：给40亿个不重复的 unsigned int 的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？怎么在海量数据中找出重复次数最多的一个？上千万或上亿数据（有重复），统计其中出现次数最多的前 N 个数据一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析Zookeeper简述ZooKeeper的选举机制Zookeeper 小知识点客户端如何正确处理CONNECTIONLOSS(连接断开) 和 SESSIONEXPIRED(Session 过期)两类连接异常？一个客户端修改了某个节点的数据，其他客户端能够马上获取到这个最新数据吗？能否收到每次节点变化的通知谈谈你对ZooKeeper的理解ZooKeeper的通知机制HiveHive表关联查询如何解决数据倾斜的问题？（☆☆☆☆☆）Hive如何进行权限控制？（☆☆☆☆☆）Hive 中的压缩格式TextFile、SequenceFile（行式存储）、RCfile 、ORCfile（列式存储）各有什么区别对Hive桶表的理解Hive优化措施（☆☆☆☆☆）（处理数据倾斜）本地模式（本地运行，不需要打包上传）表的优化小表、大表Join3.27.3.3 MapJoinCount(Distinct) 去重统计（准确的）数据倾斜Map数小文件进行合并复杂文件增加Map数Reduce数并行执行EXPLAIN（执行计划）（查看执行语句是否效率，是否命中索引）HbaseHBase的特点是什么HBase和Hive的区别？描述HBase的rowKey的设计原则？（☆☆☆☆☆）描述HBase中scan和get的功能以及实现的异同？请描述HBase中scan对象的setCache和setBatch方法的使用？（☆☆☆☆☆）以start-hbase.sh为起点，HBase启动的流程是什么简述HBase中compact用途是什么，什么时候触发，分为哪两种，有什么区别，有哪些相关配置参数？（☆☆☆☆☆）** 每天百亿数据存入HBase，如何保证数据的存储正确和在规定的时间里全部录入完毕，不残留数据？（☆☆☆☆☆）请列举几个HBase优化方法？（☆☆☆☆☆）HRegionServer宕机如何处理？（☆☆☆☆☆）HBase读写流程？（☆☆☆☆☆）HTable API有没有线程安全问题，在程序中是单例还是多例企业问题HBase有没有并发问题？（企业）你们的HBase大概在公司业务中（主要是网上商城）大概都几个表？几个表簇？都存什么样的数据？Hbase中的memstore是用来做什么的** HBase在进行模型设计时重点在什么地方？一张表中定义多少个Column Family最合适？为什么？（☆☆☆☆☆）如何提高HBase客户端的读写性能？请举例说明（☆☆☆☆☆）直接将时间戳作为行健，在写入单个region 时候会发生热点问题，为什么呢？（☆☆☆☆☆）解释一下布隆过滤器原理（☆☆☆☆☆）Flume（日志采集工具）Flume与Kafka的选取lume管道内存，flume宕机了数据丢失怎么解决flume配置方式，flume集群flume有哪些组件，flume的source、channel、sink具体是做什么的kafka（kafka可以定期或者定量自动删除日志，在配置文件中配置后，定期或者定量删除之前的日志） 请说明什么是Apache Kafka请说明Kafka相对于传统的消息传递方法有什么优势在Kafka中broker的意义是什么Kafka中的ZooKeeper是什么？Kafka是否可以脱离ZooKeeper独立运行？解释如何提高远程用户的吞吐量（企业）解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息？** 解释如何减少ISR中的扰动？broker什么时候离开ISR？（☆☆☆☆☆）请说明Kafka 的消息投递保证（delivery guarantee）机制以及如何实现？（☆☆☆☆☆）如何保证Kafka的消息有序（☆☆☆☆☆）kafka数据丢失问题,及如何保证SparkSpark master使用zookeeper进行HA的，有哪些元数据保存在Zookeeper？Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么？如何配置spark master的HA？driver的功能是什么？（提交任务的）** spark的有几种部署模式，每种模式特点？（☆☆☆☆☆）Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？Spark中Worker的主要工作是什么Spark为什么比mapreduce快？（☆☆☆☆☆）简单说一下hadoop和spark的shuffle相同和差异？（☆☆☆☆☆）（spark的shuffle 把key进行规约合并，没有mapreduce的shuffle复杂） RDD机制spark有哪些组件spark工作机制？ （☆☆☆☆☆）spark的优化怎么做？ （☆☆☆☆☆）** spark工作机制？ （☆☆☆☆☆）什么是RDD宽依赖和窄依赖？spark-submit的时候如何引入外部jar包cache和pesist的区别数据本地性是在哪个环节确定的？（☆☆☆☆☆）（数据本地性：不需要传送数据，计算节点和数据节点为一个节点，不需要移动数据，只需要在本地参与计算即可） RDD的弹性表现在哪几点？（☆☆☆☆☆）(弹性就是容错)RDD有哪些缺陷？（☆☆☆☆☆）（rdd不可变）Spark程序编写的一般步骤Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？你所理解的Spark的shuffle过程？（☆☆☆☆☆）（http://www.cnblogs.com/jxhd1/p/6528540.html） 你如何从Kafka中获取数据对于Spark中的数据倾斜问题你有什么好的方案？（☆☆☆☆☆）RDD创建有哪几种方式Spark并行度（同时执行的task）怎么设置比较合适Spark中数据的位置是被谁管理的在rdd有几种操作类型Spark程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？** map与flatMap的区别** 为什么要进行序列化介绍一下join操作优化经验？（☆☆☆☆☆）(数据倾斜)** 描述Yarn执行一个任务的过程？（☆☆☆☆☆）Yarn中的container是由谁负责销毁的，在Hadoop Mapreduce中container可以复用么？提交任务时，如何指定Spark on Yarn的运行模式不启动Spark集群Master和work服务，可不可以运行Spark程序？,可以的话如何运行spark on yarn Cluster 模式下，ApplicationMaster和driver是在同一个进程么？Spark on Yarn 模式有哪些优点？（☆☆☆☆☆）Spark on Yarn架构是怎么样的？（要会画哦，这个图）为什么会产生yarn，解决了什么问题，有什么优势http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=6785 Mapreduce的执行过程http://www.cnblogs.com/hipercomer/p/4516581.html Spark使用parquet(列式存储)文件存储格式能带来哪些好处？（☆☆☆☆☆）列式存储的好处呗介绍parition和block有什么关联关系？（☆☆☆☆☆）Spark应用程序的执行过程是什么？（☆☆☆☆☆）什么是shuffle，以及为什么需要shuffle(Spark)不需要排序的hash shuffle是否一定比需要排序的sort shuffle速度快？（☆☆☆☆☆）Spark中的HashShufle的有哪些不足？列举你了解的序列化方法，并谈谈序列化有什么好处？常见的数压缩方式，你们生产集群采用了什么压缩方式，提升了多少效率？简要描述Spark写数据的流程？（☆☆☆☆☆）spark怎么整合hiveJVM*** jvm结构？堆里面几个区？算法Linux]]></content>
      <categories>
        <category>面试题库</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>大数据</tag>
        <tag>算法</tag>
        <tag>Hbase</tag>
        <tag>hadoop</tag>
        <tag>Flume</tag>
        <tag>Linux</tag>
        <tag>Zookeeper</tag>
        <tag>Hive</tag>
        <tag>kafka</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bayes算法]]></title>
    <url>%2F2019%2F04%2F22%2Fbayes%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[贝叶斯定理 每次提到贝叶斯定理，我心中的崇敬之情都油然而生，倒不是因为这个定理多高深，而是因为它特别有用。这个定理解决了现实生活里经常遇到的问题：已知某条件概率，如何得到两个事件交换后的概率，也就是在已知P(A|B)的情况下如何求得P(B|A)。这里先解释什么是条件概率： 表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为： 贝叶斯定理之所以有用，是因为我们在生活中经常遇到这种情况：我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。下面不加证明地直接给出贝叶斯定理： 朴素贝叶斯分类 朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BitMap算法]]></title>
    <url>%2F2019%2F04%2F22%2FBitMap%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[BitMap算法1）可进行数据的快速查找，判重，删除，一般来说数据范围是int的10倍以下。2）去重数据而达到压缩数据3) 用于爬虫系统中url去重、解决全组合问题。 BitMap应用：排序示例假设我们要对0-7内的5个元素(4,7,2,5,3)排序（这里假设这些元素没有重复）。那么我们就可以采用Bit-map的方法来达到排序的目的。要表示8个数，我们就只需要8个Bit（1Bytes），首先我们开辟1Byte的空间，将这些空间的所有Bit位都置为0(如下图：) 然后遍历这5个元素，首先第一个元素是4，那么就把4对应的位置为1（可以这样操作 p+(i/8)|(0×01&lt;&lt;(i%8)) 当然了这里的操作涉及到Big-ending和Little-ending的情况，这里默认为Big-ending。不过计算机一般是小端存储的，如intel。小端的话就是将倒数第5位置1）,因为是从零开始的，所以要把第五位置为一（如下图）：然后再处理第二个元素7，将第八位置为1,，接着再处理第三个元素，一直到最后处理完所有的元素，将相应的位置为1，这时候的内存的Bit位的状态如下： 然后我们现在遍历一遍Bit区域，将该位是一的位的编号输出（2，3，4，5，7），这样就达到了排序的目的。 bitmap排序复杂度分析Bitmap排序需要的时间复杂度和空间复杂度依赖于数据中最大的数字。 bitmap排序的时间复杂度不是O(N)的，而是取决于待排序数组中的最大值MAX，在实际应用上关系也不大，比如我开10个线程去读byte数组，那么复杂度为:O(Max/10)。也就是要是读取的，可以用多线程的方式去读取。时间复杂度方面也是O(Max/n)，其中Max为byte[]数组的大小，n为线程大小。 假设最大数值为11，一个byte是占8个bit的 因此需要2个bytes 16bit 假设整数有：1.2.5.7.11当传入一个值来判断是否存在于这个整数向量内时 12345678910public void add(int num)&#123; // num/8得到byte[]的index int arrayIndex =num/8= num &gt;&gt; 3; // num%8得到在byte[index]的位置 int position =num%8= num &amp; 0x07; //将1左移position后，那个位置自然就是1，然后和以前的数据做|，这样，那个位置就替换成1了。 bits[arrayIndex] |= 1 &lt;&lt; position; &#125; 通过]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>BitMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bloom filter-布隆过滤器]]></title>
    <url>%2F2019%2F04%2F22%2Fbloom-filter-%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Bloom filter 布隆过滤器算法： 首先需要k个hash函数，每个函数可以把key散列成为1个整数 初始化时，需要一个长度为n比特的数组，每个比特位初始化为0 某个key加入集合时，用k个hash函数计算出k个散列值，并把数组中对应的比特位置为1 判断某个key是否在集合时，用k个hash函数计算出k个散列值，并查询数组中对应的比特位，如果所有的比特位都是1，认为在集合中。 优点：不需要存储key，节省空间 缺点： 算法判断key在集合中时，有一定的概率key其实不在集合中 无法删除 典型的应用场景：某些存储系统的设计中，会存在空查询缺陷：当查询一个不存在的key时，需要访问慢设备，导致效率低下。比如一个前端页面的缓存系统，可能这样设计：先查询某个页面在本地是否存在，如果存在就直接返回，如果不存在，就从后端获取。但是当频繁从缓存系统查询一个页面时，缓存系统将会频繁请求后端，把压力导入后端。 具体应用实例布隆过滤器是一个 bit 向量或者说 bit 数组，长这样：映射一个值到布隆过滤器中，我们需要使用多个不同的哈希函数生成多个哈希值，并对每个生成的哈希值指向的 bit 位置 1，例如针对值 “baidu” 和三个不同的哈希函数分别生成了哈希值 1、4、7，则上图转变为：结果：1 如果有一个获取的hash值为0的话那就代表该向量中没有这个数据。但如果都为1的话，只能代表该bit向量中可能含有该数据，但由于hash函数后的函数值是重复迭代在向量中为1的，因此不一定含有该数据。 bitmap 位图根据数据数量来定义向量位数，初始值都为0，进行遍历，如果有当前位数的话，在其位置标记为1， 优点缺点]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>过滤器</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop 部分知识点]]></title>
    <url>%2F2019%2F04%2F22%2Fhadoop-%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[hadoop 分为三种商业版 CDH cloudera hadoop–Impala相当于hive HDP Hortonworks版本开源版 hadoopblock是物理的，真正存储的位置在本地磁盘{hadoop.tmp.dir}/dfs/data，是本地存储，是块文件，不是我们所谓的HDFS分布式文件系统。 Zookeeper 作为集群的高可用Hadoop的运行模式 单机版、伪分布式、完全分布式。Zookeeper是一个文件系统 Solr集群 全文索引Oozie 定时调度， azkcaban定时调度，Java可以做 linux也可以做 mapreduce 基于磁盘 spark 基于内存]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot入门学习]]></title>
    <url>%2F2019%2F04%2F19%2FSpringboot%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Springboot入门介绍Spring 框架对于很多 Java 开发人员来说都不陌生。自从 2002 年发布以来，Spring 框架已经成为企业应用开发领域非常流行的基础框架。有大量的企业应用基于 Spring 框架来开发。Spring 框架包含几十个不同的子项目，涵盖应用开发的不同方面。如此多的子项目和组件，一方面方便了开发人员的使用，另外一个方面也带来了使用方面的问题。每个子项目都有一定的学习曲线。开发人员需要了解这些子项目和组件的具体细节，才能知道如何把这些子项目整合起来形成一个完整的解决方案。在如何使用这些组件上，并没有相关的最佳实践提供指导。对于新接触 Spring 框架的开发人员来说，并不知道如何更好的使用这些组件。Spring 框架的另外一个常见问题是要快速创建一个可以运行的应用比较麻烦。Spring Boot 是 Spring 框架的一个新的子项目，用于创建 Spring 4.0 项目。它的开发始于 2013 年。2014 年 4 月发布 1.0.0 版本。它可以自动配置 Spring 的各种组件，并不依赖代码生成和 XML 配置文件。Spring Boot 也提供了对于常见场景的推荐组件配置。Spring Boot 可以大大提升使用 Spring 框架时的开发效率 简介：使用Spring boot ，可以轻松的创建独立运行的程序，非常容易构建独立的服务组件，是实现分布式架构、微服务架构利器。Spring boot简化了第三方包的引用，通过提供的starter，简化了依赖包的配置 Spring boot的优点轻松创建独立的Spring应用程序。内嵌Tomcat、jetty等web容器，不需要部署WAR文件。提供一系列的“starter” 来简化的Maven配置，不需要添加很多依赖。开箱即用，尽可能自动配置Spring。 Springboot依赖说明 依赖包 解释说明 spring-boot-starter 核心 POM，包含自动配置支持、日志库和对 YAML 配置文件的支持。 spring-boot-starter-amqp 通过 spring-rabbit 支持 AMQP spring-boot-starter-aop 包含 spring-aop 和 AspectJ 来支持面向切面编程（AOP）。 spring-boot-starter-batch 支持 Spring Batch，包含 HSQLDB。 spring-boot-starter-data-jpa 包含 spring-data-jpa、spring-orm 和 Hibernate 来支持 JPA。 spring-boot-starter-data-mongodb 包含 spring-data-mongodb 来支持 。 spring-boot-starter-data-rest 通过 spring-data-rest-webmvc 支持以 REST 方式暴露 Spring Data 仓库。 spring-boot-starter-jdbc 支持使用 JDBC 访问数据库 spring-boot-starter-security 包含 spring-security。 spring-boot-starter-test 包含常用的测试所需的依赖，如 JUnit、Hamcrest、Mockito 和 spring-test 等。 spring-boot-starter-velocity 支持使用 Velocity 作为模板引擎。 spring-boot-starter-web 支持 Web 应用开发，包含 Tomcat 和 spring-mvc。 spring-boot-starter-websocket 支持使用 Tomcat 开发 WebSocket 应用。 spring-boot-starter-ws 支持 Spring Web Services spring-boot-starter-actuator 添加适用于生产环境的功能，如性能指标和监测等功能。 spring-boot-starter-remote-shell 添加远程 SSH 支持 spring-boot-starter-jetty 使用 Jetty 而不是默认的 Tomcat 作为应用服务器。 spring-boot-starter-log4j 添加 Log4j 的支持 spring-boot-starter-logging 使用 Spring Boot 默认的日志框架 Logback spring-boot-starter-tomcat 使用 Spring Boot 默认的 Tomcat 作为应用服务器。 注:tomcat和jetty用其一就可以了 Springboot 启动方式第一种123456789@EnableAutoConfiguration//路径可以扩大至com,zhiyou.web 相当于web包下的都会被启动@ComponentScan(basePackages=&#123;"com.zhiyou.web.controller",""&#125;)publicq class App&#123;public static void main(String[] args)&#123;//启动springboot项目SpringApplication.run(UserController.class,args);&#125;&#125; 第二种方式【常用】maven项目里的springboot定义main类 1234567@EnableAutoConfiguration@ComponenetScan(basePackages="com.zhiyou100.controller")//如果需要加多个的话="&#123;"",""&#125;"public class App&#123; public static void main(String[]args)&#123; //启动springboot项目 SpringApplication.run(App.class,args);&#125; 常用注解123456789101112@RestCibtriller //申明Rest风格的控制器—不仅有Controller功能 把参数写到链接里 而不是？问号后面了@EnableAutoConfiguration//自动配置Spring配置文件 (在配置完main类之后就可以不用写该注解了)public class 类名&#123;@RequestMapping("hello/&#123;name&#125;") //映射路径@ResponseBody //把下方return的输出转换成json格式输出到前端(返回Json数据) //@PathVariable("name")用户获取映射路径所传参数，格式 参数名称和映射路径的参数名称相同的话@PathVariable()可以不填内容 public String hello(@PathVariable("name") String name)&#123; return name+“hello,Spring boot"; &#125;&#125;//写完之后 输出localhost:8080/hello/zhangsan 就会出现 zhangsan hello.Spring boot 全局捕获异常@ExceptionHandler 表示拦截异常@ControllerAdvicecontroller 的一个辅助类，最常用的就是作为全局异常处理的切面类可以指定扫描范围约定了几种可行的返回值，如果是直接返回 model 类的话，需要使用 @ResponseBody 进行 json 转换 在com.gyf.web.exception包中定义一个全局异常类 1234567891011@ControllerAdvice//控制器切面public class GlobalExceptionHandler &#123; @ExceptionHandler(RuntimeException.class)//捕获运行时异常 @ResponseBody//返回json public Map&lt;String,Object&gt; exceptionHander()&#123;//处理异常方法 Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put("errorCode", "101"); map.put("errorMsg", "系統错误!"); return map; &#125;&#125; 在启动spring中，配置扫描包为com.gyf.web在某个映射的方法中添加个int i = 10/0的算术异常访问上的个路径结果为 传参方方式后端向前端传值 加判断-列表排序建立一个控制器 12345678910111213141516171819202122/*** @RestController 用于写ApI, 给移动客户端提供数据，一般返回json数据* @Controller一般用于写后台）（）* 本demo 介绍如何后端向前端传值*/@Controller@RequestMapping("stu")public class StudentController &#123;@RequestMapping("list")//Model 是springmvc的传数据至前端的，只要在方法的参数里写了这个参数对象，前端就可以获取该Model参数对象的值，//springBoot可以直接使用map 也可以达到Model的效果，并且 springboot也可以使用Modelpublic String list(Model model)&#123;model.addAttribute("username","zz");model.addAttribute("age","22");ArrayList&lt;Student&gt; students = new ArrayList&lt;&gt;();students.add(new Student(110,"张峥","男"));students.add(new Student(110,"lisi","男"));students.add(new Student(110,"wangwu","男"));model.addAttribute("stuList",students);return "stu/list";&#125;&#125; 对应页面 freemark模板 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;欢迎$&#123;username&#125;&lt;#--可以对model数据进行判断--&gt;&lt;#if (age&lt; 18)&gt;-小朋友&lt;#elseif (age&gt;30)&gt;-大叔&lt;#else&gt;帅哥美女&lt;/#if&gt;&lt;table border="1"&gt;&lt;tr&gt;&lt;td&gt;ID&lt;/td&gt;&lt;td&gt;名字&lt;/td&gt;&lt;td&gt;性别&lt;/td&gt;&lt;/tr&gt;&lt;#--？reverse倒叙--&gt;&lt;#--?sort_by排序--&gt;&lt;#list stuList?sort_by("id")?reverse as stu &gt;&lt;tr&gt;&lt;td&gt; $&#123;stu.id&#125;&lt;/td&gt;&lt;td&gt; $&#123;stu.username&#125;&lt;/td&gt;&lt;td&gt; $&#123;stu.gender&#125;&lt;/td&gt;&lt;/tr&gt;&lt;/#list&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 前端通过url向后端传值123456789101112@RestController@RequestMapping("user")public class UserController &#123;@ResponseBody()@RequestMapping("&#123;id&#125;")//@PathVariable("name")用户获取映射路径所传参数，格式 参数名称和映射路径的参数名称相同的话@PathVariable()内可以不填内容public User uerInfo(@PathVariable("id") Integer id1)&#123;User user = new User( "admin", "123");user.setId(id1);return user;&#125;&#125; 数据库配置文件 12345#数据库配置spring.datasource.url=jdbc:mysql://localhost:3306/sys?serverTimezone=UTC&amp;useSSL=falsespring.datasource.username=rootspring.datasource.password=123456spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver]]></content>
      <categories>
        <category>Java框架</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>Springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自定义远程目录或文件同步脚本xsync.sh]]></title>
    <url>%2F2019%2F04%2F15%2F1_%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BF%9C%E7%A8%8B%E7%9B%AE%E5%BD%95%E6%88%96%E6%96%87%E4%BB%B6%E5%90%8C%E6%AD%A5%E8%84%9A%E6%9C%ACxsync.sh%2F</url>
    <content type="text"><![CDATA[自定义远程目录或文件同步脚本xsync.sh 1234567891011121314151617181920212223242526#!/bin/bash#获取输入参数个数，如果没有参数，直接退出pcount=$# if(( pcount&lt;1 )); then echo no args; exit; fi #获取文件名称 p1=$1; fname=`basename $p1`; #echo fname=$fname; #获取上级目录的绝对路径 pdir=`cd -P $(dirname $p1) ; pwd` #echo pdir=$pdir ; #获取当前用户名称 cuser=`whoami` #循环 for((host=130;host&lt;134;host=host+1)) ; do echo ------------------ yla$host ---------------------- #echo $pdir/$fname $cuser@yla$host:$pdir rsync -rvl $pdir/$fname $cuser@yla$host:$pdir done]]></content>
      <categories>
        <category>Shell编程</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-入门笔记]]></title>
    <url>%2F2019%2F04%2F08%2Fspark%2F</url>
    <content type="text"><![CDATA[Spark可使用Java、Scala、Python、R和Sql 快速编写应用程序 rdd(Resilient Distributed Datasets)弹性分布式数据集 弹性：表示数据是可恢复的 可容错 利用的是血缘（依赖）关系 分布式：数据来源于分布式集群（如 hdfs，hbase,kafka） 数据集: 数据组织成一个集合(List) spark 运行模式： local本地模式 local[n] n表示使用n个线程，如果使用local[*]表示根据cpu核数确定，一般用于测试 standalone spark集群模式，需要搭建spark集群 例如：spark://192.168.200.129:7777… yarn模式 不需要搭建spark集群，直接将spark应用提交到yarn运行。类似于mapreduce mesos k8s。。。。。 rdd操作sc=sparkcontext 加载数据转换成rdd —spark操作的对象是rdd，因此需要把相关集合对象转换成rdd 1234val data = Array(1, 2, 3, 4, 5)val distData = sc.parallelize(data)// 4是分区数量val distData = sc.parallelize(data，4) 默认获取的数据会被分布式的存储在多个分区中，除非在sc下的方法中增加分区数量如：（，1） rdd数据源的两种，a. paralizeb. 外部数据源 textFile：spark记载rdd是lazy懒加载，也就是计算发生的时候才去加载，可以传入通配符，读取目录下所有的文件机或者压缩文件，默认情况下，一块就是一个分区，可以让分区大于快熟，不能让分区少于快数 12345scala &gt; al rdd5=sc.textFile("hdfs://192.168.200.129:9000/hbase/MasterProcWALs/state-00000000000000000009.log")//获取使用分区个数scala &gt; rdd5.partitions.size//将使用保存在该路径下scala &gt; rdd5.saveAsTextFile("/log.txt") tranformations:由rdd生成新的rdd 如 map filter groupby reducebykeyactions :由rdd生成，返回一个结果reduce saveasTextFile foreach count 注：对于转换次数比较多的rdd可以使用cache（persist方法，将rdd缓存到内存中，以空间换时间，减少计算时间）。 job-任务运行的部分当rdd运行到action方法时（reduce、 SaveAsTextFile、foreach count），会触发job的运行。由sparkcontext向spark集群提交job stage： spark将job划分成1到多个stage stage与stage之间通过shuffle划分注：如果在job过程中出现shuffle 就会划分成新的stage 发生shuffle的情况有：group by ,reduce by key , join ,distinct…. stage里是不会发生shuffle,但在stage与stage可能会发生shuffle操作suffle操作相当于有网络通信 stage内部计算是并行的 不会发生网络通信stage之间计算时串行的 会发生网络通信 注： shuffle执行，就会多一个stage action执行就是一个job job一定比stage少， stage一定等于或多于job 一个job可以包含一个或者多个stage task一个分区上的一次计算称为task多个task组成stage driverdriver 运行sparkcontext的应用，用于做任务job的提交 executorexecutor：运行任务，对数据进行具体的计算 闭包闭包：函数访问了外部的变量 driver里的代码是没有进行对数据做处理的代码是driver,executor 如果调用了driver的变量就会出错 形成闭包的状态 { driver val conf=new SparkConf() //设置app的名字 .setAppName(“demo”) //设置spark运行模式 这里以local为例 .setMaster(“local[2]”) val sc=new SparkContext(conf)} { executor val rdd=sc.parallelize(Range(1,10000),4) Thread.sleep(10000)} spark Java Api新建一个maven或者Java项目，然后新建一个文件scala 于原本的资源包同目录，然后设置该scala文件为资源包，且右击项目→_→ AddFramework support→_→ 在左侧菜单栏选择scala →_→选择应用 这样就可以使用scala的class以及object了 1234567891011121314151617181920212223242526272829package com.day01import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * @Author: 张峥 * @Date: 2019/4/8 17:39 */object sparkCoreDemo &#123; def main(args: Array[String]): Unit = &#123; val conf=new SparkConf() //设置app的名字 .setAppName("demo") //设置spark运行模式 这里以local为例 .setMaster("local[2]") /* spark 运行模式： 1，local本地模式 local[n] n表示使用n个线程，如果使用local[*]表示根据cpu核数确定，一般用于测试 2，standalone spark集群模式，需要搭建spark集群 3，yarn模式 不需要搭建spark集群，直接将spark应用提交到yarn运行。类似于mapreduce 4.mesos k8s。。。。。 */ val sc=new SparkContext(conf) val rdd=sc.parallelize(Range(1,10000),4) Thread.sleep(10000) &#125;&#125; Apitransformations map reduceByKey groupByKey Api高性能算子-mapPartition某些情况下需要频繁的创建和销毁资源，高性能算子可以在只在分区上创建或销毁。不用在每个元素上创建或销毁。提升了性能。 mapPartitionWithindex 再给你个索引下标 sample 抽样 union 并集 intersection 交集 aggregateByKey 聚合 分为三段： 给定初始值， 进行分块聚合运算, 块与块之间的聚合运算 sortByKey 按key排序 join cogroup 分组 cartesian 笛卡尔积 coalesce 合并 减少rdd分区的数量 过滤大型数据集后，可以更有效的运行操作 repatition 重新分区 随机重新分区，增加或者减少分区数量，进行平衡 用于数据倾斜 默认是has分区 但使用has分区-相同的key一定分到一个分区 actions1111 Spark 应用spark 应用提交与部署 spark应用部署到yarn有两种模式cluster:driver运行在集群的某一台主机上client:driver运行在客户端 首先 在idea中编译好自己的jar包之后pom.xml中 加入 scala代码注意需要注释掉spark的运行模式，因为后面会直接使用在yarn上且在最后要写好 输出到什么地方 12345678910111213141516171819202122232425262728package day02import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * @Author: 张峥 * @Date: 2019/4/9 10:35 */object SparkWordCount &#123; def main(args: Array[String]): Unit = &#123; val conf=new SparkConf() //设置app的名字 .setAppName("demo") //设置spark运行模式 这里以local为例 // .setMaster("local[4]")/*reduceByKey 只作用于 keyValue格式下的 */ val sc=new SparkContext(conf) val rdd=sc.parallelize(List("hello","hello","world","github")) rdd.map(x=&gt;(x,1)).reduceByKey(_+_).take(3).foreach(println) rdd.map(x=&gt;(x,1)).countByKey().foreach(println) rdd.map(x=&gt;(x,1)).groupByKey().map(kv=&gt;(kv._1,kv._2.sum)).foreach(println) //rdd.groupBy(x=&gt;x).map(kv=&gt;(kv._1,kv._2.size)).foreach(println) // 结果打包到该路径下 rdd.groupBy(x=&gt;x).map(kv=&gt;(kv._1,kv._2.size)).saveAsTextFile("hdfs://192.168.200.129:9000/SparkWordCount") &#125;&#125; 打包前需要在pom.xml中加入下列代码以防scala代码不编译 1234567891011121314151617181920&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 然后再maven里clean后 点M字符的命令行， 再选中需要导入的模块项目 打入以下命令- clean scala:compile compile package -DskipTests=true 首次加载需要下载很多东西，稍等一会，然后完成后就可以打包了 打包完成之后放入linux /tmp/spark目录下 然后进入spark bin目录下 spark-submit --class day02.SparkWordCount --master yarn --deploy-mode cluster /tmp/spark/sparkDemo101-1.0-SNAPSHOT.jar 注：如果运行报错没有权限的话，在当前目录下输入 chmod +x -R ./* 运行最后出现以下，则为正确 然后可进入ip:8088 查看状态 全局变量广播变量:可以再exector中使用，可以分发给各个分布式节点使用,但不能修改作用：一般用于大小表的join,小标广播能够节省网络传输，避免shuffle map join实用于广播维度表给事实表使用 累加器作用于driver， exector只有向driver发送累加请求，由driver进行累加 窄依赖 宽依赖窄依赖：子rdd的一个分区，只依赖与父rdd的一个分区宽依赖：子rdd的一个分区 依赖于父rdd的多个分区 区分的关键在于shuffle不发生shuffle是窄依赖，发生shuffle是宽依赖 rpc 远程方法调用传输通道socket网络通信客户端动态代理服务端反射 客户端 client.java这是一个远程累加器的服务端客户端的案例 注意看客户端的前段注释理解！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.day03.rpc;import java.io.InputStream;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.OutputStream;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.net.Socket;/** * 输出流和输入流是一个通道，每个端都有自己的输入输出，两者方向不同各为其主 * 而让通道以及request和response可以结合使用的方法时socket 网络通信 * * 客户端通过动态代理 获取向服务端发送请求和接收数据信息的功能 * 运行过程： * 1，建立连接之后将方法名和参数放在request请求之中，开启输出流，然后写进客户端向服务端方向的输出流里 * 2，服务端开启输入流，接收客户端的request请求，获取其中的方法名和参数，然后通过反射机制调用方法且生成返回值 * 3，将方法生成的返回值放入response中，再开启服务端的输出流，并将结果返回值 写进服务端指向客户端的输出流， * 4，客户端得到服务端的回应，开启客户端的输入流接收response的返回值 输出即可 * * * 形象理解 * A区为服务端 B区为客户端 快递就是request ，回应消息 response * A区和B区之间的公路就是通道 * A的输入流就是通过公路B区到A区的 输出流就是通过公路A区到B区的 * 快递员通过将快递从B送到A，A收到了就打电话回应消息 */public class Client &#123; public static void main(String[] args) &#123; //通过动态代理 向服务端发送请求 Acc proxy = (Acc) Proxy.newProxyInstance(Request.class.getClassLoader(), new Class[]&#123;Acc.class&#125;, new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Request request = new Request(); request.setMethod(method.getName()); request.setPara(args); //建立与服务端的链接 Socket socket = new Socket("192.168.1.129", 999); OutputStream outputStream = socket.getOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(outputStream); //将请求以及你需要的方法名以及参数放入request中，写入到输出流里“暂存” oos.writeObject(request); //得到服务端的相应 InputStream inputStream = socket.getInputStream(); //对象流 反序列化 ObjectInputStream ois = new ObjectInputStream(inputStream); Response response = (Response) ois.readObject(); //通过服务端的执行后的结果， return response.getResult(); &#125; &#125;); int result = proxy.add(10000); System.out.println(result); /*AccImpl acc = new AccImpl(); int add = acc.add(10);*/ &#125;&#125; 服务端 server.java1234567891011121314151617181920212223242526272829303132333435363738package com.day03.rpc;import java.io.*;import java.net.ServerSocket;import java.net.Socket;public class Server &#123; public static void main(String[] args) throws Exception, ClassNotFoundException &#123; //需要使用的方法 AccImpl acc = new AccImpl(); //建立通信连接 ServerSocket serverSocket = new ServerSocket(999); for(;;)&#123; //接收客户端的连接 Socket client = serverSocket.accept(); //开启输入流，为了接收客户端请求 InputStream inputStream = client.getInputStream(); ObjectInputStream ois = new ObjectInputStream(inputStream); //接受来自客户端的请求 请求包括 方法名和参数 Request request = (Request) ois.readObject(); //AccImpl acc = new AccImpl();//需要使用的方法,如果将acc的实例化放到循环外就是会形成累加 //需要通过反射机制 ,来调用方法,使用方法返回返回值给result Object result = AccImpl.class.getMethod(request.getMethod(), int.class).invoke(acc, request.getPara()); //System.out.println("server result:"+request); //输出访问客户端的ip以及方法返回的结果 System.out.println("client"+client.getInetAddress().getHostAddress()+" ，result："+(int)result); //将结果发送回去 //将结果保存在response中，然后通过输出流将结果保存在response中 Response response = new Response(); response.setResult(result); //开启输出流，写入含有result的response对象 OutputStream outputStream = client.getOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(outputStream); oos.writeObject(response); &#125; &#125;&#125; 方法的接口 Acc12345package com.day03.rpc;public interface Acc &#123; int add(int num);&#125; 方法 AccImp.java12345678910111213141516171819202122package com.day03.rpc;import java.lang.reflect.Method;public class AccImpl implements Acc &#123; private int acc; @Override public int add(int num) &#123; acc+=num; return acc; &#125; /* public static void main(String[] args) throws Exception, IllegalAccessException, InstantiationException, NoSuchMethodException &#123; Acc object = (Acc) Class.forName("com.day03.rpc.AccImpl").newInstance(); //知道方法名 调用方法 通过反射 Method method = Class.forName("com.day03.rpc.AccImpl").getDeclaredMethod("add", int.class); int result = (int) method.invoke(object, 10); System.out.println(result); &#125;*/&#125; request和response123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.day03.rpc;import java.io.Serializable;//需要反序列化接口实现/** * Request在这里只是用来保存客户端向服务端发送的数据信息 */public class Request implements Serializable &#123; private String method; private Object[] para; public String getMethod() &#123; return method; &#125; public void setMethod(String method) &#123; this.method = method; &#125; public Object[] getPara() &#123; return para; &#125; public void setPara(Object[] para) &#123; this.para = para; &#125;&#125;//-----------------------------------------------------------------//package com.day03.rpc;import java.io.Serializable;//需要反序列化接口实现/** * Respnse在这里只是用来保存服务端向客户端发送的数据信息 */public class Response implements Serializable &#123; private Object result; public Object getResult() &#123; return result; &#125; public void setResult(Object result) &#123; this.result = result; &#125;&#125; Spark SQLsql 命令行DataFrames在spark bin目录下打开，命令行窗口 输入.\spark-shell –master local[2]进入scala命令行 12345scala &gt; val df = spark.read.json("../examples/src/main/resources/people.json")scala &gt; df.showscala &gt; df.createOrReplaceTempView("people")//sparksql（）这句话获得的是一个DataFrames 需要show才能以结构化显示scala &gt; spark.sql("select * from people where age&gt;20").show sql 案例代码注：本地连接虚拟机的mysql出现拒绝连接的问题 此时给其授权 grant all privileges on . to root@’%’ identified by “gxyzxf”；flush privileges; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.day03import java.util.Propertiesimport org.apache.spark.sql.SparkSessionimport org.joda.time.DateMidnight.Property/** * @Author: 张峥 * @Date: 2019/4/10 14:26 */object SparkSql &#123; //定义case class case class Record(name:String,age:Int) case class Word(word:String) def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .appName("Spark SQL basic example") .master("local[*]") //.config("spark.some.config.option", "some-value") .getOrCreate() // For implicit conversions like converting RDDs to DataFrames import spark.implicits._ val df= spark.read.json("D:\\zz\\Tools\\spark-2.3.0-bin-hadoop2.7\\spark-2.3.0-bin-hadoop2.7\\examples\\src\\main\\resources\\people.json") //df.show() //读取jdbc数据 val prop=new Properties() prop.setProperty("user","root") prop.setProperty("password","123456") prop.setProperty("driver","com.mysql.jdbc.Driver") spark.read.jdbc("jdbc:mysql://master1:3306/test","test",prop).show() //在实际操作中，很多数据一开始，没有模式，如日志，文本文件 //可以先将数据变换成rdd 然后再将rdd转化成dataframe //dataframe就是给一个rdd加一个模式 使其结构化 val rdd=spark.sparkContext.parallelize(List("tom,18","jerry,2")) //case class Record(name:String,age:Int) val df2=rdd.map(s=&gt;&#123; val parts=s.split(",") Record(parts(0),parts(1).toInt) &#125;).toDF() df2.show() //用sparksql实现wordcount //case class Word(word:String) val df3=spark.sparkContext.parallelize(List("a","a","a","b")) df3.map(s=&gt;Word(s)).toDF().createOrReplaceTempView("wc") spark.sql("select word,count(word) cw from wc group by word order by cw desc").show() &#125;&#125; 运行结果 rdd-数据类型dataframe就是给一个rdd加一个模式 使其结构化sparksessionsparkcontext例如 sparksession.sparkcontext dataset当使用api操作数据的时候，建议使用dataset当通过sql操作的时候，两种都行 自定义函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package day04import com.day03.SparkSql.Record/** * @Author: 张峥 * @Date: 2019/4/11 9:22 */object SparkDataSetDemo &#123; case class Record(name:String,age:Int) def main(args: Array[String]): Unit = &#123; import org.apache.spark.sql.SparkSession val spark=SparkSession .builder() .appName("Spark SQL basic example") .master("local[*]") //.config("spark.some.config.option", "some-value") .getOrCreate() val rdd=spark.sparkContext.parallelize(List("tom,18","jerry,2")) //隐式转换 import spark.implicits._ val ds=rdd.map(s=&gt;&#123; val parts=s.split(",") Record(parts(0),parts(1).toInt) &#125;).toDS() val df=rdd.map(s=&gt;&#123; val parts=s.split(",") Record(parts(0),parts(1).toInt) &#125;).toDF() //怎么变成rdd //建议使用 ：dataset：rdd[Object] dataset 可以通过面向对象的方式来操作数据 ds.filter(record=&gt;record.age&gt;3).show() //dataframe=dataset[row] dataframe通过的是sql方式进行操作数据 df.filter(row=&gt;row.getAs[Int]("age")&gt;18) //dataSet 和 dataFrame 可以相互转换 建议使用dataset //无论使用dataset 还是dataFrame 变成表操作都一样 ds.createOrReplaceTempView("t1") df.createOrReplaceTempView("t2") //自定义sql函数 val toUpper=(word:String)=&gt;word.substring(0,1).toUpperCase()+word.substring(1) //注册函数 spark.sqlContext.udf.register("toUpper",toUpper) //自定义函数用于sql中 spark.sql("select toUpper(name),age from t1 where age&gt;1").show() //ds.show() &#125;&#125; 小结针对以上代码进行小结rdd数据文件直接new SparkContext().textFile(“//“) 将文件读取进来 成为rdd或者自定义集合数组成为rdd new SparkContext().parallelize(List(“”))在rdd弹性分布式结果集中可以使用api进行调用 dataFrame dataSet两种数据类型 将结构化文件通过SparkSession.read.api(“//“) 注：api里得有相应的格式 非结构化的文件只能先通过textfile 成为rdd ,然后导包 import spark.implicits._隐式转换的形式，再rdd().toDF 或者rdd().toDS 将rdd转换成dataFrame或者dataSet 建立的集合也是同2需要从rdd进行转换 sparkcontext rddsparksession dataframe datasetsparkstreaming dstream dataFrame 针对于sql方式操作的dataSet 通过面向对象进行的操作的 大数据处理的两种:实时/流处理（real-time）:数据的产生于数据的计算完成相隔时间很短，大约在秒级sparkstreaming /storm SparkStreaming一般SparkStreaming 配合kafka使用 一方面原因是因为利用kafka的缓存通道， sparkstreaming 有状态计算状态：一个动态的属性 如累计 checkpoint UpdateStateByKey 状态计算注：如果报错java.lang.NoSuchMethodError: net.jpountz.lz4.LZ4BlockInputStream原因：应用在执行时对数据解码（反序列化）时，使用了默认的lz4解压缩算法，在spark-core中依赖的lz4版本是1.4，而kafka-client中依赖的lz4版本是1.3版本，在生成解压器时，版本不兼容异常。解决方法： conf = SparkConf().set(“spark.io.compression.codec”, “snappy”) pom.xml1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;lz4&lt;/artifactId&gt; &lt;groupId&gt;net.jpountz.lz4&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 窗口计算计算最近一段时间的数据运算 离线处理（off-line）:存储后处理 时间间隔长 mapreduce hive spark sparksql ….. Parquet?]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[过滤器和监听器]]></title>
    <url>%2F2019%2F04%2F05%2F1_%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E7%9B%91%E5%90%AC%E5%99%A8%2F</url>
    <content type="text"><![CDATA[过滤器和监听器一. 过滤器什么是过滤器就是把不需要的东西过滤掉，筛选掉，排除掉 filterServlet中过滤器的作用: 可以过滤掉不需要的资源 阻止当前资源的访问。使其跳转至其他资源 登录校验 敏感字过滤 还可以过滤请求中的字符编码 在MVC框架中做前端控制器（处理所有请求共同操作。） 二. 监听器什么是监听器主要用于Servlet作用域对象的创建、对象属性的增加，删除，更新的监视 监听作用域对象的创建和销毁ServletRequestListener 监听请求对象的创建和销毁HttpSessionListener 监听会话对象的创建和销毁ServletContextListener 监听应用对象的创建和销毁 监听作用域对象的属性和增加、删除、更新ServletRequestAttributeListenerHttpSessionAttributeListenerServletContextAttributeListener]]></content>
      <categories>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>过滤器</tag>
        <tag>监听器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala讲师笔记]]></title>
    <url>%2F2019%2F04%2F01%2FScala%2F</url>
    <content type="text"><![CDATA[scala 简介 为什么要学习scala因为spark是用scala语言编写、 scala是什么是结合了面向对象和函数式编程思想的一门语言 scala适合写计算型的代码 是一个静态类型的语言 Jvm类型语言 注：面向对象适合做命令式编程-流程型的代码 JDK是SDK的一种 SDK software devlpoment kit 软件开发工具箱 JDK Java devlpoment kit Java开发工具箱 OOP AOP FP 面向对象，面向切面， 函数式编程 var 变量val 常量lazy 延迟加载：在定义的时候是不执行的，在使用时才执行初始化 数据类型 Any : 所有类的父类 Nothing 所有类都的子类 Null 所有引用类的子类 Unit AnyVal（Byte,Short,Int,Long，Float,Double|Char,Boolean,Unit(空)）所有值类型的父类AnyRef所有引用类型的父类 ————下图实线表示父子关系， ——表示隐式转换trainl类似于接口 ，，可以让方法实现多继承 extend a with b,c,d {} 表达式 赋值语句等等都是有返回值的 Scala编程语言不支持++ – 操作，但支持+= -=操作 定义函数方法def m1(x:Int,Y:Int):Int = {x+y}def 关键字，用来声明定义方法m1 方法名称x,y 形参：Int) 括号内的属于参数的类型（x:Int,y:Int参数列表= 表示连接方法名称、参数列表以及=后的代码执行体{} 方法体)：Int括号外的:Int表示返回值的类型 def cal(x:Int,f:(Int,Int)=&gt;Int,y:Int)=f(x,y)f:(Int,Int)=&gt;Int 表示参数方法f(x,y) 表示调用方法的格式 方法转换为函数var function1=m1var function1: （Int，Int）=&gt;int=(x:Int,y:Int)=&gt;{x+y}var a : Int ={2+6} 纯粹的函数式编程就是调用某一个逻辑的时候，还可以传入另外一个逻辑（方法或函数） 最终结论（方法和函数）都可以作为另外一个（方法或函数）的参数 scala练习123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142package com.demoimport scala.util.control.Breaks/** * @Author: 张峥 * @Date: 2019/4/1 10:41 *//** * object是关键字 ，只能在object中定义main方法 */object HelloWorld &#123; //def 关键字 表示方法 //参数类型写在参数后的【】内 //【】表示泛型 //Arrary[String]=String[] //unit 表示返回值为空，相当于java中的void //scala中没有static关键词，在object里的所有东西都是静态的 val name="Tom" val age:Int =12 //等于Java中的 public final int age=12 var hobby="足球" // hobby:String 可省略：后的类型，这是scala的隐式推断 def main(args: Array[String]): Unit = &#123; /* println("helloword") println(s"I am :,$name") println(s"age: ,$age") println( """ 111111111111 222222222222 3333333333333 44444444444 55555555555 """) def isOdd(a: Int) = &#123; if(a%2==0) true else false; &#125; val a=2 val result=isOdd(a); println(result) //if本身可以有值的 val result2=if(a%2==0) true else false print(result2)*/ bearkDemo(); hanshu(); &#125; ////方法def add(x: Int,y: Int): Int=x+y // cal（）括号里面是输入参数 =f（）后面的是调用方法格式 def cal(x:Int,f:(Int,Int)=&gt;Int,y:Int)=f(x,y) //在scala函数是一个完整的对象，可以作为参数传递def hanshu(): Unit = &#123; val add1=(x:Int,y:Int)=&gt;x+y val minus=(x:Int,y:Int)=&gt;x-y val mul=(x:Int,y:Int)=&gt;x*y val div=(x:Int,y:Int)=&gt;x/y add1(1,1) val add2=add1; add2(2,2) println("+ "+cal(3,add1,1)) println("- "+cal(3,minus,1)) println("* "+cal(3,mul,1)) println("/ "+cal(3,div,1)) // cal(8,(参数1,参数2)=&gt;表达式,5） print("8%5结果等于： "+cal(8,(x:Int,y:Int)=&gt;x%y,5))&#125; /** * continue */ def contimueDemo(): Unit = &#123; val loop = new Breaks for (a &lt;- 1 to 10) &#123; println(a) loop.breakable( if (a == 5) &#123; loop.break(); &#125; ) &#125;&#125; /** * break * op: =&gt;unit 代表（）里面的代码块必须不能有输出和输出参数 * &#123;&#125; 大括号包起来的 是一个代码块，且代码块可以有值 值是最后一行代码 * 表达式本身是没有值得到，运算表达式！ */ def bearkDemo(): Unit = &#123; var loop=new Breaks var aa=loop.breakable( for (a &lt;- 1 to 10)&#123; if(a==5)&#123; loop.break(); &#125; println(a) &#125; ) &#125; //for //遍历list 且 过滤list def listDemo(): Unit = &#123; val numList=List(1,2,3,4,5,6,7,8) for(e &lt;- numList if e!=3;if e%2==0)&#123; println(e) &#125; val newlist=for(e &lt;-numList if e!=3;if e%2==0) yield e &#125;//to可以用until代替 但左闭右开 //to 等于 def forName(): Unit = &#123; for(a &lt;- 1 to 10) &#123; print(a) &#125; &#125; def run(): Unit =&#123; println("Zaiwomen ") &#125;&#125; Scala集合数组（Array）分成两大类 定长数组：Array(Insert,delete是没有的，只能进行修改操作) 变长数组：ArrayBuffer 缓冲数组 没有指定素组长度，理论上可以无限增加元素 += 表示追加一个元素或者多个元素++= 表示追加一个Array或者ArrayBuffer或者其他的seq类型的实现类的对象insert(a,b) a是位置 b是需要追加的元素remove（a,b）a是位置， b是个数 要点var aa=new ArrayInt打印为：aa=new Array(0.0.0)) var aa=ArrayInt打印为：aa=new Array(3) var aa=Array.apply(3)打印为：aa=new Array(3) 如果偶在构造数组的时候带了new表示new申请一个固定长度的数组 如果没有使用new，其实也在构造数组，只不过调用了Array.apply(3) 定长数组转换成变长数组aa.Buffer变长数组转换成定长数组aa.toArray 数组的常用操作集合（Seq 序列/列表，Set集合，Map映射）Scala集合分为可变和不可变的集合函数作为对象 可以作为参数传递–::+:用于追加集合里的值，合并为一个新的集合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.demo/** * @Author: 张峥 * @Date: 2019/4/2 9:23 */object ListDemo&#123; def main(args: Array[String]): Unit = &#123; var l1=List(1,2,13,4,5) var l2=List("henan 1000","hebei 800","jiangsu 1200","shanghai 1100") //排序 l2.sortWith((x,y)=&gt;x.split(" ")(1).toInt&lt;y.split(" ")(1).toInt).foreach(println) print("----------------------- fileter ----------------------------") l2.filter(x=&gt;x.split(" ")(1).toInt&gt;1000).foreach(println) l2.filter(x=&gt;x.split(" ")(1).toInt&gt;1000).map(s=&gt;s.split("")(0)).foreach(println) l2.map(e=&gt;if(e.split(" ")(1).toInt&gt;1000) println(e)) //reduce排序 x用来获取reduce的返回值的默认为0 y 为每次从list获取的值 println("aaa "+l1.reduce((x,y)=&gt;if(x&lt;y) y else x)) println("aaa "+l1.reduce((x,y)=&gt;if(x&lt;y) y else x)) //特殊写法 l1.reduce((n1,n2)=&gt;n1+n2) l1.reduce(_+_)//效果等于上方的效果 //排序 l1.filter(x=&gt;x%2==0).foreach(println) l1.filter(y=&gt;y&gt;2&amp;&amp;y&lt;4).foreach(println) //foreach 和map唯一区别 有无输出返回值 println(sum(l1)) // listTest1 &#125; def sum(list:List[Int]):Int=&#123;if(list.size==1) list.head else list.head+sum(list.tail) &#125; def listTest1: Unit =&#123; val l1=List(1,2,3,4) //map（）对集合的每个元素做一个操作 val l0=List(2,3,4) val l12=l1.map(x=&gt;(l0.map(y=&gt;x*y)).sum) println(l12) var l9=l1.map(x=&gt;x+1) var l10=l1.map(x=&gt;if(x%2==0) "X"+x else "Y"+x) println(l10) val l2=l1 :: l1 val l3="1213"+:l2 val l4=l2:+4 val l5=Nil //空集合 val l6=4::Nil::l1 //::: 用于集合对集合 val l7=l1:::(4::Nil) //删除元素，会生成新的 被删除过的集合 val l8=l1.drop(1) println(l1) println(l2) &#125;&#125; Scala Java互操作由于Java和Scala的编译后的都是.class文件，都是通过JVM进行操作的，因此可以互相调用Scala可以调用Java生成的类，反之亦然。使用时只要注意语法规范即可 特殊情况:java不能使用scala的不可变集合 隐式转换在scala中，当对象调用一个它本身并没有的方法时，编译器会从当前代码上下文中寻找拥有此方法的对象，然后在原对象的所拥有的方法中追加此方法。这个转换过程，不需要通过开发者手动操作，是编译器在背后默默地转换。因此称之为隐式转换。这样可以再不更改原来代码的基础上，增加新的功能。提高代码的可维护性。类似于设计模式中的代理模式。 java集合转换成scala定长集合 1234567891011121314151617package com.demo;import java.util.ArrayList;/** * @Author: 张峥 * @Date: 2019/4/8 9:19 */public class javazChange &#123; public static void main(String[] args) &#123; ArrayList&lt;String&gt; l=getList(); &#125; public static ArrayList&lt;String&gt; getList() &#123; return new ArrayList&lt;String&gt;(); &#125; 123456789101112131415161718package com.demo/** * @Author: 张峥 * @Date: 2019/4/8 9:19 */object scalaChange &#123; def main(args: Array[String]): Unit = &#123; val l=javazChange.getList(); l.add("123") //把java集合转换成scala不可变集合(隐式转换) import collection.JavaConverters._ val l1=l.asScala.toList &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell-编程]]></title>
    <url>%2F2019%2F03%2F31%2Fshell%2F</url>
    <content type="text"><![CDATA[#Shell入门学习 shell简介 shell是一个命令行解释器，它为用户提供一个向linux内核发送请求以便运行程序的界面系统程序，用户可以用Shell来启动、挂起、停止、甚至别写一些程序。 因为是跑在linux的shell中，所以叫shell脚本。说白了，shell脚本就是一些命令的集合。举个例子，我想实现这样的操作：1）进入到/tmp/目录；2）列出当前目录中所有的文件名；3）把所有当前的文件拷贝到/root/目录下；4）删除当前目录下所有的文件。简单的4步在shell窗口中需要你敲4次命令，按4次回车。这样是不是很麻烦？当然这4步操作非常简单，如果是更加复杂的命令设置需要几十次操作呢？那样的话一次一次敲键盘会很麻烦。所以不妨把所有的操作都记录到一个文档中，然后去调用文档中的命令，这样一步操作就可以完成。其实这个文档呢就是shell脚本了，只是这个shell脚本有它特殊的格式。Shell脚本能帮助我们很方便的去管理服务器，因为我们可以指定一个任务计划定时去执行某一个shell脚本实现我们想要需求。 简单的说，shell脚本就是一些命令的集合 入门编程 下面进入正题，先从简单的开始来学习 第一步、我们需要创建一个脚本，就和创建一个文件一样， 1vim filename 一般脚本文件名我们以.sh结尾，就相当于甲鱼的屁股。 然后当我们写好脚本最后，就可以直接执行可以赋予权限 一般使用两种执行方法 1.第一种先给写好的脚本附加权限即可直接执行（注意这里面是 + x）。 2.在脚本前面加 bash 或者 sh 也可以执行无需赋权限 3.一般我们在初次接触或者试写脚本的时候都会加一个 (- x) 这样不仅可以看到脚本执行的结果，还可以看到脚本脚本对应的命令是什么 4.date命令一般用于表现时间， 可以在后面加“+%Y%m%d %H%M%S” 不仅如此还可以在日期前面加日期 +%w 在此就不一一演示了 shell脚本中的变量变量：是shell传递数据的一种方式，用来代表每个取值的符号名。使用变量是为了解决脚本臃肿问题，且使脚本变得更加灵活。命名规范 变量名可以由字母，数字和下划线组成，但是不能以数字开头，环境变量名建议大写，便于区分。 在bash中，变量的默认类型都是字符串型的，如果要进行数值运行，则必须制定变量类型为数值型。 变量用等号连接值，等号左右两侧不能有空格。 变量的值如果有空格吗，需要使用单引号或者双引号包括。 变量基本用法 使用反引号运行里面的命令，并把记过返回给Isvalue变量。从而可以通过Isvalue变量，来执行命令反引号里面的命令 同样的道理 反引号里面放的是命令， 当变量等于其他值，当把这个变量的值传给其他变量，这个所谓的其他变量一样拥有本身变量的值，而且变量是，可以进行拼接使用的，这个就不做演示了。 符号标识符 命令连接符号 ； 可以一行打多条命令 用；分号连接 &amp;&amp; 逻辑与 条件满足，才执行后面的语句 || 逻辑或，条件不满足，才执行后面的语句test “$name” == ”yangmi” &amp;&amp; echo “&amp;&amp;后，命令正确执行我” || echo “||后，命令失败执行我咯”vim 单引号和双引号的区别： 单引号是直接显示里面的所有内容，不做任何处理 双引号是将引号里面的特殊字符进行处理执行后在显示出来。 当双引号的加了\就等同于单引号的效果，不会做任何处理 “\ ”=’’ $ 符号的用法(使用$符号就是需要在执行脚本时 在后面追加变量如（sh hello.sh 1 2）) $n 按第n个获取自己变量，从权限类型之后 包括脚本名称都可以获取， 按空格区分个数,但超过10且包括10的参数就需要使用大括号包含，如{${10} 注：$0一般表示为脚本名 $*代表该命令行中所有的命令，是把所有参数作为一个整体进行输出“$1 $2 $3 $4” $@ 同样也是代表命令行的所有参数，但它是把所有参数逐个进行输出”$1” “$2” “$3” “$4” $#可以记录有多少参数 $? 执行上一个命令的返回值 执行成功，返回0，执行失败，返回非0（具体数字由命令决定） 一般用于命令之后加 ;$?这样可以直接一行命令知晓命令是否正确运行 $$ 当前进程的进程号（PID），即当前脚本执行时生成的进程号 $! 后台运行的最后一个进程的进程号（PID），最近一个被放入后台执行的进程 $符号总结起来，作为变量名的话是一个标识，做一个表达式的前缀而言就是要处理$符号后面的表达式或者语句 shift指令：参数左移，每执行一次，参数序列顺次左移一个位置，$# 的值减1，用于分别处理每个参数，移出去的参数不再可用 输入参数，计算求和 read 命令 可以直接一行read x 就可以做到让用户输入的效果，虽然x没有加$符号，但是那只是在获取变量值的情况下，在运算或者赋值给别人的时候，就需要加上$符号了, 可以写 read x read p 类似于echo的作用,也可以将上一句的输出和这一句的输入连在一起写，这样就不需要换行输入了 read [选项] 值 :read -p(提示语句) -n(字符个数) -t(等待时间，单位为秒) –s(隐藏输入) 其实变量和java很类似，你可以一开始就申明好变量，也可以在使用的时候直接$+变量名 $()与${}的区别 $( )的用途和反引号 ``一样，用来表示优先执行的命令 ${ } 就是取变量了 $((运算内容)) 适用于数值运算 Shell脚本中的判断逻辑if判断if 判断语句; then command fishell 脚本中if还经常判断关于档案属性，比如判断是普通文件还是目录，判断文件是否有读写执行权限等。常用的也就几个选项：-e ：判断文件或目录是否存在-d ：判断是不是目录，并是否存在-f ：判断是否是普通文件，并存在-r ：判断文档是否有读权限-w ：判断是否有写权限-x ：判断是否可执行使用if判断时，具体格式为：if [ -e filename ] ; then case 命令case命令是一个多分支的if/else命令，case变量的值用来匹配value1,value2,value3等等。匹配到后则执行跟在后面的命令直到遇到双分号为止(;;)case命令以esac作为终止符。注： ）内的关键词就是脚本后面跟的$1 如果两者相等才会进入’echo’ * 代表除了以上其他的情况 ;;每一个case之前都要加;; 不能忘！ for 循环除了我们下图看到：一种方式：最原始的for循环方式另一种方式：类似于增加for循环， while循环 自定义函数function 自定义函数名{ }]]></content>
      <categories>
        <category>Shell编程</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper部分笔记]]></title>
    <url>%2F2019%2F03%2F28%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[Zookeeper简介Zookeeper 多个相同的节点，每一个里面存储的都一模一样，并且随时同步，且运行时所有数据存放在内存当中。Zookeeper本身就是高可用，因此只要存在一个Zookeeper节点就可以正常运作 Paxos算法解决分布式一致的问题 raft zab协议 解决zookeeper分布式不一致的问题 高可用： 在Zookeeper中成功创建第一个相关目录 成为master 就是高可用的现象作用集群选择奇数，支持少于半数的宕机，少于半数的宕机，可以保持分布式一致性]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase-note]]></title>
    <url>%2F2019%2F03%2F27%2Fhbase-note%2F</url>
    <content type="text"><![CDATA[Hbase基础理论HBase以表的形式存储数据。表有行和列组成。划分为若干个列族。 1.HBase简介Hbase是什么？ Hbase是一个高可靠性、高性能、列存储、面向列、可伸缩、实时读写的分布式存储系统，利用HBASE技术，可以让我们在廉价便宜的PC Server上搭建大规模结构化存储的集群。 HBase仅需使用普通的硬件配置，就可以能够处理，由成千上万的行和列所组成的大型数据库（非关系型数据库） 存储形式 HBase以表的形式存储数据。表有行和列组成。列划分为若干个列族(column-family)。 Hbase与Hdfs的关系Hbase依托于Hadoop的HDFS作为最基本存储基础单元，通过使用hadoop的DFS工具就可以看到这些这些数据存储文件夹的结构。Hbase与MR的关系Hadoop可以通过Map/Reduce的框架(算法)对HBase进行操作。 #1 HBase的几大组件HMaster功能： 监控RegionServer 处理RefionServer故障转移 处理元数据的变更 处理region的分配和移除 在空闲时间进行数据的负载均衡 通过Zookeeper发布自己得位置给客户端 RegionServer功能： 负责存储HBase的实际数据 处理分配给它的Region 刷新缓存到HDFS 维护HLog 执行压缩 负责处理Region分片 其他组件1.Write-Ahead logsHBase的修改记录，当对Hbase读写数据的时候，数据不是直接写进磁盘的，它会在内存中保留一段时间。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件当中，然后再写入内存中，所以在系统出现故障的时候，数据可以通过日志文件重建数据 2.HFile这是在磁盘上保存原始数据的实际物理文件，是实际的存储文件 3.StoreHFile存储在Store中，一个Store对应HBase表 中的一个列族。 4.MemStore内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在Write-Ahead-Logs中会后，RegionServer会在内存中存储键值对 5.RegionHbase表的分片：Hbase表会根据RowKey值的不同，被切分成不同的region，存储在RegionServer中，在一个RegionServer中可以有多个不同的region。 HBase架构 Hbase之所以可以很快的进行读写操作 是因为以空间换时间。 Hbase所进行的命令操作，会一开始存放在内存当中，当达到一定峰值64M/128M的时候会提交进磁盘当中。且也在写入内存之前，先写入log当中，以防宕机数据丢失，log是顺序读写，因此也不占性能，很快。 下图为Hbase架构图 HBase 数据结构Row KeyRow key是用来检索记录的主键。访问HBASE table中的行，只有三种方式： 通过单个row key 访问 通过row key 的range（正则访问） 全表扫描 Row key保存为字节数组，存储时，数据按照Row key的字典序排序存储，Ps.设计Row key时，将经常一起读取的行存储在一起（位置相关性） Hbase Rowkey设计在hbase当中rowkey是唯一的索引a. 将经常需要查询的条件，放入rowkey中b. hash rowkey为了避免读写热点 rowkey=hash(userkey)+userkey 反转rowkey reverse(rowkey) ps：随机数不行，查询时无法获取 Columns Family列族：Hbase表中的每个列都属于某个列族，列族想相当于一些列的map集合，列族是表的schema的一部分（而列不是），必须在使用表之前定义，列名都以列族作为前缀。 Cell由{row key,columnFamily,version} 唯一确定的单元 HBase中通过row和columns确定的唯一一个存贮单元称为cell。 由{row key, column(= + ), version} 唯一确定的单元。 cell中的数据是没有类型的，全部是字节码形式存贮。 每一个cell可以存储多个数据。 TimeStamp Hbase中通过rowkey和columns确定的唯一一个存贮单元成为cell。每个cell都保存着同一份数据的多个版本（创建表的时候可以设定版本数量，当存储相同列数据时，显示出来的是按照时间戳排序显示最新put的数据）。版本同构时间戳来索引，时间戳的类型是64位整型。 为了避免数据存放过多版本造成管理负担，Hbase提供了两种数据版本回收方式。 一是保存数据的最后N个版本， 二是存储一段时间内的版本。用可针对每个列族进行设置 ###hbase 预分区默认情况下，hbase会为了新创建的表创建一个region。因此会带来热点问题。可通过在创建表的时候，指定多个region解决热点问题。方法有两种： 命令方式：create ‘t2’,’cf1’,SPILTS=&gt;[‘b’,’c’,’d’,’e’] 代码方式: HBase索引Row key 唯一的一级索引；因此对于列值的查询，只能扫描全表，效率低。因此可以借助： hbase 协处理器 solr 分布式索引 实现二级索引：使用协处理器，将需要的二级索引和一级索性 处理成 二级索引对应所有的一级索引，然后由solr分布式索引存储，以备使用 协处理器（coprocesser）触发器 （trigger）当我们在进行某些操作如增删改查的时候，执行的某个操作当向Hbase进行写操作的时候，使用触发器进行中途拦截，触发-预处理-实现二级索引的实现a. 继承BaseRegionObserverb. 重写preput,predelete方法c. 打包上传hbase lib 目录d. 为表添加协处理器 1alter &apos;t3&apos;,&apos;coprocessor&apos;=&gt;&apos;/develop/hbase/hbase-1.4.8/lib/hbasedemo-1.0-SNAPSHOT.jar|com.demo.SecondIndexObserver|1001|&apos; Hbase安装与使用Hbase 伪分布部署伪分布模式需要用到hadoop文件系统 ，所以配置会比单机模式麻烦很多 (1)、修改conf/hbase-env.shexport HBASE_CLASSPATH=/home/lin/hadoop/hadoop-2.6.1/etc/hadoop (2)、编辑hbase-site.xmlhbase.rootdir 要配置为hdfs上的路径；hdfs://master:9000/hbase 12345678&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; (3)、启动hbase 1bin/start-hbase.sh jps出现三个H开启的进程则ok 然后就可以进入shell进行对hbase的操作。 1bin/hbase shell Quick Start（Shell操作）create &#39;test&#39;,&#39;cf1&#39;,&#39;cf2&#39;创建表test 列族cf1,cf2 list查看表 describe &#39;test&#39;查看表test结构 put &#39;test&#39;,&#39;jack&#39;,&#39;cf1:age&#39;,&#39;18&#39;根据表名、行键以列族进行插入某行数据 get &#39;test&#39;,&#39;jack&#39;根据键值查询数据 只能查一行 get &#39;test&#39;,&#39;jack&#39;，&#39;cf1:name&#39;根据列族中的列查询数据 只能查一行 scan &#39;test&#39;扫描全表数据 scan &#39;test&#39;,{STARTROW=&gt;&#39;jack&#39;}从头开始扫描rowkey为jack的行的数据 count &#39;test&#39;读取表数据行数 deleteall &#39;test&#39;,&#39;jack&#39;删除某rowkey的全部数据 delete&#39;test&#39;,&#39;jack&#39;，&#39;cf1:name&#39;删除某rowkey的某一列数据 disable &#39;test&#39;truncate &#39;test&#39;清空表数据（清空标的操作顺序为先disable，然后再truncate） disable &#39;test&#39;drop &#39;test&#39;删除表，也要先让表处于disable状态才能删除 create &#39;test&#39; ,{name=&gt;&#39;cf1&#39;,versions=&gt;3}创建test表 列族名为cf1并存放在3个版本里 alter &#39;student&#39;,{Name=&gt;&#39;cf1&#39;,versions=&gt;3}修改表的信息：将cf1列族的数据存放在3个版本里修改我的我 tips:HBase只支持单行事务，也就是说对一行数据进行修改的时候会对行进行锁定，不想sql可以进行多行锁定 Zookeeper 存储了 Hbase元数据的位置 Hbase的元数据存放在自己身上，我们通过连接Zookeeper获取HBase元数据在HBase的位置， Hmaster管理表-增删改查 RegionServer具体管理表的读写Store是由Region创建 HBase APIHbase读写扫描 pom.xml加入依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.6&lt;/version&gt; &lt;/dependency&gt;配置Hbase.properties hbase.master=192.168.200.129:60000hbase.zookeeper.quorum=192.168.200.129 添加config类 1234567891011121314public class Config &#123; private static Properties conf=new Properties(); static &#123; InputStream resource = Config.class.getClassLoader().getResourceAsStream("hbase.properties"); try &#123; conf.load(resource); &#125; catch (IOException e) &#123; &#125; &#125; public static String getProperty(String key) &#123; return conf.getProperty(key); &#125;&#125; 读写、批量写入、扫描查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.zhiyou100.kafka.Hbase;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.ArrayList;import java.util.Iterator;import java.util.List;public class HbaseCurd &#123; static Connection connection = null; //静态块为连接驱动的固定写法 static &#123; Configuration hbaseConfig = HBaseConfiguration.create(); hbaseConfig.set("hbase.master", Config.getProperty("hbase.master")); hbaseConfig.set("hbase.zookeeper.quorum", Config.getProperty("hbase.zookeeper.quorum")); try &#123; connection = ConnectionFactory.createConnection(hbaseConfig); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws IOException &#123; //readDemo(); //writeDemo(); batchWriteDemo(); //scanDemo(); &#125; private static void scanDemo() throws IOException &#123; Table test = connection.getTable(TableName.valueOf("test")); Scan scan = new Scan(); scan.setStartRow(Bytes.toBytes("tom1000")); scan.setStopRow(Bytes.toBytes("tom10000")); ResultScanner scanner = test.getScanner(scan); while (true)&#123; Result result = scanner.next(); List&lt;Cell&gt; cells = result.listCells(); Iterator&lt;Cell&gt; iterator = cells.iterator(); while (iterator.hasNext())&#123; Cell cell = iterator.next(); System.out.println(Bytes.toString(cell.getValue())); &#125; &#125; &#125; private static void batchWriteDemo() throws IOException &#123; Table test = connection.getTable(TableName.valueOf("test")); long start=System.currentTimeMillis(); ArrayList&lt;Put&gt; puts = new ArrayList&lt;&gt;(); for(int i=0;i&lt;1000*1000;i++) &#123; Put tom = new Put(Bytes.toBytes("tom"+i)); tom.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("hobby"), Bytes.toBytes("zhuolaoshu")); tom.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("age"), Bytes.toBytes("2")); tom.addColumn(Bytes.toBytes("cf1"), Bytes.toBytes("gender"), Bytes.toBytes("xiong")); puts.add(tom); if(i%1000==0) &#123; test.put(puts); puts.clear(); &#125; &#125; test.close(); connection.close(); long end=System.currentTimeMillis(); System.out.println("cost :" +(end-start)/1000); &#125; //增加数据操作 private static void writeDemo() throws IOException &#123; Table test = connection.getTable(TableName.valueOf("test")); Put tom = new Put(Bytes.toBytes("tom")); tom.addColumn(Bytes.toBytes("cf1"),Bytes.toBytes("hobby"),Bytes.toBytes("zhuolaoshu")); tom.addColumn(Bytes.toBytes("cf1"),Bytes.toBytes("age"),Bytes.toBytes("2")); tom.addColumn(Bytes.toBytes("cf1"),Bytes.toBytes("gender"),Bytes.toBytes("xiong")); test.put(tom); test.close(); connection.close(); &#125; //获取数据操作 private static void readDemo() throws IOException &#123; Table test = connection.getTable(TableName.valueOf("test")); //rowkey jack Get get = new Get(Bytes.toBytes("jack")); Result result = test.get(get); List&lt;Cell&gt; cells = result.listCells(); for (Cell cell : cells) &#123; String key = Bytes.toString(cell.getQualifier()); String value = Bytes.toString(cell.getValue()); System.out.println("key: "+key+" value: "+value); &#125; //byte[] value = result.getValue(Bytes.toBytes("cf1"), Bytes.toBytes("age")); //byte[] value = result.getValue(Bytes.toBytes("cf1"), Bytes.toBytes("age")); //System.out.println("age: "+Bytes.toString(value)); test.close(); connection.close(); &#125;&#125; HBase过滤器同上配置的config类，在此不做详解 下列为过滤指定的列以及列族，类似于查询条件语句 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class HbaseFilterDemo &#123; static Connection connection = null; //静态块为连接驱动的固定写法 static &#123; Configuration hbaseConfig = HBaseConfiguration.create(); hbaseConfig.set("hbase.master", Config.getProperty("hbase.master")); hbaseConfig.set("hbase.zookeeper.quorum", Config.getProperty("hbase.zookeeper.quorum")); try &#123; connection = ConnectionFactory.createConnection(hbaseConfig); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; try &#123; //过滤指定列 rowFilterDemo(); //过滤指定列族的 //familyFilterDemo(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private static void familyFilterDemo() throws IOException, InterruptedException &#123; Table test = connection.getTable(TableName.valueOf("test")); FamilyFilter cf1 = new FamilyFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes("cf1"))); Scan scan = new Scan(); scan.setFilter(cf1); ResultScanner scanner = test.getScanner(scan); Iterator&lt;Result&gt; iterator = scanner.iterator(); while (iterator.hasNext()) &#123; Thread.sleep(100); //打印列 System.out.println(Bytes.toString(iterator.next().getRow())); &#125; &#125; //过滤rowKey //得到比tom2大比tom3小的rowkey // private static void rowFilterDemo() throws IOException, InterruptedException &#123; Table test = connection.getTable(TableName.valueOf("test")); //创建一个rowkeyfilter 操作符&gt; 比较器：按字典集的tom2 进行 Filter filter1 = new RowFilter(CompareFilter.CompareOp.GREATER,new BinaryComparator(Bytes.toBytes("tom2900"))); Filter filter2 = new RowFilter(CompareFilter.CompareOp.LESS,new BinaryComparator(Bytes.toBytes("tom4"))); List&lt;Filter&gt; rowFilters = Arrays.asList(new Filter[]&#123;filter1, filter2&#125;); //List&lt;Filter&gt; rowFilters = Arrays.asList(new Filter[]&#123;filter1, filter2&#125;); Scan scan = new Scan(); //scan.setFilter(filter1); //scan.setFilter(filter2); FilterList list = new FilterList(rowFilters); //list.addFilter(rowFilters); scan.setFilter(list); ResultScanner scanner = test.getScanner(scan); Iterator&lt;Result&gt; iterator = scanner.iterator(); while (iterator.hasNext()) &#123; // Thread.sleep(100); //打印列 System.out.println(Bytes.toString(iterator.next().getRow())); &#125; &#125;&#125; JAVA 第二索引的配置及使用方法加依赖 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.2.6&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package com.demo;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;import org.apache.hadoop.hbase.coprocessor.ObserverContext;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;import org.apache.hadoop.hbase.regionserver.wal.WALEdit;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.List;/** * @Author: 张峥 * @Date: 2019/3/29 10:34 */public class SecondIndexObserver extends BaseRegionObserver &#123; //在数据进入Hbase之前，可以进行的操作 //提取数据 建立二级索引 @Override public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException &#123; /*** * 获取操作表中的rowkey * 获取操作表中的age信息 * 获取索引表的表名 * 通过age从索引表中获取rowkey数据， * 再通过字符串拼接的形式追加“，+ ‘新的rowkey’”（加判断如果没有数据就不用追加 直接等于） * 再将索引及value写入索引表中 */ byte[] rowkey = put.getRow(); //获取操作表中的age List&lt;Cell&gt; cells = put.get(Bytes.toBytes("cf"), Bytes.toBytes("age")); Cell kv = cells.get(0); byte[] age = kv.getValue(); HTableInterface index_t3 = e.getEnvironment().getTable(TableName.valueOf("index_t3")); //以age为索引 Get get = new Get(age); //获取get有关的所有信息 Result result = index_t3.get(get); byte[] rkvalue = result.getValue(Bytes.toBytes("cf"), Bytes.toBytes("rk")); String newrkvalue; if(rkvalue!=null)&#123; newrkvalue = Bytes.toString(rkvalue) + "," + Bytes.toString(rowkey);&#125; else&#123; newrkvalue =Bytes.toString(rowkey); &#125; //以age为索引 Put index = new Put(age); //将rowkey的值传入以age为索引的表的相对应的列族‘rk’中 index.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("rk"),Bytes.toBytes( newrkvalue)); index_t3.put(index); &#125; //删除操作 @Override public void preDelete(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException &#123; /** * 获取rowkey值 * 获取年龄（索引） * 获取索引表 * 通过索引查询获取索引表中的相关数据 * 再从中获取对应的rowkey信息，并转换成字符串的形式 * 将需要删除的“，rowkey”替换成空 * 并通过该索引将改过的数据写入索引表中 */ //获取数据表中的索引 rowkey byte[] row = delete.getRow(); //通过上下文环境以及rowkey获取删除操作的相关数据 //e.getEnvironment().getRegion() 相当于表：因为是上下文操作关系 Result result = e.getEnvironment().getRegion().get(new Get(row)); //获取操作信息表中的age信息 byte[] age = result.getValue(Bytes.toBytes("cf"), Bytes.toBytes("age")); //获取索引表 HTableInterface index_t3 = e.getEnvironment().getTable(TableName.valueOf("index_t3")); Get get = new Get(age); Result index_result = index_t3.get(get); byte[] value = index_result.getValue(Bytes.toBytes("cf"), Bytes.toBytes("rk")); String rkvalue=Bytes.toString(value); byte[] newrkvalue = Bytes.toBytes(rkvalue.replace("," + Bytes.toString(row), "")); Put index = new Put(age); index.addColumn(Bytes.toBytes("cf"),Bytes.toBytes("rk"),newrkvalue); //将索引写入索引表中 index_t3.put(index); &#125;&#125; 做好之后，打包 放入Hbase文件夹中（注意如果放入HBase lib中，运行alter时报错无法找到此文件的话，就上传jar包至Hdfs一定可以）然后再创建]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-DaoVice]]></title>
    <url>%2F2019%2F03%2F27%2FHexo-DaoVice%2F</url>
    <content type="text"><![CDATA[注册 首先需要注册一个 DaoVoice，点击注册。 注册成功后，进入后台控制台，进入到 应用设置–&gt;安装到网站 页面，可以得到一个 app_id： 设置以 next 主题为例，打开 themes/next/layout/_partials/head.swig 文件中添加如下代码，位置随意： 123456789&#123;% if theme.daovoice %&#125; &lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i[&quot;DaoVoiceObject&quot;]=r;i[r]=i[r]||function()&#123;(i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset=&quot;utf-8&quot;;m.parentNode.insertBefore(a,m)&#125;)(window,document,&quot;script&quot;,(&apos;https:&apos; == document.location.protocol ? &apos;https:&apos; : &apos;http:&apos;) + &quot;//widget.daovoice.io/widget/0f81ff2f.js&quot;,&quot;daovoice&quot;) daovoice(&apos;init&apos;, &#123; app_id: &quot;&#123;&#123;theme.daovoice_app_id&#125;&#125;&quot; &#125;); daovoice(&apos;update&apos;); &lt;/script&gt;&#123;% endif %&#125; 在主题配置文件 _config.yml，添加如下代码： 123# Online contact daovoice: truedaovoice_app_id: 这里输入前面获取的app_id hexo s 调试模式下进行调试，效果满意后部署就可以看到最终效果啦]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase-伪分布部署]]></title>
    <url>%2F2019%2F03%2F27%2FHbase%E4%BC%AA%E5%88%86%E5%B8%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Hbase 伪分布部署伪分布模式需要用到hadoop文件系统 ，所以配置会比单机模式麻烦很多 (1)、修改conf/hbase-env.shexport HBASE_CLASSPATH=/home/lin/hadoop/hadoop-2.6.1/etc/hadoop (2)、编辑hbase-site.xmlhbase.rootdir 要配置为hdfs上的路径；hdfs://master:9000/hbase 12345678&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; (3)、启动hbase·bin/start-hbase.sh 12345678**jps**出现三个H开启的进程则ok![](https://i.loli.net/2019/03/27/5c9ae56bbd0f6.png)**然后就可以进入shell进行对hbase的操作。**``` bash bin/hbase shell 12345 create 'index_t3','cf' create 't3','cf'alter 't3','coprocessor'=&gt;'hdfs://192.168.200.129:9000/hbase/Hbasetest-1.0-SNAPSHOT.jar|com.demo.SecondIndexObserver|1001|' 然后就可以测试啦，对于t3进行写入几条语句的操作，然后再看索引表里是否新增了几条对应的操作]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程]]></title>
    <url>%2F2019%2F03%2F25%2F1_%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[线程并行与并发 程序在没有流程控制前提下，代码都是从上而下逐行执行的。 并发和并行是即相似又有区别(微观概念)： 并行指两个或多个事件在同一时刻点发生并发指两个或多个事件在同一时间段内发生在操作系统中，在多道程序环境下，并发性是指在一段时间内宏观上有多个程序在同时运行，但在单CPU系统中，每一时刻却仅能有一道程序执行（时间片），故微观上这些程序只能是分时地交替执行。倘若计算机系统中有多个CPU，则这些可以并发执行的程序便可被分配到多个处理器上，实现多任务并行执行，即利用每个处理器来处理一个可并发执行的程序，这样，多个程序便可以同时执行，因为是微观的，所以大家在使用电脑的时候感觉就是多个程序是同时执行的。所以，大家买电脑的时候喜欢买“核”多的，其原因就是“多核处理器”电脑可以同时并行地处理多个程序，从而提高了电脑的运行效率。单核处理器的计算机肯定是不能并行的处理多个任务的，只能是多个任务在单个CPU上并发运行。同理,线程也是一样的，从宏观角度上理解线程是并行运行的，但是从微观角度上分析却是串行运行的，即一个线程一个线程的去运行，当系统只有一个CPU时，线程会以某种顺序执行多个线程，我们把这种情况称之为线程调度。 时间片即CPU分配给各个程序的运行时间(很小的概念). 进程和线程进程进程是指一个内存中运行中的应用程序。每个进程都有自己独立的一块内存空间，一个应用程序可以同时启动多个进程。比如在Windows系统中，一个运行的abc.exe的程序就是一个进程。那么我们此时就可以处理同时玩游戏和听音乐的问题了，我们可以设计成两个程序，一个专门负责玩游戏，一个专门负责听音乐。但是问题来了，要是要设计一个植物大战僵尸游戏，我得开N个进程才能完成多个功能，这样的设计显然是不合理的。更何况大多数操作系统都不需要一个进程访问其他进程的内存空间，也就是说进程之间的通信很不方便。 此时我们得引入“线程”这门技术，来解决这个问题。线程线程是指进程中的一个执行任务(控制单元)，一个进程可以同时并发运行多个线程，如：多线程下载软件。多任务系统,该系统可以运行多个进程,一个进程也可以执行多个任务,一个进程可以包含多个线程.一个进程至少有一个线程，为了提高效率，可以在一个进程中开启多个执行任务,即多线程。 多进程：操作系统中同时运行的多个程序。多线程：在同一个进程中同时运行的多个任务。在操作系统中允许多个任务，每一个任务就是一个进程，每一个进程也可以同时执行多个任务，每一个任务就是线程。进程与线程的区别进程：有独立的内存空间，进程中的数据存放空间（堆空间和栈空间）是独立的，至少有一个线程。线程：堆空间是共享的，栈空间是独立的，线程消耗的资源也比进程小，相互之间可以影响的，又称为轻型进程或进程元。因为一个进程中的多个线程是并发运行的，那么从微观角度上考虑也是有先后顺序的，那么哪个线程执行完全取决于CPU调度器(JVM来调度)，程序员是控制不了的。我们可以把多线程并发性看作是多个线程在瞬间抢CPU资源，谁抢到资源谁就运行，这也造就了多线程的随机性。Java程序的进程(Java的一个程序运行在系统中)里至少包含主线程和垃圾回收线程(后台线程)。线程调度计算机通常只有一个CPU时,在任意时刻只能执行一条计算机指令,每一个进程只有获得CPU的使用权才能执行指令.所谓多进程并发运行,从宏观上看,其实是各个进程轮流获得CPU的使用权,分别执行各自的任务.那么,在可运行池中,会有多个线程处于就绪状态等到CPU,JVM就负责了线程的调度.JVM采用的是抢占式调度,没有采用分时调度,因此可以能造成多线程执行结果的的随机性。多线程优势多线程是为了同步完成多项任务，不是为了提供程序运行效率，而是通过提高资源使用效率来提高系统的效率。 宽带带宽是以位（bit）计算，而下载速度是以字节（Byte）计算，1字节（Byte）等于8位（bit），所以1024kb/s是代表上网带宽为1024千位（1M），而下载速度需要1024千位/秒（1024kb/s）带宽除以8，得出128千字节/秒（128KB/s）。 创建和启动线程方式1：继承Thread类: 定义一个类A继承于java.lang.Thread类. 在A类中覆盖Thread类中的run方法. 我们在run方法中编写需要执行的操作—-&gt;run方法里的,线程执行体. 在main方法(线程)中,创建线程对象,并启动线程.创建线程类对象: A类 a = new A类(); 调用线程对象的start方法: a.start();//启动一个线程 注意:千万不要调用run方法,如果调用run方法好比是对象调用方法,依然还是只有一个线程,并没有开启新的线程. 方式2：实现Runnable接口: 定义一个类A实现于java.lang.Runnable接口,注意A类不是线程类. 在A类中覆盖Runnable接口中的run方法. 我们在run方法中编写需要执行的操作—-&gt;run方法里的,线程执行体. 在main方法(线程)中,创建线程对象,并启动线程.创建线程类对象: Thread t = new Thread(new A());调用线程对象的start方法: t.start(); 继承与接口方式的区别: 继承 Java中是单继承的，如果使用继承的方式，那么这个类就不能有其他的直接父类 继承方式操作简单，获取线程的名字简单 共享资源来说：继承方式不能简单的实现（可以通过间接继承麻烦的实现） 实现 Java中可以实现多个接口，此时该类还可以继承其他类，并且还可以实现其他接口（在设计上更为优雅） 实现方式操作有点麻烦，获取线程的名字需要通过Thread.currentThread().getName()来获取 可以共享同一个资源 线程同步解决方案 : 当张羽执行-1的操作的时候，别人只能等待，等到张羽执行完毕，小周和小李才能继续执行 同步代码块 同步方法 锁机制（Lock） 同步代码块语法 ```java synchronized (同步锁) { 需要同步操作的代码块 }``` 同步锁在java中为了保证每一个线程都能正常执行原子操作，Java引入了线程同步机制。同步监听对象/同步锁/同步监听器/互斥锁对象的同步锁都是一个概念，只是在需要共享的数据对象上标记一个锁。当前并发的时候需要访问的共享资源叫做同步监听对象。任何时候，Java只允许一个线程拥有同步锁 同步方法语法 synchronized public void print() 同步锁： 对于非静态方法来说，同步锁就是this 对于静态方法来说，同步锁就是字节码对象（Apple.class）Lock锁Java 1.5之后出现的 为了取代 synchronized 代码块和同步方法 使用Condition接口对象的await()，signal(),signalAll()取代wait()，notify()，notifyAll() 避免死锁当多个线程都要访问某些共享资源A,B,C,D时候，保证每一个线程都按照相同的顺序去访问，比如说先访问A,然后B,C,D线程的生命周期: 新建new 的时候，在调用start方法之前，new Thread() 就绪start方法之后，等待JVM进行调用，分配资源 运行获取资源之后。 阻塞一般情况下，线程因为某些原因放弃CPU，暂时停止运行正在运行，需要获取同步锁的时候，但是这个锁被其他线程获取。 等待状态wait()，sleep() 死亡线程终止]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程]]></title>
    <url>%2F2019%2F03%2F21%2F1_%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[网络编程重点 TCP https://blog.csdn.net/qzcsu/article/details/72861891 UDP 网络分层为了减少网络设计的复杂性，绝大多数网络采用分层设计方法。所谓分层设计方法，就是按照信息的流动过程将网络的整体功能分解为一个个的功能层，不同机器上的同等功能层之间采用相同的协议，同一机器上的相邻功能层之间通过接口进行信息传递。 网络模型(了解)网络模型一般是指OSI七层参考模型和TCP/IP四层参考模型。这两个模型在网络中应用最为广泛。开放系统互连参考模型 (Open System Interconnect 简称OSI）是国际标准化组织(ISO)和国际电报电话咨询委员会(CCITT)联合制定的开放系统互连参考模型，为开放式互连信息系统提供了一种功能结构的框架。它从低到高分别是：物理层、数据链路层、网络层、传输层、会话层、表示层和应用层。数据通讯过程TCP/IPTCP/IP是一组用于实现网络互连的通信协议。Internet网络体系结构以TCP/IP为核心。基于TCP/IP的参考模型将协议分成四个层次，它们分别是：网络访问层、网际互联层、传输层（主机到主机）、和应用层。 应用层 应用层对应于OSI参考模型的高层，为用户提供所需要的各种服务，例如：FTP、Telnet、DNS、SMTP等. 传输层传输层对应于OSI参考模型的传输层，为应用层实体提供端到端的通信功能，保证了数据包的顺序传送及数据的完整性。该层定义了两个主要的协议：传输控制协议（TCP）和用户数据报协议（UDP).TCP协议提供的是一种可靠的、通过“三次握手”来连接的数据传输服务；而UDP协议提供的则是不保证可靠的（并不是不可靠）、无连接的数据传输服务. 网际互联层网际互联层对应于OSI参考模型的网络层，主要解决主机到主机的通信问题。它所包含的协议设计数据包在整个网络上的逻辑传输。注重重新赋予主机一个IP地址来完成对主机的寻址，它还负责数据包在多种网络中的路由。该层有三个主要协议：网际协议（IP）、互联网组管理协议（IGMP）和互联网控制报文协议（ICMP）。IP协议是网际互联层最重要的协议，它提供的是一个可靠、无连接的数据报传递服务。 网络接入层（即主机-网络层）网络接入层与OSI参考模型中的物理层和数据链路层相对应。它负责监视数据在主机和网络之间的交换。事实上，TCP/IP本身并未定义该层的协议，而由参与互连的各网络使用自己的物理层和数据链路层协议，然后与TCP/IP的网络接入层进行连接。地址解析协议（ARP）工作在此层，即OSI参考模型的数据链路层。 网络编程什么是套接字源IP地址和目的IP地址以及源端口号和目的端口号的组合称为套接字。其用于标识客户端请求的服务器和服务. 什么是网络编程: 通过使用套接字来达到进程间通信目的的编程就是网络编程. 进程之间的通信: 在同一台电脑上,A进程和B进程相互通信. 在网络中(外网/内网),A电脑中的aa程序和B电脑中的bb程序相互通信(有网络连接). 为什么需要网络编程: 如果没有网络,只能玩单机游戏.(斗地主,三国杀,CS等) 有了网络,QQ游戏等. 网络编程三要素: IP地址. 网络之间互连的协议（IP）是Internet Protocol的外语缩写，中文缩写为“网协”.在Java中使用InetAddress类表示. 1234567891011121314151617 package com.zhiyou100.net;import java.net.InetAddress;public class TestIp &#123; public static void main(String[] args) throws Exception &#123; //根据主机名确定主机的IP地址 InetAddress ip = InetAddress.getByName("DESKTOP-4DTSU5N"); System.out.println(ip.toString()); //获取主机名 System.out.println(ip.getHostName()); //获取IP地址 System.out.println(ip.getHostAddress()); //获取本机IP System.out.println(InetAddress.getLocalHost()); //是否可以连接,与ping有点像 System.out.println(InetAddress.getLocalHost().isReachable(1000)); &#125;&#125; IP地址编址方案将IP地址空间划分为A、B、C、D、E五类，其中A、B、C是基本类，D、E类作为多播和保留使用。 * A类 10.0.0.0--10.255.255.255 * B类 172.16.0.0--172.31.255.255 * C类 192.168.0.0--192.168.255.255 表示本机 - 方式1:本机IP - 方式2:127.0.0.1 - 方式3:localhost 端口 “端口”是英文port的意译，可以认为是设备与外界通讯交流的出口。端口可分为虚拟端口和物理端口，其中虚拟端口指计算机内部或交换机路由器内的端口，不可见。例如计算机中的80端口、21端口、23端口等。物理端口又称为接口，是可见端口，计算机背板的RJ45网口，交换机路由器集线器等RJ45端口。电话使用RJ11插口也属于物理端口的范畴。 协议端口如果把IP地址比作一间房子 ，端口就是出入这间房子的门。真正的房子只有几个门，但是一个IP地址的端口可以有65536（即：2^16）个之多！端口是通过端口号来标记的，端口号只有整数，范围是从0 到65535（2^16-1）。 协议:规则,数据传递/交互规则.协议(protocol)，网络协议的简称，网络协议是通信计算机双方必须共同遵从的一组约定。如怎么样建立连接、怎么样互相识别等。只有遵守这个约定，计算机之间才能相互通信交流。它的三要素是：语法、语义、时序。网络协议，也可简称协议，通常由三要素组成：(1)语法：即数据与控制信息的结构或格式；(2)语义：即需要发出何种控制信息，完成何种动作以及做出何种响应；(3)时序（同步），即事件实现顺序的详细说明。 URI与URLURI统一资源标识符（Uniform Resource Identifier，或URI)是一个用于标识某一互联网资源名称的字符串。包含:主机名,标识符,相对URI.如:https://www.baidu.com:80/index.phpURL统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。区别在Java中 URI表示一个统一资源的标识符,不能用于定位任何资源,唯一的作用就是解析. URL则包含一个可以打开到达该资源的输入流,可以简单理解URL是URI的特例.简单理解: URI和URL都表示一个资源路径.创建URL对象:URL(String protocol, String host, int port, String file) 案例112345678910111213141516171819public static void main(String[] args) throws Exception &#123; //创建URL对象 URL url = new URL("http", "www.baidu.com", 80, "/index.php"); //打开URL的连接对象 URLConnection con = url.openConnection(); //获取连接中的流数据 InputStream is = con.getInputStream(); //通过扫描器扫描输入流 Scanner sc = new Scanner(is); String line = null; //一次读取一行，打印到控制台 while(sc.hasNextLine()) &#123; line = sc.nextLine(); System.out.println(line); &#125; sc.close(); is.close(); &#125;案例21234567891011121314151617181920212223242526272829303132public static void main(String[] args) throws Exception &#123; /*//创建URL对象 URL url = new URL("http", "www.baidu.com", 80, "/img/bd_logo1.png"); //https://infinitypro-img.infinitynewtab.com/findaphoto/bigLink/10298.jpg //打开URL的连接对象 URLConnection con = url.openConnection(); //获取连接中的流数据 InputStream is = con.getInputStream(); //通过Files工具类进行文件拷贝 Files.copy(is, Paths.get("login.png")); is.close();*/ for(int i = 1188;i&lt;3000;i++) &#123; downloadImgs(i); &#125; &#125; public static void downloadImgs(int num) throws Exception&#123; //创建URL对象 URL url = new URL("http", "infinitypro-img.infinitynewtab.com", 80, "/findaphoto/bigLink/"+num+".jpg"); //https://infinitypro-img.infinitynewtab.com/findaphoto/bigLink/10298.jpg //打开URL的连接对象 HttpURLConnection con = (HttpURLConnection) url.openConnection(); if(con.getResponseCode() == 200) &#123; //获取连接中的流数据 InputStream is = con.getInputStream(); System.out.println(url); //通过Files工具类进行文件拷贝 Files.copy(is, Paths.get("D:\\test\\"+num+".jpg"),StandardCopyOption.REPLACE_EXISTING); is.close(); &#125; &#125;传输层的协议TCP和UDP的区别: TCP ：面向连接(经历三次握手)、传输可靠(保证数据正确性,保证数据顺序)、用于传输大量数据(流模式)、速度慢，建立连接需要开销较多(时间，系统资源)。 服务端和客户端 1234567891011121314151617public class Server &#123; public static void main(String[] args) throws Exception &#123; //创建服务端，指定端口号为6666 ServerSocket server = new ServerSocket(6666); System.out.println("服务端已启动"); //接受连接该服务端的客户端对象 Socket client = server.accept(); System.out.println("有一个客户端连接"+client.getInetAddress()); //服务端给客户端发个消息,获取客户端的输出流对象 PrintStream ps = new PrintStream(client.getOutputStream()); ps.println("美女，你好"); ps.close(); server.close(); &#125;&#125; 1234567891011121314 public class Client &#123; public static void main(String[] args) throws Exception, IOException &#123; //创建客户端，指明服务端的主机和端口 Socket client = new Socket("localhost",6666); //获取客户端的输入流对象，得到数据 Scanner sc = new Scanner(client.getInputStream()); while(sc.hasNextLine()) &#123; System.out.println(sc.nextLine()); &#125; sc.close(); client.close(); &#125;&#125; **服务端向客户端不断发送消息** 123456789101112131415161718public class Server &#123; public static void main(String[] args) throws Exception &#123; //创建服务端，指定端口号为6666 ServerSocket server = new ServerSocket(6666); System.out.println("服务端已启动"); //接受连接该服务端的客户端对象 Socket client = server.accept(); System.out.println("有一个客户端连接"+client.getInetAddress()); Scanner sc = new Scanner(System.in); while(true) &#123; String str = sc.nextLine(); PrintStream ps = new PrintStream(client.getOutputStream()); ps.println(str); &#125; &#125;&#125; 12345678910111213141516public class Client &#123; public static void main(String[] args) throws Exception, IOException &#123; //创建客户端，指明服务端的主机和端口 Socket client = new Socket("localhost",6666); while(true) &#123; //获取客户端的输入流对象，得到数据 Scanner sc = new Scanner(client.getInputStream()); while(sc.hasNextLine()) &#123; System.out.println(sc.nextLine()); &#125; sc.close(); &#125; &#125;&#125; **类QQ**-客户端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Client &#123; public static void main(String[] args) throws IOException &#123; // TODO Auto-generated method stub Socket client = new Socket("127.0.0.1", 8888); System.out.println("客户端已启动"); // 1.启动线程用于写给服务器 new ClientRead(client).start(); // 2.启动线程用于读取服务器的数据 new ClientWrite(client).start(); &#125;&#125;class ClientRead extends Thread &#123; private Socket client; public ClientRead(Socket client) &#123; this.client = client; &#125; @Override public void run() &#123; try &#123; BufferedReader br = new BufferedReader(new InputStreamReader(client.getInputStream())); String line = ""; while (true) &#123; line = br.readLine(); if (!"".equals(line) &amp;&amp; line != null) &#123; System.out.println("服务器说：" + line); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;/** * 读键盘，写到服务器 * * @author zhang */class ClientWrite extends Thread &#123; private Socket client; public ClientWrite(Socket client) &#123; this.client = client; &#125; @Override public void run() &#123; BufferedReader input = new BufferedReader(new InputStreamReader(System.in)); try &#123; BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(client.getOutputStream())); String str = ""; while (true) &#123; str = input.readLine(); bw.write(str); bw.newLine(); bw.flush(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; **类QQ**-服务端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class Server &#123; public static void main(String[] args) throws Exception &#123; ServerSocket server = new ServerSocket(8888); System.out.println("服务器已经建立，，等待客户端连接。。"); Socket socket = server.accept(); System.out.println("已有客户端连接。。。"); // 读，或者写 // 启动线程，用于读客户端的数据 new ServerRead(socket).start(); // 启动线程，用于向客户端写数据 new ServerWrite(socket).start(); &#125;&#125;/** * 读客户端的数据：socket中输入流，读，打印输出 * * @author zhang * */class ServerRead extends Thread &#123; private Socket s; public ServerRead(Socket s) &#123; this.s = s; &#125; @Override public void run() &#123; try &#123; BufferedReader br = new BufferedReader(new InputStreamReader(s.getInputStream())); String line = ""; while (true) &#123; line = br.readLine(); if (!"".equals(line) &amp;&amp; line != null) &#123; System.out.println("客户端说：" + line); &#125; &#125; &#125; catch (Exception e) &#123; &#125; &#125;&#125; /** * 向客户端写数据：socket中获取输出流，读键盘，写给客户端 * * @author zhang * */class ServerWrite extends Thread &#123; private Socket socket; public ServerWrite(Socket socket) &#123; this.socket = socket; &#125; @Override public void run() &#123; // 1.获取流 BufferedReader input = new BufferedReader(new InputStreamReader(System.in)); try &#123; BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(socket.getOutputStream())); String str = ""; // 2.循环向客户端写数据 while (true) &#123; str = input.readLine(); bw.write(str); bw.newLine(); bw.flush(); &#125; &#125; catch (Exception e) &#123; &#125; &#125;&#125; UDP：面向无连接、传输不可靠(丢包[数据丢失])、用于传输少量数据(数据报包模式)、速度快。发送端和接收端 客户端和服务端 1234567891011121314public class Send &#123; public static void main(String[] args) throws Exception &#123; String data = "你好，美女"; //创建发送端对象 DatagramSocket sender = new DatagramSocket(6666); //发送数据 DatagramPacket dp = new DatagramPacket(data.getBytes(), 0, data.getBytes().length, InetAddress.getLocalHost(), 10086); sender.send(dp); sender.close(); &#125; &#125; 123456789101112public class Receive &#123; public static void main(String[] args) throws Exception &#123; DatagramSocket receiver = new DatagramSocket(10086); byte[] buffer = new byte[1024]; DatagramPacket dp = new DatagramPacket(buffer,1024); receiver.receive(dp); String msg = new String(dp.getData(),0,dp.getLength()); System.out.println(msg); &#125;&#125;]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
</search>
